{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7c0f7ff-5a5d-4182-8bef-f9164ebf2739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df = df[df.tax < 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed1fbc30-da28-4056-ad07-514181ab2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"tax\", \"weight\"], axis=1).to_numpy(dtype=np.float32)\n",
    "Y = df.tax.to_numpy(dtype=np.float32)\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "random = np.random.rand(len(X))\n",
    "train = random < TRAIN_SPLIT\n",
    "val = (random >= TRAIN_SPLIT) * (random < TRAIN_SPLIT + VAL_SPLIT)\n",
    "test = random >= TRAIN_SPLIT + VAL_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61d9bcb7-492d-4146-9167-c6866b13514f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b6640e80990588e4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b6640e80990588e4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d8698c7-7a53-4a21-9ac0-529085c3efa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 468913112.6154 - mae: 11829.0666 - val_loss: 36232740.0000 - val_mae: 2703.5986\n",
      "Epoch 2/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 20860271.7692 - mae: 2172.0386 - val_loss: 9534626.0000 - val_mae: 1807.8424\n",
      "Epoch 3/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 9622266.6615 - mae: 1768.7535 - val_loss: 8217215.5000 - val_mae: 1626.1466\n",
      "Epoch 4/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 8451330.0154 - mae: 1620.1016 - val_loss: 7352000.5000 - val_mae: 1537.7913\n",
      "Epoch 5/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 7436445.4231 - mae: 1509.1798 - val_loss: 6778967.0000 - val_mae: 1473.5048\n",
      "Epoch 6/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6966907.9769 - mae: 1458.8457 - val_loss: 6193967.0000 - val_mae: 1389.5188\n",
      "Epoch 7/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6052685.9462 - mae: 1336.3255 - val_loss: 5598411.5000 - val_mae: 1279.2606\n",
      "Epoch 8/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 5481340.6308 - mae: 1238.1806 - val_loss: 5000984.0000 - val_mae: 1184.9061\n",
      "Epoch 9/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4875134.8692 - mae: 1142.8877 - val_loss: 4393570.0000 - val_mae: 1060.6392\n",
      "Epoch 10/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4336408.9269 - mae: 1025.1893 - val_loss: 3906121.5000 - val_mae: 952.6094\n",
      "Epoch 11/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3717373.7077 - mae: 913.7462 - val_loss: 3554496.2500 - val_mae: 901.3394\n",
      "Epoch 12/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3443148.6654 - mae: 875.5829 - val_loss: 3285414.0000 - val_mae: 853.2419\n",
      "Epoch 13/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3199682.0962 - mae: 832.0934 - val_loss: 3101320.2500 - val_mae: 826.9703\n",
      "Epoch 14/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3042912.2231 - mae: 812.9066 - val_loss: 3011914.7500 - val_mae: 823.3470\n",
      "Epoch 15/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2929083.5346 - mae: 798.2487 - val_loss: 2883425.2500 - val_mae: 795.9766\n",
      "Epoch 16/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2754041.0962 - mae: 769.3841 - val_loss: 2800989.7500 - val_mae: 782.7807\n",
      "Epoch 17/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2658951.5423 - mae: 750.8867 - val_loss: 2759212.5000 - val_mae: 775.8868\n",
      "Epoch 18/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2665646.2538 - mae: 751.0977 - val_loss: 2719505.7500 - val_mae: 771.7771\n",
      "Epoch 19/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2488923.7846 - mae: 734.7247 - val_loss: 2653309.5000 - val_mae: 750.3057\n",
      "Epoch 20/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2531169.4846 - mae: 725.8983 - val_loss: 2604939.0000 - val_mae: 739.9998\n",
      "Epoch 21/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2535962.0692 - mae: 725.0258 - val_loss: 2571631.0000 - val_mae: 724.9497\n",
      "Epoch 22/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2445331.0192 - mae: 703.7655 - val_loss: 2610839.5000 - val_mae: 734.7006\n",
      "Epoch 23/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2352731.1231 - mae: 687.1461 - val_loss: 2501175.7500 - val_mae: 700.6915\n",
      "Epoch 24/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2390206.3962 - mae: 684.8012 - val_loss: 2506039.7500 - val_mae: 697.4798\n",
      "Epoch 25/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2350923.2346 - mae: 677.4613 - val_loss: 2452813.7500 - val_mae: 681.9243\n",
      "Epoch 26/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2288865.6269 - mae: 653.8941 - val_loss: 2437025.0000 - val_mae: 678.1956\n",
      "Epoch 27/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2287832.9500 - mae: 658.9791 - val_loss: 2414775.5000 - val_mae: 679.1658\n",
      "Epoch 28/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2175059.5077 - mae: 641.9677 - val_loss: 2453122.0000 - val_mae: 678.5957\n",
      "Epoch 29/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2379055.1077 - mae: 676.9230 - val_loss: 2371426.0000 - val_mae: 654.6823\n",
      "Epoch 30/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2273246.9154 - mae: 640.2145 - val_loss: 2390431.0000 - val_mae: 660.6818\n",
      "Epoch 31/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2166376.5154 - mae: 624.9074 - val_loss: 2380997.2500 - val_mae: 661.6984\n",
      "Epoch 32/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2271984.3885 - mae: 656.0478 - val_loss: 2328535.7500 - val_mae: 634.5649\n",
      "Epoch 33/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2171183.7423 - mae: 611.5035 - val_loss: 2327874.0000 - val_mae: 641.4491\n",
      "Epoch 34/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2179143.5635 - mae: 618.4298 - val_loss: 2303872.0000 - val_mae: 633.0233\n",
      "Epoch 35/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2182373.4808 - mae: 619.8045 - val_loss: 2283522.0000 - val_mae: 626.3726\n",
      "Epoch 36/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2229907.9923 - mae: 619.3786 - val_loss: 2282925.0000 - val_mae: 629.8049\n",
      "Epoch 37/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2136381.0365 - mae: 608.9420 - val_loss: 2288283.2500 - val_mae: 636.7883\n",
      "Epoch 38/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2170624.4192 - mae: 626.6679 - val_loss: 2377820.0000 - val_mae: 647.2956\n",
      "Epoch 39/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2193504.3346 - mae: 622.4454 - val_loss: 2255136.5000 - val_mae: 623.2456\n",
      "Epoch 40/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2128431.2692 - mae: 607.7574 - val_loss: 2252236.7500 - val_mae: 617.5167\n",
      "Epoch 41/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2113569.9365 - mae: 598.5242 - val_loss: 2251534.2500 - val_mae: 630.2752\n",
      "Epoch 42/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2137797.0404 - mae: 615.9345 - val_loss: 2239760.2500 - val_mae: 621.5226\n",
      "Epoch 43/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2097665.9250 - mae: 599.8391 - val_loss: 2222509.0000 - val_mae: 615.4938\n",
      "Epoch 44/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2112606.6731 - mae: 602.9803 - val_loss: 2238214.2500 - val_mae: 630.6546\n",
      "Epoch 45/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2058952.4442 - mae: 598.3818 - val_loss: 2240656.0000 - val_mae: 622.9678\n",
      "Epoch 46/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2125696.5077 - mae: 603.2542 - val_loss: 2209624.0000 - val_mae: 612.6627\n",
      "Epoch 47/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2057053.1250 - mae: 592.6556 - val_loss: 2190162.2500 - val_mae: 611.6563\n",
      "Epoch 48/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2102714.2827 - mae: 595.5082 - val_loss: 2191169.5000 - val_mae: 606.4658\n",
      "Epoch 49/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2034196.1615 - mae: 587.7960 - val_loss: 2178620.2500 - val_mae: 603.7759\n",
      "Epoch 50/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2011788.7538 - mae: 584.1126 - val_loss: 2242871.2500 - val_mae: 619.8702\n",
      "Epoch 51/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2156280.8635 - mae: 608.7340 - val_loss: 2175221.0000 - val_mae: 606.3193\n",
      "Epoch 52/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2106402.2154 - mae: 604.8099 - val_loss: 2203651.5000 - val_mae: 623.0914\n",
      "Epoch 53/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2158811.5827 - mae: 613.4802 - val_loss: 2224287.0000 - val_mae: 620.6886\n",
      "Epoch 54/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2053730.2346 - mae: 601.2626 - val_loss: 2172517.5000 - val_mae: 617.4536\n",
      "Epoch 55/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2143326.5135 - mae: 602.6478 - val_loss: 2180832.2500 - val_mae: 608.4752\n",
      "Epoch 56/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2077726.6231 - mae: 595.4135 - val_loss: 2155135.7500 - val_mae: 607.0546\n",
      "Epoch 57/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2059954.3519 - mae: 588.4174 - val_loss: 2154479.2500 - val_mae: 609.7852\n",
      "Epoch 58/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2064487.0865 - mae: 595.8901 - val_loss: 2243676.7500 - val_mae: 625.8745\n",
      "Epoch 59/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2043428.6788 - mae: 602.7522 - val_loss: 2150187.2500 - val_mae: 613.9385\n",
      "Epoch 60/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2165279.2442 - mae: 624.2443 - val_loss: 2199706.0000 - val_mae: 616.5661\n",
      "Epoch 61/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2072690.8635 - mae: 601.8799 - val_loss: 2189431.5000 - val_mae: 617.8596\n",
      "Epoch 62/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2125956.2654 - mae: 620.9838 - val_loss: 2157509.5000 - val_mae: 602.6140\n",
      "Epoch 63/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1962307.4308 - mae: 580.0932 - val_loss: 2137638.5000 - val_mae: 595.3508\n",
      "Epoch 64/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2047922.8904 - mae: 579.4005 - val_loss: 2149153.5000 - val_mae: 611.9210\n",
      "Epoch 65/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2079298.7885 - mae: 596.3948 - val_loss: 2149643.2500 - val_mae: 605.9852\n",
      "Epoch 66/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2004492.4096 - mae: 583.0835 - val_loss: 2141387.0000 - val_mae: 622.4834\n",
      "Epoch 67/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2050017.1077 - mae: 596.4176 - val_loss: 2139298.2500 - val_mae: 600.8677\n",
      "Epoch 68/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2014461.8058 - mae: 585.4551 - val_loss: 2129939.2500 - val_mae: 591.8900\n",
      "Epoch 69/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2027534.9519 - mae: 580.3376 - val_loss: 2140437.5000 - val_mae: 619.8713\n",
      "Epoch 70/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2042831.8365 - mae: 584.7705 - val_loss: 2133477.2500 - val_mae: 602.1390\n",
      "Epoch 71/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2005867.9500 - mae: 585.5882 - val_loss: 2159699.7500 - val_mae: 607.8256\n",
      "Epoch 72/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2064185.6981 - mae: 589.4692 - val_loss: 2135474.2500 - val_mae: 609.7348\n",
      "Epoch 73/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1994798.9500 - mae: 585.8653 - val_loss: 2119030.2500 - val_mae: 601.4354\n",
      "Epoch 74/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2000164.7923 - mae: 578.6969 - val_loss: 2149317.5000 - val_mae: 617.8364\n",
      "Epoch 75/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2017209.4058 - mae: 593.3788 - val_loss: 2164883.7500 - val_mae: 603.0970\n",
      "Epoch 76/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2019524.1385 - mae: 575.6317 - val_loss: 2160168.7500 - val_mae: 625.2858\n",
      "Epoch 77/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2051333.1481 - mae: 594.0332 - val_loss: 2109347.2500 - val_mae: 600.4630\n",
      "Epoch 78/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2023758.6423 - mae: 586.2245 - val_loss: 2140006.7500 - val_mae: 632.1097\n",
      "Epoch 79/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1988616.3327 - mae: 588.4981 - val_loss: 2105612.7500 - val_mae: 599.5085\n",
      "Epoch 80/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2046890.8096 - mae: 589.9871 - val_loss: 2114490.5000 - val_mae: 626.7119\n",
      "Epoch 81/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1992747.6885 - mae: 595.5081 - val_loss: 2194333.7500 - val_mae: 621.8085\n",
      "Epoch 82/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2101170.7673 - mae: 618.5289 - val_loss: 2096144.5000 - val_mae: 606.1025\n",
      "Epoch 83/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2031198.5577 - mae: 597.0972 - val_loss: 2105927.0000 - val_mae: 596.3193\n",
      "Epoch 84/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1998498.4731 - mae: 589.6654 - val_loss: 2097680.2500 - val_mae: 608.6927\n",
      "Epoch 85/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1977841.5673 - mae: 588.1001 - val_loss: 2092093.8750 - val_mae: 600.5760\n",
      "Epoch 86/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2021879.1519 - mae: 587.0063 - val_loss: 2132372.0000 - val_mae: 604.7719\n",
      "Epoch 87/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2035115.4827 - mae: 595.0453 - val_loss: 2093549.6250 - val_mae: 597.8020\n",
      "Epoch 88/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1998537.8712 - mae: 590.8910 - val_loss: 2131215.2500 - val_mae: 617.2552\n",
      "Epoch 89/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1995510.4462 - mae: 597.1813 - val_loss: 2091679.2500 - val_mae: 611.2778\n",
      "Epoch 90/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1978893.6442 - mae: 590.3101 - val_loss: 2098879.7500 - val_mae: 595.8851\n",
      "Epoch 91/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2034774.5192 - mae: 591.5692 - val_loss: 2078671.7500 - val_mae: 598.9788\n",
      "Epoch 92/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1977612.6981 - mae: 587.5840 - val_loss: 2075798.5000 - val_mae: 600.1059\n",
      "Epoch 93/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2029645.7173 - mae: 588.0893 - val_loss: 2129348.7500 - val_mae: 605.0857\n",
      "Epoch 94/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2082971.4077 - mae: 600.7711 - val_loss: 2183939.7500 - val_mae: 648.8748\n",
      "Epoch 95/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2049471.3077 - mae: 610.4609 - val_loss: 2077359.7500 - val_mae: 592.5668\n",
      "Epoch 96/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2028794.3481 - mae: 589.1016 - val_loss: 2102047.0000 - val_mae: 600.8204\n",
      "Epoch 97/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2064843.4558 - mae: 592.5850 - val_loss: 2184042.7500 - val_mae: 663.5787\n",
      "Epoch 98/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2121696.2538 - mae: 620.2705 - val_loss: 2135564.5000 - val_mae: 605.6795\n",
      "Epoch 99/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1998021.5615 - mae: 586.7320 - val_loss: 2314236.7500 - val_mae: 638.9303\n",
      "Epoch 100/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2113032.0635 - mae: 621.4553 - val_loss: 2075275.0000 - val_mae: 612.9825\n",
      "Epoch 101/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2000034.0481 - mae: 591.7633 - val_loss: 2152901.5000 - val_mae: 607.8240\n",
      "Epoch 102/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2008321.0558 - mae: 591.0966 - val_loss: 2184707.5000 - val_mae: 655.5316\n",
      "Epoch 103/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2067811.7558 - mae: 605.8745 - val_loss: 2092905.8750 - val_mae: 594.6902\n",
      "Epoch 104/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2019343.5481 - mae: 591.5640 - val_loss: 2130909.5000 - val_mae: 637.8047\n",
      "Epoch 105/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1953604.6808 - mae: 594.3864 - val_loss: 2082374.8750 - val_mae: 617.8263\n",
      "Epoch 106/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2004898.6385 - mae: 588.8441 - val_loss: 2207616.5000 - val_mae: 625.1802\n",
      "Epoch 107/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2150424.7962 - mae: 623.4751 - val_loss: 2065209.8750 - val_mae: 601.9797\n",
      "Epoch 108/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1981471.5692 - mae: 601.1191 - val_loss: 2137006.2500 - val_mae: 606.4232\n",
      "Epoch 109/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2025238.8692 - mae: 594.0321 - val_loss: 2059665.7500 - val_mae: 588.2193\n",
      "Epoch 110/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2076734.1750 - mae: 600.1977 - val_loss: 2098114.5000 - val_mae: 597.7287\n",
      "Epoch 111/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2013461.8981 - mae: 592.9015 - val_loss: 2056927.1250 - val_mae: 604.9630\n",
      "Epoch 112/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1971057.7096 - mae: 588.7021 - val_loss: 2049646.0000 - val_mae: 591.9232\n",
      "Epoch 113/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1984494.6788 - mae: 596.1111 - val_loss: 2161183.0000 - val_mae: 616.6161\n",
      "Epoch 114/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1986011.2404 - mae: 592.9786 - val_loss: 2361987.5000 - val_mae: 666.8719\n",
      "Epoch 115/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2205973.2250 - mae: 648.4664 - val_loss: 2282501.2500 - val_mae: 637.7569\n",
      "Epoch 116/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2180880.7327 - mae: 645.4339 - val_loss: 2116270.5000 - val_mae: 610.3837\n",
      "Epoch 117/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1967336.7000 - mae: 599.2633 - val_loss: 2045823.6250 - val_mae: 602.9421\n",
      "Epoch 118/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2008798.9115 - mae: 596.3185 - val_loss: 2127724.5000 - val_mae: 640.2408\n",
      "Epoch 119/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1961756.8000 - mae: 595.5109 - val_loss: 2142038.0000 - val_mae: 644.6857\n",
      "Epoch 120/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2009787.5635 - mae: 591.3397 - val_loss: 2078799.1250 - val_mae: 592.3657\n",
      "Epoch 121/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1925384.4731 - mae: 579.8730 - val_loss: 2061953.5000 - val_mae: 592.3483\n",
      "Epoch 122/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1975439.8365 - mae: 588.8843 - val_loss: 2068778.7500 - val_mae: 599.6721\n",
      "Epoch 123/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2022782.4288 - mae: 591.6408 - val_loss: 2036164.0000 - val_mae: 599.6097\n",
      "Epoch 124/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1923295.8615 - mae: 586.6428 - val_loss: 2029552.7500 - val_mae: 594.6846\n",
      "Epoch 125/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1986322.7346 - mae: 586.0762 - val_loss: 2052013.1250 - val_mae: 627.2499\n",
      "Epoch 126/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2006830.5365 - mae: 608.6812 - val_loss: 2127226.2500 - val_mae: 647.6079\n",
      "Epoch 127/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1999228.5154 - mae: 601.2033 - val_loss: 2089890.0000 - val_mae: 608.4768\n",
      "Epoch 128/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1930151.0635 - mae: 595.1421 - val_loss: 2091709.2500 - val_mae: 634.5729\n",
      "Epoch 129/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1997305.7327 - mae: 605.3960 - val_loss: 2065839.0000 - val_mae: 598.1886\n",
      "Epoch 130/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1973600.8712 - mae: 587.3446 - val_loss: 2031674.0000 - val_mae: 599.3280\n",
      "Epoch 131/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1943854.7385 - mae: 584.9252 - val_loss: 2065444.1250 - val_mae: 620.6185\n",
      "Epoch 132/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2035990.0808 - mae: 602.2056 - val_loss: 2066447.1250 - val_mae: 597.2330\n",
      "Epoch 133/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1970113.3654 - mae: 587.0165 - val_loss: 2022281.1250 - val_mae: 602.7902\n",
      "Epoch 134/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1904405.7173 - mae: 577.4017 - val_loss: 2023042.2500 - val_mae: 598.2227\n",
      "Epoch 135/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1933807.2558 - mae: 583.3290 - val_loss: 2027325.0000 - val_mae: 612.3328\n",
      "Epoch 136/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1939912.6404 - mae: 591.1471 - val_loss: 2216867.0000 - val_mae: 674.3608\n",
      "Epoch 137/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2119706.5865 - mae: 645.7247 - val_loss: 2017371.7500 - val_mae: 597.3965\n",
      "Epoch 138/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1975612.2173 - mae: 590.4726 - val_loss: 2048604.1250 - val_mae: 591.3357\n",
      "Epoch 139/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1943989.7654 - mae: 582.5356 - val_loss: 2070953.1250 - val_mae: 602.8817\n",
      "Epoch 140/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1954375.1904 - mae: 588.2848 - val_loss: 2234400.0000 - val_mae: 696.0399\n",
      "Epoch 141/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2114498.4385 - mae: 630.9808 - val_loss: 2014966.7500 - val_mae: 598.2916\n",
      "Epoch 142/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1878588.5250 - mae: 570.8809 - val_loss: 2060824.1250 - val_mae: 630.9040\n",
      "Epoch 143/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1929597.4519 - mae: 582.0863 - val_loss: 2231267.0000 - val_mae: 677.9644\n",
      "Epoch 144/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2098591.5865 - mae: 628.1109 - val_loss: 2034284.1250 - val_mae: 625.8790\n",
      "Epoch 145/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1907468.2154 - mae: 586.5447 - val_loss: 2009936.7500 - val_mae: 603.6391\n",
      "Epoch 146/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1987253.3192 - mae: 592.0094 - val_loss: 2028665.1250 - val_mae: 618.4961\n",
      "Epoch 147/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1910996.9058 - mae: 589.0595 - val_loss: 2019083.1250 - val_mae: 605.7331\n",
      "Epoch 148/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1902640.6154 - mae: 581.3102 - val_loss: 1997868.2500 - val_mae: 602.9745\n",
      "Epoch 149/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1964983.4346 - mae: 592.6861 - val_loss: 2087488.2500 - val_mae: 608.3475\n",
      "Epoch 150/256\n",
      "64/64 [==============================] - ETA: 0s - loss: 1957484.5938 - mae: 595.582 - 0s 2ms/step - loss: 1952056.6404 - mae: 593.2145 - val_loss: 2007918.3750 - val_mae: 596.9892\n",
      "Epoch 151/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1944604.2423 - mae: 583.9265 - val_loss: 2004483.3750 - val_mae: 597.7855\n",
      "Epoch 152/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1959241.9288 - mae: 588.9056 - val_loss: 2279410.2500 - val_mae: 693.5125\n",
      "Epoch 153/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2042165.3750 - mae: 618.9857 - val_loss: 2024696.8750 - val_mae: 632.8474\n",
      "Epoch 154/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1895033.2596 - mae: 602.0812 - val_loss: 2061513.8750 - val_mae: 635.5521\n",
      "Epoch 155/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1969032.9250 - mae: 605.4446 - val_loss: 2025242.8750 - val_mae: 595.5216\n",
      "Epoch 156/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1926861.8846 - mae: 586.2171 - val_loss: 2463637.2500 - val_mae: 691.2352\n",
      "Epoch 157/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2310423.0423 - mae: 675.2365 - val_loss: 2040605.3750 - val_mae: 634.0971\n",
      "Epoch 158/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1924314.8019 - mae: 593.1509 - val_loss: 2242040.0000 - val_mae: 687.0374\n",
      "Epoch 159/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2077344.1769 - mae: 608.6340 - val_loss: 1999616.7500 - val_mae: 611.9281\n",
      "Epoch 160/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1948928.7750 - mae: 590.0550 - val_loss: 2015762.8750 - val_mae: 619.4335\n",
      "Epoch 161/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2027767.5077 - mae: 613.0796 - val_loss: 1990577.8750 - val_mae: 600.3275\n",
      "Epoch 162/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1916150.7635 - mae: 584.1245 - val_loss: 1984080.2500 - val_mae: 597.0739\n",
      "Epoch 163/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1979406.3500 - mae: 590.6366 - val_loss: 2016576.0000 - val_mae: 591.1142\n",
      "Epoch 164/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1928790.4019 - mae: 582.8013 - val_loss: 1979279.3750 - val_mae: 592.1737\n",
      "Epoch 165/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1941095.1212 - mae: 584.4240 - val_loss: 1979316.5000 - val_mae: 597.8296\n",
      "Epoch 166/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1912427.1000 - mae: 592.0899 - val_loss: 1993713.8750 - val_mae: 619.5430\n",
      "Epoch 167/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1921456.4712 - mae: 592.2908 - val_loss: 1972307.6250 - val_mae: 594.8917\n",
      "Epoch 168/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1927636.2250 - mae: 581.8488 - val_loss: 2228251.7500 - val_mae: 702.3571\n",
      "Epoch 169/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2176392.5288 - mae: 656.7161 - val_loss: 2003484.1250 - val_mae: 603.8615\n",
      "Epoch 170/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1897365.3000 - mae: 597.1018 - val_loss: 1972106.5000 - val_mae: 595.0277\n",
      "Epoch 171/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1893505.3596 - mae: 584.9316 - val_loss: 2044494.7500 - val_mae: 632.4523\n",
      "Epoch 172/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1966265.1865 - mae: 598.5827 - val_loss: 1996480.6250 - val_mae: 613.4212\n",
      "Epoch 173/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1909011.4808 - mae: 581.0269 - val_loss: 2040610.7500 - val_mae: 635.5802\n",
      "Epoch 174/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1943663.1365 - mae: 599.0175 - val_loss: 1997878.8750 - val_mae: 618.1055\n",
      "Epoch 175/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1918753.9615 - mae: 592.9176 - val_loss: 1968540.1250 - val_mae: 596.2700\n",
      "Epoch 176/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1969147.1192 - mae: 586.3716 - val_loss: 2021979.5000 - val_mae: 599.8329\n",
      "Epoch 177/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2005574.7673 - mae: 609.3987 - val_loss: 1979142.1250 - val_mae: 597.1962\n",
      "Epoch 178/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1999401.1288 - mae: 601.6154 - val_loss: 2087407.3750 - val_mae: 603.6021\n",
      "Epoch 179/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1904706.1077 - mae: 581.7803 - val_loss: 1988284.2500 - val_mae: 606.7836\n",
      "Epoch 180/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1965813.2673 - mae: 611.1508 - val_loss: 1973076.5000 - val_mae: 603.0458\n",
      "Epoch 181/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1928278.1173 - mae: 591.7771 - val_loss: 2001659.5000 - val_mae: 615.0252\n",
      "Epoch 182/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1934507.3231 - mae: 591.5117 - val_loss: 1967265.6250 - val_mae: 598.8129\n",
      "Epoch 183/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1861463.9962 - mae: 576.7951 - val_loss: 1976679.2500 - val_mae: 589.1156\n",
      "Epoch 184/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1869054.5615 - mae: 572.7990 - val_loss: 2029301.0000 - val_mae: 640.8812\n",
      "Epoch 185/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1898073.4577 - mae: 596.1986 - val_loss: 1990110.1250 - val_mae: 588.1203\n",
      "Epoch 186/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1909657.3000 - mae: 586.1134 - val_loss: 1967555.0000 - val_mae: 599.7609\n",
      "Epoch 187/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1950115.4231 - mae: 585.6644 - val_loss: 1995897.5000 - val_mae: 593.0876\n",
      "Epoch 188/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1941482.5058 - mae: 590.8251 - val_loss: 2107240.5000 - val_mae: 654.2589\n",
      "Epoch 189/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1938587.6308 - mae: 590.3494 - val_loss: 2025998.7500 - val_mae: 595.3021\n",
      "Epoch 190/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1915289.4365 - mae: 587.5562 - val_loss: 1994746.5000 - val_mae: 590.2921\n",
      "Epoch 191/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1880300.3942 - mae: 581.8109 - val_loss: 1984360.6250 - val_mae: 596.3420\n",
      "Epoch 192/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1913109.3481 - mae: 581.4973 - val_loss: 1961713.6250 - val_mae: 599.5119\n",
      "Epoch 193/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1940308.2846 - mae: 590.3610 - val_loss: 1964031.0000 - val_mae: 606.7161\n",
      "Epoch 194/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1839896.4692 - mae: 577.5522 - val_loss: 1970411.3750 - val_mae: 611.2811\n",
      "Epoch 195/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1941761.9673 - mae: 596.5644 - val_loss: 1960455.7500 - val_mae: 611.0018\n",
      "Epoch 196/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1861544.0308 - mae: 593.4678 - val_loss: 2233080.7500 - val_mae: 689.6790\n",
      "Epoch 197/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2012168.7135 - mae: 616.1476 - val_loss: 1957315.3750 - val_mae: 594.4717\n",
      "Epoch 198/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1906131.3154 - mae: 585.8916 - val_loss: 1971764.5000 - val_mae: 593.0285\n",
      "Epoch 199/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1859120.0808 - mae: 578.2439 - val_loss: 1994324.0000 - val_mae: 629.9561\n",
      "Epoch 200/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1877122.4404 - mae: 590.5769 - val_loss: 1997578.1250 - val_mae: 619.5816\n",
      "Epoch 201/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1904071.6962 - mae: 591.7211 - val_loss: 1956539.1250 - val_mae: 597.3492\n",
      "Epoch 202/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1909083.5558 - mae: 586.1183 - val_loss: 1955054.2500 - val_mae: 601.4885\n",
      "Epoch 203/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1950224.7250 - mae: 597.8312 - val_loss: 1956754.8750 - val_mae: 607.9492\n",
      "Epoch 204/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1912342.3692 - mae: 589.2599 - val_loss: 1984681.0000 - val_mae: 592.5877\n",
      "Epoch 205/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1864680.6019 - mae: 576.7593 - val_loss: 1957035.2500 - val_mae: 620.6818\n",
      "Epoch 206/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1946876.0769 - mae: 605.0212 - val_loss: 1951562.2500 - val_mae: 603.7476\n",
      "Epoch 207/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1883697.5308 - mae: 588.1988 - val_loss: 1942482.7500 - val_mae: 592.0449\n",
      "Epoch 208/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1844219.5442 - mae: 583.1292 - val_loss: 1957969.0000 - val_mae: 619.8635\n",
      "Epoch 209/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1854529.6692 - mae: 588.1012 - val_loss: 1991236.3750 - val_mae: 627.2314\n",
      "Epoch 210/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1937766.7577 - mae: 597.5983 - val_loss: 2035743.1250 - val_mae: 602.2601\n",
      "Epoch 211/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1918192.3750 - mae: 599.2615 - val_loss: 1963501.3750 - val_mae: 602.7313\n",
      "Epoch 212/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1824446.1269 - mae: 585.4222 - val_loss: 1961702.8750 - val_mae: 624.3595\n",
      "Epoch 213/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1859598.2462 - mae: 590.4447 - val_loss: 2058724.0000 - val_mae: 623.3150\n",
      "Epoch 214/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1984614.6519 - mae: 606.0451 - val_loss: 1951382.5000 - val_mae: 602.5826\n",
      "Epoch 215/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1950930.0519 - mae: 617.4869 - val_loss: 1952162.7500 - val_mae: 630.9094\n",
      "Epoch 216/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1943973.5135 - mae: 604.1258 - val_loss: 1959533.0000 - val_mae: 593.9443\n",
      "Epoch 217/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1882310.6942 - mae: 577.9989 - val_loss: 1932577.0000 - val_mae: 597.1797\n",
      "Epoch 218/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1835684.1577 - mae: 575.6395 - val_loss: 1930891.1250 - val_mae: 608.7001\n",
      "Epoch 219/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1977674.5192 - mae: 598.2620 - val_loss: 1936097.2500 - val_mae: 597.4036\n",
      "Epoch 220/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1857741.5365 - mae: 580.1460 - val_loss: 2144611.2500 - val_mae: 618.3646\n",
      "Epoch 221/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1965856.9923 - mae: 601.4599 - val_loss: 1930459.3750 - val_mae: 597.6747\n",
      "Epoch 222/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1916011.2423 - mae: 594.9285 - val_loss: 1940563.7500 - val_mae: 620.1204\n",
      "Epoch 223/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1935461.8962 - mae: 600.0978 - val_loss: 1958542.7500 - val_mae: 585.0308\n",
      "Epoch 224/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1868098.1115 - mae: 585.7148 - val_loss: 1927907.7500 - val_mae: 606.8712\n",
      "Epoch 225/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1919277.4019 - mae: 591.7335 - val_loss: 1951962.2500 - val_mae: 628.3829\n",
      "Epoch 226/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1911210.7077 - mae: 607.2338 - val_loss: 1939609.2500 - val_mae: 616.6219\n",
      "Epoch 227/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1904883.1135 - mae: 594.0241 - val_loss: 1937871.7500 - val_mae: 611.7946\n",
      "Epoch 228/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1939749.2923 - mae: 609.8328 - val_loss: 1965375.0000 - val_mae: 597.8896\n",
      "Epoch 229/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1891536.2173 - mae: 586.3484 - val_loss: 1939118.0000 - val_mae: 608.6988\n",
      "Epoch 230/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1858586.5404 - mae: 583.7574 - val_loss: 1968779.5000 - val_mae: 591.1028\n",
      "Epoch 231/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1915797.3692 - mae: 591.0349 - val_loss: 1950405.6250 - val_mae: 597.5195\n",
      "Epoch 232/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1898090.8615 - mae: 593.9009 - val_loss: 2096264.5000 - val_mae: 617.0363\n",
      "Epoch 233/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1976729.0173 - mae: 606.6200 - val_loss: 1929628.2500 - val_mae: 597.1066\n",
      "Epoch 234/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1881307.6231 - mae: 594.2825 - val_loss: 2017279.2500 - val_mae: 603.1104\n",
      "Epoch 235/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1877820.7577 - mae: 587.7605 - val_loss: 1924642.7500 - val_mae: 606.1302\n",
      "Epoch 236/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1862203.3788 - mae: 594.2708 - val_loss: 1965737.5000 - val_mae: 602.7148\n",
      "Epoch 237/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1889039.4596 - mae: 589.1814 - val_loss: 2081822.3750 - val_mae: 609.3618\n",
      "Epoch 238/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1913283.6942 - mae: 592.9486 - val_loss: 1938013.0000 - val_mae: 614.0919\n",
      "Epoch 239/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1858559.3385 - mae: 580.6562 - val_loss: 1942466.1250 - val_mae: 590.3595\n",
      "Epoch 240/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1922983.2288 - mae: 588.0011 - val_loss: 2100294.0000 - val_mae: 609.5444\n",
      "Epoch 241/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2079075.7115 - mae: 622.4480 - val_loss: 1918795.0000 - val_mae: 596.1354\n",
      "Epoch 242/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1844622.8788 - mae: 582.4042 - val_loss: 2056377.2500 - val_mae: 657.8261\n",
      "Epoch 243/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1993757.8788 - mae: 621.7780 - val_loss: 2001404.6250 - val_mae: 644.8793\n",
      "Epoch 244/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1984464.0212 - mae: 624.1755 - val_loss: 1914129.2500 - val_mae: 606.7938\n",
      "Epoch 245/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1831926.0962 - mae: 587.9044 - val_loss: 1914044.8750 - val_mae: 599.0916\n",
      "Epoch 246/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1894120.7538 - mae: 590.4452 - val_loss: 2159910.5000 - val_mae: 621.7866\n",
      "Epoch 247/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2033256.4000 - mae: 619.6255 - val_loss: 1969738.1250 - val_mae: 622.3970\n",
      "Epoch 248/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1949670.5404 - mae: 601.4289 - val_loss: 1973603.5000 - val_mae: 602.9556\n",
      "Epoch 249/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1866722.7827 - mae: 589.7358 - val_loss: 1905877.5000 - val_mae: 601.7170\n",
      "Epoch 250/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1877044.3865 - mae: 587.9287 - val_loss: 1912024.0000 - val_mae: 601.6837\n",
      "Epoch 251/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1853064.3019 - mae: 593.1000 - val_loss: 1997094.5000 - val_mae: 597.0471\n",
      "Epoch 252/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2047967.1115 - mae: 620.6557 - val_loss: 1923551.7500 - val_mae: 592.5468\n",
      "Epoch 253/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1848142.8615 - mae: 589.9499 - val_loss: 1915714.2500 - val_mae: 607.4886\n",
      "Epoch 254/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1854203.1885 - mae: 587.1069 - val_loss: 1903726.0000 - val_mae: 600.9432\n",
      "Epoch 255/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1866888.3731 - mae: 596.8227 - val_loss: 1980178.5000 - val_mae: 607.1752\n",
      "Epoch 256/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1876027.7962 - mae: 597.7075 - val_loss: 2015473.2500 - val_mae: 602.7029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb203061d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras import Model\n",
    "from datetime import datetime\n",
    "\n",
    "weights = tf.constant(df.weight.to_numpy(dtype=np.float32))\n",
    "\n",
    "inp = Input(shape=(36,))\n",
    "num_vars = Dense(8, activation=\"relu\")(inp)\n",
    "bool_vars = Dense(8, activation=\"sigmoid\")(inp)\n",
    "concat = Concatenate()([num_vars, bool_vars])\n",
    "final = Dense(16, activation=\"relu\")(concat)\n",
    "out = Dense(1, activation=\"linear\")(final)\n",
    "\n",
    "logdir = \"logs/scalars/hybrid-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X[train], Y[train], validation_data=(X[val], Y[val]), batch_size=1024, epochs=256, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebf03a16-447c-4472-a189-1383160f23fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3707997538.4615 - mae: 36522.1644 - val_loss: 2544419584.0000 - val_mae: 30023.8945\n",
      "Epoch 2/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2269573439.0154 - mae: 27897.3705 - val_loss: 1470368768.0000 - val_mae: 22127.3047\n",
      "Epoch 3/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1254971260.0615 - mae: 20148.5255 - val_loss: 826792512.0000 - val_mae: 16150.4658\n",
      "Epoch 4/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 708229320.8615 - mae: 14900.3109 - val_loss: 470175584.0000 - val_mae: 12519.5078\n",
      "Epoch 5/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 410855877.9077 - mae: 11704.4933 - val_loss: 282679232.0000 - val_mae: 9824.1885\n",
      "Epoch 6/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 251632576.2462 - mae: 9257.4544 - val_loss: 188577584.0000 - val_mae: 7931.2256\n",
      "Epoch 7/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 172494234.5846 - mae: 7535.8056 - val_loss: 134916416.0000 - val_mae: 6610.6123\n",
      "Epoch 8/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 129516545.2308 - mae: 6373.7163 - val_loss: 101271416.0000 - val_mae: 5684.6011\n",
      "Epoch 9/256\n",
      "64/64 [==============================] - 0s 999us/step - loss: 96650436.4308 - mae: 5550.7934 - val_loss: 77689464.0000 - val_mae: 4989.2305\n",
      "Epoch 10/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 76233688.1231 - mae: 4910.4230 - val_loss: 59767628.0000 - val_mae: 4411.5469\n",
      "Epoch 11/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 58438663.8154 - mae: 4328.8204 - val_loss: 46133128.0000 - val_mae: 3913.3894\n",
      "Epoch 12/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 45888020.4923 - mae: 3866.6543 - val_loss: 35850772.0000 - val_mae: 3479.5835\n",
      "Epoch 13/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 35588620.7385 - mae: 3411.7043 - val_loss: 28148078.0000 - val_mae: 3109.0703\n",
      "Epoch 14/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 29127642.5538 - mae: 3113.3348 - val_loss: 22433620.0000 - val_mae: 2800.9915\n",
      "Epoch 15/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 22181004.6769 - mae: 2749.7086 - val_loss: 18424078.0000 - val_mae: 2538.1138\n",
      "Epoch 16/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 18947709.6615 - mae: 2507.3252 - val_loss: 15496514.0000 - val_mae: 2324.3972\n",
      "Epoch 17/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 15384336.8923 - mae: 2279.2784 - val_loss: 13411370.0000 - val_mae: 2149.8972\n",
      "Epoch 18/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 13589138.5385 - mae: 2116.8354 - val_loss: 11988662.0000 - val_mae: 2019.4254\n",
      "Epoch 19/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 12152297.5846 - mae: 1993.0719 - val_loss: 11019989.0000 - val_mae: 1906.8528\n",
      "Epoch 20/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 10971754.6000 - mae: 1862.7120 - val_loss: 10326149.0000 - val_mae: 1820.6678\n",
      "Epoch 21/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 10437830.6308 - mae: 1798.2590 - val_loss: 9819791.0000 - val_mae: 1768.1104\n",
      "Epoch 22/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9758980.7385 - mae: 1729.5325 - val_loss: 9438903.0000 - val_mae: 1730.7253\n",
      "Epoch 23/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 9247287.6154 - mae: 1685.0987 - val_loss: 9136281.0000 - val_mae: 1700.5751\n",
      "Epoch 24/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8936283.9308 - mae: 1663.2887 - val_loss: 8895127.0000 - val_mae: 1668.7129\n",
      "Epoch 25/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8464739.1538 - mae: 1618.1182 - val_loss: 8687771.0000 - val_mae: 1643.6344\n",
      "Epoch 26/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8549252.3462 - mae: 1608.1606 - val_loss: 8503235.0000 - val_mae: 1619.0532\n",
      "Epoch 27/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8368746.4615 - mae: 1588.0274 - val_loss: 8339427.5000 - val_mae: 1597.6385\n",
      "Epoch 28/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8070143.0769 - mae: 1552.2847 - val_loss: 8183110.5000 - val_mae: 1574.4510\n",
      "Epoch 29/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7932159.7385 - mae: 1539.3933 - val_loss: 8033979.5000 - val_mae: 1556.5898\n",
      "Epoch 30/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7875027.1000 - mae: 1515.0078 - val_loss: 7895422.0000 - val_mae: 1540.3740\n",
      "Epoch 31/256\n",
      "64/64 [==============================] - 0s 982us/step - loss: 7722425.8692 - mae: 1502.6734 - val_loss: 7761005.0000 - val_mae: 1525.9629\n",
      "Epoch 32/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7383097.8538 - mae: 1478.8168 - val_loss: 7635289.5000 - val_mae: 1507.7349\n",
      "Epoch 33/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7482544.7308 - mae: 1468.8930 - val_loss: 7514041.5000 - val_mae: 1492.7543\n",
      "Epoch 34/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7369656.8462 - mae: 1464.9080 - val_loss: 7396748.0000 - val_mae: 1477.7893\n",
      "Epoch 35/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7065280.8923 - mae: 1432.9715 - val_loss: 7285556.5000 - val_mae: 1461.2928\n",
      "Epoch 36/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7208201.3769 - mae: 1431.5390 - val_loss: 7175912.0000 - val_mae: 1450.5862\n",
      "Epoch 37/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7008084.4154 - mae: 1418.4335 - val_loss: 7072216.5000 - val_mae: 1439.2363\n",
      "Epoch 38/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7143297.0000 - mae: 1407.3985 - val_loss: 6969850.5000 - val_mae: 1428.5312\n",
      "Epoch 39/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6879298.3692 - mae: 1402.4755 - val_loss: 6866330.0000 - val_mae: 1413.3259\n",
      "Epoch 40/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6801829.3769 - mae: 1374.5600 - val_loss: 6772812.5000 - val_mae: 1402.3348\n",
      "Epoch 41/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6381019.1000 - mae: 1345.5485 - val_loss: 6678955.5000 - val_mae: 1390.7954\n",
      "Epoch 42/256\n",
      "64/64 [==============================] - 0s 989us/step - loss: 6407520.5923 - mae: 1345.5677 - val_loss: 6586493.5000 - val_mae: 1379.6044\n",
      "Epoch 43/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6547958.5154 - mae: 1363.4099 - val_loss: 6496117.0000 - val_mae: 1366.8409\n",
      "Epoch 44/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6357700.3769 - mae: 1326.3476 - val_loss: 6411083.0000 - val_mae: 1356.4121\n",
      "Epoch 45/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6041940.1923 - mae: 1305.3092 - val_loss: 6326558.0000 - val_mae: 1347.8702\n",
      "Epoch 46/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6156051.5462 - mae: 1311.1340 - val_loss: 6243905.0000 - val_mae: 1337.1628\n",
      "Epoch 47/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6255729.8769 - mae: 1310.8371 - val_loss: 6163197.5000 - val_mae: 1320.6208\n",
      "Epoch 48/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5894386.6154 - mae: 1281.9479 - val_loss: 6089606.5000 - val_mae: 1313.0095\n",
      "Epoch 49/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5968246.5846 - mae: 1285.8563 - val_loss: 6015034.5000 - val_mae: 1306.7595\n",
      "Epoch 50/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5937216.4385 - mae: 1273.4769 - val_loss: 5938957.5000 - val_mae: 1289.4530\n",
      "Epoch 51/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5629649.0923 - mae: 1239.2002 - val_loss: 5873512.5000 - val_mae: 1283.8020\n",
      "Epoch 52/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5813693.4462 - mae: 1251.3706 - val_loss: 5794623.5000 - val_mae: 1272.6531\n",
      "Epoch 53/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5521100.6000 - mae: 1227.1222 - val_loss: 5726854.0000 - val_mae: 1265.9045\n",
      "Epoch 54/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5547902.1000 - mae: 1230.7117 - val_loss: 5651687.5000 - val_mae: 1247.9978\n",
      "Epoch 55/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5501814.0846 - mae: 1218.4923 - val_loss: 5585434.5000 - val_mae: 1245.8485\n",
      "Epoch 56/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5341691.7308 - mae: 1203.6285 - val_loss: 5522035.0000 - val_mae: 1239.4180\n",
      "Epoch 57/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5456639.6385 - mae: 1209.9378 - val_loss: 5453638.0000 - val_mae: 1222.1620\n",
      "Epoch 58/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5380438.9000 - mae: 1192.6518 - val_loss: 5388652.5000 - val_mae: 1214.7214\n",
      "Epoch 59/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5135317.4692 - mae: 1178.2691 - val_loss: 5323059.5000 - val_mae: 1205.4839\n",
      "Epoch 60/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5221827.4846 - mae: 1169.4536 - val_loss: 5261572.5000 - val_mae: 1200.7847\n",
      "Epoch 61/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5121531.5077 - mae: 1164.6642 - val_loss: 5201559.0000 - val_mae: 1189.0410\n",
      "Epoch 62/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5063166.7154 - mae: 1155.5295 - val_loss: 5141587.5000 - val_mae: 1182.8165\n",
      "Epoch 63/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5034268.6846 - mae: 1141.7460 - val_loss: 5078715.0000 - val_mae: 1173.1188\n",
      "Epoch 64/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4797998.4615 - mae: 1122.5492 - val_loss: 5023416.0000 - val_mae: 1172.6403\n",
      "Epoch 65/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4779676.9308 - mae: 1121.8503 - val_loss: 4963594.0000 - val_mae: 1159.5483\n",
      "Epoch 66/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4740765.0692 - mae: 1127.5197 - val_loss: 4904891.0000 - val_mae: 1154.0474\n",
      "Epoch 67/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4940277.8231 - mae: 1130.8666 - val_loss: 4836868.5000 - val_mae: 1134.3776\n",
      "Epoch 68/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4523274.5385 - mae: 1089.5416 - val_loss: 4787057.5000 - val_mae: 1136.8922\n",
      "Epoch 69/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4607147.1654 - mae: 1099.5036 - val_loss: 4717489.5000 - val_mae: 1121.6484\n",
      "Epoch 70/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4577201.4692 - mae: 1091.0418 - val_loss: 4666264.5000 - val_mae: 1118.2277\n",
      "Epoch 71/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4503447.7308 - mae: 1082.9365 - val_loss: 4610160.0000 - val_mae: 1102.7213\n",
      "Epoch 72/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4511301.3769 - mae: 1076.9115 - val_loss: 4556627.5000 - val_mae: 1096.2787\n",
      "Epoch 73/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4470948.7615 - mae: 1074.3913 - val_loss: 4499477.0000 - val_mae: 1087.7112\n",
      "Epoch 74/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4388649.6269 - mae: 1062.6487 - val_loss: 4444875.0000 - val_mae: 1076.1562\n",
      "Epoch 75/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4238812.2269 - mae: 1045.1889 - val_loss: 4394458.5000 - val_mae: 1080.0164\n",
      "Epoch 76/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4285778.2192 - mae: 1058.3718 - val_loss: 4338705.0000 - val_mae: 1066.5518\n",
      "Epoch 77/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4145606.5538 - mae: 1027.8369 - val_loss: 4288400.5000 - val_mae: 1060.3668\n",
      "Epoch 78/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4180906.5654 - mae: 1035.2993 - val_loss: 4239157.0000 - val_mae: 1051.3683\n",
      "Epoch 79/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4193936.7346 - mae: 1028.4674 - val_loss: 4195880.5000 - val_mae: 1041.8732\n",
      "Epoch 80/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4036980.2577 - mae: 1015.0680 - val_loss: 4157078.2500 - val_mae: 1047.1044\n",
      "Epoch 81/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3972722.4577 - mae: 1012.3621 - val_loss: 4108553.5000 - val_mae: 1038.3982\n",
      "Epoch 82/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3905912.8731 - mae: 999.4702 - val_loss: 4061578.0000 - val_mae: 1024.7343\n",
      "Epoch 83/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3838595.8000 - mae: 981.4204 - val_loss: 4019240.5000 - val_mae: 1022.2474\n",
      "Epoch 84/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3869332.9808 - mae: 992.0687 - val_loss: 3976931.2500 - val_mae: 1011.0225\n",
      "Epoch 85/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3923071.7154 - mae: 1011.2018 - val_loss: 3939021.0000 - val_mae: 1010.3544\n",
      "Epoch 86/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3816865.9308 - mae: 995.2494 - val_loss: 3912709.5000 - val_mae: 1010.8446\n",
      "Epoch 87/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3786236.3000 - mae: 972.7211 - val_loss: 3877333.0000 - val_mae: 1007.4002\n",
      "Epoch 88/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3775182.7577 - mae: 980.0925 - val_loss: 3830216.0000 - val_mae: 991.0871\n",
      "Epoch 89/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3662824.2654 - mae: 959.8135 - val_loss: 3794344.2500 - val_mae: 986.7067\n",
      "Epoch 90/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3598361.8308 - mae: 957.5483 - val_loss: 3770828.2500 - val_mae: 985.8439\n",
      "Epoch 91/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3550905.3731 - mae: 946.9410 - val_loss: 3735300.5000 - val_mae: 977.0092\n",
      "Epoch 92/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3601046.4692 - mae: 954.7564 - val_loss: 3706691.7500 - val_mae: 974.5385\n",
      "Epoch 93/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3547868.2808 - mae: 948.0959 - val_loss: 3676336.2500 - val_mae: 964.9977\n",
      "Epoch 94/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3533143.8231 - mae: 940.7722 - val_loss: 3657195.0000 - val_mae: 962.1110\n",
      "Epoch 95/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3427984.9731 - mae: 929.3966 - val_loss: 3647649.2500 - val_mae: 970.2389\n",
      "Epoch 96/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3566344.5769 - mae: 957.8449 - val_loss: 3604397.0000 - val_mae: 954.7693\n",
      "Epoch 97/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3502623.5538 - mae: 940.4049 - val_loss: 3595046.5000 - val_mae: 959.8826\n",
      "Epoch 98/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3490289.1192 - mae: 934.6219 - val_loss: 3571344.7500 - val_mae: 955.8624\n",
      "Epoch 99/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3488893.5962 - mae: 932.3573 - val_loss: 3553246.2500 - val_mae: 947.1436\n",
      "Epoch 100/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3466504.7577 - mae: 925.8151 - val_loss: 3540410.7500 - val_mae: 949.0674\n",
      "Epoch 101/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3415304.0846 - mae: 922.8027 - val_loss: 3514539.5000 - val_mae: 935.6063\n",
      "Epoch 102/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3484638.2115 - mae: 919.5340 - val_loss: 3507006.7500 - val_mae: 944.5127\n",
      "Epoch 103/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3440060.8423 - mae: 921.7628 - val_loss: 3488250.5000 - val_mae: 935.5767\n",
      "Epoch 104/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3438120.0462 - mae: 921.3397 - val_loss: 3482502.2500 - val_mae: 936.0273\n",
      "Epoch 105/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3326299.3885 - mae: 914.6193 - val_loss: 3463773.5000 - val_mae: 929.8893\n",
      "Epoch 106/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3400987.0654 - mae: 916.7681 - val_loss: 3451330.7500 - val_mae: 930.4430\n",
      "Epoch 107/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3406247.3192 - mae: 912.8979 - val_loss: 3442414.0000 - val_mae: 922.5792\n",
      "Epoch 108/256\n",
      "64/64 [==============================] - 0s 979us/step - loss: 3513076.5077 - mae: 922.5977 - val_loss: 3434517.5000 - val_mae: 924.0954\n",
      "Epoch 109/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3377453.1500 - mae: 908.0962 - val_loss: 3430381.2500 - val_mae: 929.0504\n",
      "Epoch 110/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3334214.2077 - mae: 905.8609 - val_loss: 3420339.0000 - val_mae: 924.0493\n",
      "Epoch 111/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3299196.4538 - mae: 900.3240 - val_loss: 3412042.7500 - val_mae: 924.0706\n",
      "Epoch 112/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3338207.0154 - mae: 900.0559 - val_loss: 3413092.0000 - val_mae: 925.3583\n",
      "Epoch 113/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3215437.2385 - mae: 895.1174 - val_loss: 3401264.7500 - val_mae: 920.8338\n",
      "Epoch 114/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3172706.0269 - mae: 886.7538 - val_loss: 3394842.2500 - val_mae: 918.5010\n",
      "Epoch 115/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3277651.3577 - mae: 898.4037 - val_loss: 3397222.2500 - val_mae: 922.9457\n",
      "Epoch 116/256\n",
      "64/64 [==============================] - 0s 981us/step - loss: 3312922.6808 - mae: 907.4010 - val_loss: 3394846.2500 - val_mae: 924.8115\n",
      "Epoch 117/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3259513.4846 - mae: 899.7219 - val_loss: 3393096.0000 - val_mae: 925.4033\n",
      "Epoch 118/256\n",
      "64/64 [==============================] - 0s 950us/step - loss: 3365542.2692 - mae: 910.0959 - val_loss: 3402088.7500 - val_mae: 935.4879\n",
      "Epoch 119/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3320142.3231 - mae: 930.0133 - val_loss: 3382129.7500 - val_mae: 924.8691\n",
      "Epoch 120/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3334449.5269 - mae: 907.5659 - val_loss: 3377492.0000 - val_mae: 924.2180\n",
      "Epoch 121/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3248620.0846 - mae: 897.7058 - val_loss: 3399081.0000 - val_mae: 935.3726\n",
      "Epoch 122/256\n",
      "64/64 [==============================] - 0s 995us/step - loss: 3273729.4731 - mae: 908.0195 - val_loss: 3369726.2500 - val_mae: 922.8644\n",
      "Epoch 123/256\n",
      "64/64 [==============================] - 0s 987us/step - loss: 3428321.0308 - mae: 910.4359 - val_loss: 3389834.2500 - val_mae: 938.3490\n",
      "Epoch 124/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3272572.2923 - mae: 910.5242 - val_loss: 3379219.0000 - val_mae: 932.2585\n",
      "Epoch 125/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3404708.7462 - mae: 929.8959 - val_loss: 3371878.7500 - val_mae: 918.2182\n",
      "Epoch 126/256\n",
      "64/64 [==============================] - 0s 971us/step - loss: 3312525.8500 - mae: 908.4056 - val_loss: 3367404.0000 - val_mae: 931.9688\n",
      "Epoch 127/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3284786.7538 - mae: 900.8382 - val_loss: 3371932.5000 - val_mae: 929.0686\n",
      "Epoch 128/256\n",
      "64/64 [==============================] - 0s 998us/step - loss: 3292129.5923 - mae: 907.4036 - val_loss: 3360185.2500 - val_mae: 921.8574\n",
      "Epoch 129/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3261900.2385 - mae: 903.8678 - val_loss: 3367999.5000 - val_mae: 934.1071\n",
      "Epoch 130/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3237940.1308 - mae: 905.7637 - val_loss: 3366384.7500 - val_mae: 927.8401\n",
      "Epoch 131/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3213973.3846 - mae: 894.7469 - val_loss: 3355385.2500 - val_mae: 924.6813\n",
      "Epoch 132/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3167109.2423 - mae: 899.7617 - val_loss: 3358978.0000 - val_mae: 921.5804\n",
      "Epoch 133/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3308305.6192 - mae: 908.3352 - val_loss: 3366096.0000 - val_mae: 931.0801\n",
      "Epoch 134/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3143013.3154 - mae: 899.8196 - val_loss: 3353675.5000 - val_mae: 927.9737\n",
      "Epoch 135/256\n",
      "64/64 [==============================] - 0s 999us/step - loss: 3301293.0462 - mae: 910.0957 - val_loss: 3358310.5000 - val_mae: 927.1943\n",
      "Epoch 136/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3236719.4577 - mae: 919.5707 - val_loss: 3365522.0000 - val_mae: 926.4709\n",
      "Epoch 137/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3166050.5269 - mae: 897.9276 - val_loss: 3351066.2500 - val_mae: 930.7200\n",
      "Epoch 138/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3302552.7923 - mae: 917.7732 - val_loss: 3346976.7500 - val_mae: 924.2053\n",
      "Epoch 139/256\n",
      "64/64 [==============================] - 0s 995us/step - loss: 3269715.5385 - mae: 912.4512 - val_loss: 3347201.5000 - val_mae: 932.7671\n",
      "Epoch 140/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3336717.7231 - mae: 913.3831 - val_loss: 3353794.0000 - val_mae: 935.2678\n",
      "Epoch 141/256\n",
      "64/64 [==============================] - 0s 979us/step - loss: 3279855.9423 - mae: 917.8761 - val_loss: 3359953.0000 - val_mae: 934.2452\n",
      "Epoch 142/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3374074.9269 - mae: 917.2694 - val_loss: 3342683.0000 - val_mae: 924.4670\n",
      "Epoch 143/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3231531.4231 - mae: 905.0860 - val_loss: 3349975.7500 - val_mae: 934.5511\n",
      "Epoch 144/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3170934.7308 - mae: 905.7492 - val_loss: 3354961.2500 - val_mae: 927.1110\n",
      "Epoch 145/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3184796.3769 - mae: 902.0595 - val_loss: 3349773.0000 - val_mae: 930.6688\n",
      "Epoch 146/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3369722.6231 - mae: 911.3285 - val_loss: 3375059.5000 - val_mae: 941.5027\n",
      "Epoch 147/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3293203.5192 - mae: 921.0640 - val_loss: 3338645.0000 - val_mae: 935.8260\n",
      "Epoch 148/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3174921.3192 - mae: 916.5077 - val_loss: 3367822.5000 - val_mae: 946.1388\n",
      "Epoch 149/256\n",
      "64/64 [==============================] - 0s 967us/step - loss: 3382050.6962 - mae: 925.9955 - val_loss: 3375248.7500 - val_mae: 949.4934\n",
      "Epoch 150/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3274780.0962 - mae: 912.8242 - val_loss: 3337485.5000 - val_mae: 930.4699\n",
      "Epoch 151/256\n",
      "64/64 [==============================] - 0s 973us/step - loss: 3188128.1962 - mae: 902.8101 - val_loss: 3337820.2500 - val_mae: 936.2762\n",
      "Epoch 152/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3227316.1423 - mae: 909.6940 - val_loss: 3338924.2500 - val_mae: 929.1060\n",
      "Epoch 153/256\n",
      "64/64 [==============================] - 0s 983us/step - loss: 3142109.9654 - mae: 904.1367 - val_loss: 3368750.0000 - val_mae: 950.5068\n",
      "Epoch 154/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3200577.5731 - mae: 927.6277 - val_loss: 3372223.2500 - val_mae: 949.8745\n",
      "Epoch 155/256\n",
      "64/64 [==============================] - 0s 970us/step - loss: 3270622.8462 - mae: 919.4356 - val_loss: 3358617.5000 - val_mae: 946.2384\n",
      "Epoch 156/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3240434.5000 - mae: 919.7187 - val_loss: 3340072.0000 - val_mae: 940.4786\n",
      "Epoch 157/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3180087.7692 - mae: 913.1314 - val_loss: 3440509.2500 - val_mae: 973.5822\n",
      "Epoch 158/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3419630.1038 - mae: 961.7466 - val_loss: 3327622.7500 - val_mae: 933.2188\n",
      "Epoch 159/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3187605.9192 - mae: 911.4990 - val_loss: 3352101.5000 - val_mae: 937.8688\n",
      "Epoch 160/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3173028.3846 - mae: 914.9310 - val_loss: 3328055.0000 - val_mae: 934.5269\n",
      "Epoch 161/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3167021.8769 - mae: 915.5676 - val_loss: 3343488.7500 - val_mae: 936.5175\n",
      "Epoch 162/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3232410.3231 - mae: 919.2270 - val_loss: 3330729.0000 - val_mae: 940.0557\n",
      "Epoch 163/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3351362.0923 - mae: 923.0536 - val_loss: 3338865.0000 - val_mae: 947.8613\n",
      "Epoch 164/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3144955.7385 - mae: 913.4337 - val_loss: 3355953.7500 - val_mae: 952.9633\n",
      "Epoch 165/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3312726.4038 - mae: 938.3972 - val_loss: 3324882.7500 - val_mae: 941.0260\n",
      "Epoch 166/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3200927.4231 - mae: 919.0167 - val_loss: 3325865.7500 - val_mae: 938.9880\n",
      "Epoch 167/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3300053.6385 - mae: 929.9559 - val_loss: 3329566.2500 - val_mae: 943.9719\n",
      "Epoch 168/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3315504.2538 - mae: 927.0099 - val_loss: 3380718.0000 - val_mae: 960.0058\n",
      "Epoch 169/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3309915.6962 - mae: 926.2687 - val_loss: 3345843.0000 - val_mae: 952.9608\n",
      "Epoch 170/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3268175.8385 - mae: 925.9284 - val_loss: 3330850.7500 - val_mae: 945.2711\n",
      "Epoch 171/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3301150.6692 - mae: 942.5134 - val_loss: 3328670.5000 - val_mae: 937.9047\n",
      "Epoch 172/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3228122.5692 - mae: 922.5776 - val_loss: 3340952.5000 - val_mae: 942.9703\n",
      "Epoch 173/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3171750.2462 - mae: 915.5774 - val_loss: 3359538.5000 - val_mae: 961.1005\n",
      "Epoch 174/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3385360.9577 - mae: 948.8008 - val_loss: 3320774.0000 - val_mae: 941.7496\n",
      "Epoch 175/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3136639.7538 - mae: 914.0041 - val_loss: 3319323.0000 - val_mae: 947.4352\n",
      "Epoch 176/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3280404.6346 - mae: 940.1082 - val_loss: 3363221.0000 - val_mae: 960.7718\n",
      "Epoch 177/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3467022.7962 - mae: 947.5449 - val_loss: 3315048.0000 - val_mae: 941.4059\n",
      "Epoch 178/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3300767.5385 - mae: 938.0804 - val_loss: 3319113.5000 - val_mae: 945.7247\n",
      "Epoch 179/256\n",
      "64/64 [==============================] - 0s 988us/step - loss: 3190913.3577 - mae: 926.6615 - val_loss: 3317826.2500 - val_mae: 947.1665\n",
      "Epoch 180/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3259030.7423 - mae: 933.5033 - val_loss: 3319794.7500 - val_mae: 948.1005\n",
      "Epoch 181/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3114778.7692 - mae: 915.9456 - val_loss: 3323804.7500 - val_mae: 949.7276\n",
      "Epoch 182/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3311949.1231 - mae: 930.1846 - val_loss: 3323307.7500 - val_mae: 938.2861\n",
      "Epoch 183/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3176711.8846 - mae: 921.4684 - val_loss: 3317732.2500 - val_mae: 951.1399\n",
      "Epoch 184/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3263818.6923 - mae: 931.0629 - val_loss: 3312531.2500 - val_mae: 946.2756\n",
      "Epoch 185/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3237051.3192 - mae: 926.4469 - val_loss: 3319469.7500 - val_mae: 952.5439\n",
      "Epoch 186/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3276182.0500 - mae: 931.9236 - val_loss: 3309815.0000 - val_mae: 946.7825\n",
      "Epoch 187/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3324442.4538 - mae: 939.0056 - val_loss: 3307489.0000 - val_mae: 947.8691\n",
      "Epoch 188/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3124205.1269 - mae: 922.2066 - val_loss: 3315312.2500 - val_mae: 956.6168\n",
      "Epoch 189/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3310171.7462 - mae: 946.0576 - val_loss: 3312578.5000 - val_mae: 945.4101\n",
      "Epoch 190/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3287431.5385 - mae: 943.5949 - val_loss: 3349427.0000 - val_mae: 950.9725\n",
      "Epoch 191/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3218948.8923 - mae: 936.3615 - val_loss: 3324435.5000 - val_mae: 959.0342\n",
      "Epoch 192/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3227314.0077 - mae: 938.6551 - val_loss: 3402314.7500 - val_mae: 982.1840\n",
      "Epoch 193/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3207945.9346 - mae: 930.9365 - val_loss: 3481436.5000 - val_mae: 1000.1291\n",
      "Epoch 194/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3479628.8615 - mae: 988.2720 - val_loss: 3329692.0000 - val_mae: 960.8779\n",
      "Epoch 195/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3310609.2500 - mae: 949.8845 - val_loss: 3334880.7500 - val_mae: 965.7516\n",
      "Epoch 196/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3167925.8038 - mae: 935.3136 - val_loss: 3303675.2500 - val_mae: 950.6664\n",
      "Epoch 197/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3231853.8346 - mae: 939.1473 - val_loss: 3346315.7500 - val_mae: 968.7996\n",
      "Epoch 198/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3346807.7308 - mae: 950.3081 - val_loss: 3307780.2500 - val_mae: 957.1533\n",
      "Epoch 199/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3255808.1192 - mae: 937.6986 - val_loss: 3304306.7500 - val_mae: 960.9896\n",
      "Epoch 200/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3122183.6885 - mae: 932.7335 - val_loss: 3304552.2500 - val_mae: 954.4395\n",
      "Epoch 201/256\n",
      "64/64 [==============================] - 0s 994us/step - loss: 3186220.7500 - mae: 935.6077 - val_loss: 3305432.2500 - val_mae: 957.1707\n",
      "Epoch 202/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3251208.1115 - mae: 941.1580 - val_loss: 3326914.2500 - val_mae: 966.4039\n",
      "Epoch 203/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3169263.6962 - mae: 944.6586 - val_loss: 3307904.5000 - val_mae: 959.3473\n",
      "Epoch 204/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3250300.2692 - mae: 940.4003 - val_loss: 3307747.2500 - val_mae: 959.9127\n",
      "Epoch 205/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3191553.5654 - mae: 935.3712 - val_loss: 3348222.2500 - val_mae: 964.7817\n",
      "Epoch 206/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3205326.2923 - mae: 944.4169 - val_loss: 3301404.0000 - val_mae: 956.1496\n",
      "Epoch 207/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3289873.7769 - mae: 947.1855 - val_loss: 3296599.0000 - val_mae: 956.7274\n",
      "Epoch 208/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3222031.5654 - mae: 938.1470 - val_loss: 3304479.5000 - val_mae: 960.4764\n",
      "Epoch 209/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3194295.0000 - mae: 936.1583 - val_loss: 3311340.5000 - val_mae: 966.8890\n",
      "Epoch 210/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3178441.8538 - mae: 937.9062 - val_loss: 3321785.5000 - val_mae: 975.7676\n",
      "Epoch 211/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3522984.4231 - mae: 1004.6835 - val_loss: 3349163.5000 - val_mae: 961.5202\n",
      "Epoch 212/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3344048.1615 - mae: 947.9052 - val_loss: 3315266.2500 - val_mae: 954.7631\n",
      "Epoch 213/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3273177.1154 - mae: 941.3079 - val_loss: 3311698.0000 - val_mae: 957.4304\n",
      "Epoch 214/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3234269.6538 - mae: 934.3363 - val_loss: 3350408.0000 - val_mae: 976.4645\n",
      "Epoch 215/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3285220.3000 - mae: 950.9490 - val_loss: 3325234.2500 - val_mae: 970.2016\n",
      "Epoch 216/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3253797.3115 - mae: 944.5605 - val_loss: 3301978.2500 - val_mae: 961.4760\n",
      "Epoch 217/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3339144.5808 - mae: 955.9636 - val_loss: 3320897.7500 - val_mae: 969.7924\n",
      "Epoch 218/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3424460.0654 - mae: 993.6386 - val_loss: 3302589.7500 - val_mae: 967.0369\n",
      "Epoch 219/256\n",
      "64/64 [==============================] - 0s 998us/step - loss: 3180838.6577 - mae: 943.5520 - val_loss: 3292563.5000 - val_mae: 961.3337\n",
      "Epoch 220/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3209322.1769 - mae: 945.3444 - val_loss: 3296437.0000 - val_mae: 961.3198\n",
      "Epoch 221/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3168084.8846 - mae: 940.3134 - val_loss: 3334385.5000 - val_mae: 970.4563\n",
      "Epoch 222/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3200193.5077 - mae: 949.4353 - val_loss: 3310669.7500 - val_mae: 969.5475\n",
      "Epoch 223/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3299015.4769 - mae: 952.1521 - val_loss: 3314483.5000 - val_mae: 958.5641\n",
      "Epoch 224/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3257542.0038 - mae: 948.2528 - val_loss: 3291630.2500 - val_mae: 962.2451\n",
      "Epoch 225/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3319886.5192 - mae: 950.2211 - val_loss: 3296595.5000 - val_mae: 967.1119\n",
      "Epoch 226/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3138481.6077 - mae: 940.1970 - val_loss: 3304974.5000 - val_mae: 969.0244\n",
      "Epoch 227/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3187917.0731 - mae: 939.8020 - val_loss: 3323304.2500 - val_mae: 974.4915\n",
      "Epoch 228/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3259685.0346 - mae: 946.5874 - val_loss: 3311936.2500 - val_mae: 973.2095\n",
      "Epoch 229/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3219063.7462 - mae: 946.3481 - val_loss: 3298597.0000 - val_mae: 969.0323\n",
      "Epoch 230/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3198506.7346 - mae: 947.7225 - val_loss: 3317565.5000 - val_mae: 975.1131\n",
      "Epoch 231/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3214105.8462 - mae: 950.0112 - val_loss: 3329690.2500 - val_mae: 976.2665\n",
      "Epoch 232/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3258610.7885 - mae: 946.9805 - val_loss: 3304899.7500 - val_mae: 973.9299\n",
      "Epoch 233/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3128755.7885 - mae: 951.5905 - val_loss: 3298172.7500 - val_mae: 971.6915\n",
      "Epoch 234/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3176303.8731 - mae: 950.1776 - val_loss: 3291678.5000 - val_mae: 962.6489\n",
      "Epoch 235/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3175758.8731 - mae: 946.4866 - val_loss: 3291587.5000 - val_mae: 962.4376\n",
      "Epoch 236/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3255875.8769 - mae: 958.2894 - val_loss: 3285712.7500 - val_mae: 963.9103\n",
      "Epoch 237/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3245828.3423 - mae: 952.5186 - val_loss: 3284999.7500 - val_mae: 965.7153\n",
      "Epoch 238/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3246945.8038 - mae: 950.5192 - val_loss: 3295407.5000 - val_mae: 971.6804\n",
      "Epoch 239/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3149056.7115 - mae: 951.2929 - val_loss: 3313132.0000 - val_mae: 979.0989\n",
      "Epoch 240/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3155320.6846 - mae: 946.0069 - val_loss: 3302974.0000 - val_mae: 973.7543\n",
      "Epoch 241/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3259521.5346 - mae: 956.0517 - val_loss: 3283152.7500 - val_mae: 968.3074\n",
      "Epoch 242/256\n",
      "64/64 [==============================] - 0s 988us/step - loss: 3164399.4500 - mae: 945.2626 - val_loss: 3296416.0000 - val_mae: 975.5994\n",
      "Epoch 243/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3159308.3192 - mae: 949.9608 - val_loss: 3296055.5000 - val_mae: 980.6685\n",
      "Epoch 244/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3175660.3923 - mae: 955.4240 - val_loss: 3281724.0000 - val_mae: 966.3940\n",
      "Epoch 245/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3207620.3808 - mae: 954.1298 - val_loss: 3322974.7500 - val_mae: 963.2657\n",
      "Epoch 246/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3295209.5346 - mae: 963.8221 - val_loss: 3289425.0000 - val_mae: 972.9911\n",
      "Epoch 247/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3269726.1038 - mae: 960.9628 - val_loss: 3286329.0000 - val_mae: 971.4688\n",
      "Epoch 248/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3191908.0462 - mae: 951.2797 - val_loss: 3304748.7500 - val_mae: 974.2327\n",
      "Epoch 249/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3129653.8423 - mae: 945.4305 - val_loss: 3293144.2500 - val_mae: 963.9588\n",
      "Epoch 250/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3167819.7769 - mae: 948.6087 - val_loss: 3298058.0000 - val_mae: 974.9500\n",
      "Epoch 251/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3311401.6538 - mae: 968.2493 - val_loss: 3284206.2500 - val_mae: 965.1858\n",
      "Epoch 252/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3284332.7192 - mae: 959.2174 - val_loss: 3296646.7500 - val_mae: 977.3024\n",
      "Epoch 253/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3280904.3885 - mae: 956.8547 - val_loss: 3338950.5000 - val_mae: 987.1240\n",
      "Epoch 254/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3185187.1192 - mae: 960.0365 - val_loss: 3280374.5000 - val_mae: 970.4633\n",
      "Epoch 255/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3119677.5692 - mae: 941.7571 - val_loss: 3282911.0000 - val_mae: 971.7084\n",
      "Epoch 256/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3206782.1115 - mae: 954.9316 - val_loss: 3281334.7500 - val_mae: 968.0921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbbe8ad9290>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras import Model\n",
    "from datetime import datetime\n",
    "\n",
    "weights = tf.constant(df.weight.to_numpy(dtype=np.float32))\n",
    "\n",
    "inp = Input(shape=(36,))\n",
    "out = Dense(1, activation=\"linear\")(inp)\n",
    "\n",
    "logdir = \"logs/scalars/linear-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X[train], Y[train], validation_data=(X[val], Y[val]), batch_size=1024, epochs=256, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "378a3e23-6fed-4fe1-94e5-f39fe3e56648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 23968614.5846 - mae: 2986.3195 - val_loss: 7516345.5000 - val_mae: 1612.8802\n",
      "Epoch 2/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 6940883.7923 - mae: 1457.1859 - val_loss: 4325323.0000 - val_mae: 1104.1128\n",
      "Epoch 3/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4306144.9231 - mae: 1048.5107 - val_loss: 3700195.7500 - val_mae: 990.7369\n",
      "Epoch 4/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3777230.6308 - mae: 942.0306 - val_loss: 3158589.5000 - val_mae: 862.9087\n",
      "Epoch 5/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3215084.2923 - mae: 845.4916 - val_loss: 2977763.5000 - val_mae: 825.4895\n",
      "Epoch 6/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2993547.1615 - mae: 792.1166 - val_loss: 2725041.5000 - val_mae: 758.2440\n",
      "Epoch 7/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2799580.5846 - mae: 753.9175 - val_loss: 2469576.7500 - val_mae: 702.4161\n",
      "Epoch 8/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2467045.7019 - mae: 693.6917 - val_loss: 2599188.7500 - val_mae: 738.9159\n",
      "Epoch 9/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2495470.8269 - mae: 714.1494 - val_loss: 2360716.0000 - val_mae: 673.7520\n",
      "Epoch 10/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2285447.6769 - mae: 653.2290 - val_loss: 2316287.2500 - val_mae: 678.1525\n",
      "Epoch 11/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2297084.5923 - mae: 670.1901 - val_loss: 2240747.0000 - val_mae: 661.1271\n",
      "Epoch 12/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2160941.5654 - mae: 639.5169 - val_loss: 2304738.0000 - val_mae: 684.4112\n",
      "Epoch 13/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2229689.6346 - mae: 655.0846 - val_loss: 2191210.5000 - val_mae: 635.1340\n",
      "Epoch 14/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2156214.1885 - mae: 635.4234 - val_loss: 2217941.7500 - val_mae: 637.4244\n",
      "Epoch 15/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2189998.6346 - mae: 629.7673 - val_loss: 2189300.7500 - val_mae: 636.9565\n",
      "Epoch 16/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2079044.0673 - mae: 617.0146 - val_loss: 2151532.7500 - val_mae: 620.5081\n",
      "Epoch 17/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2144636.5058 - mae: 627.0321 - val_loss: 2123956.7500 - val_mae: 616.8163\n",
      "Epoch 18/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2174092.0558 - mae: 615.7439 - val_loss: 2141512.0000 - val_mae: 616.4938\n",
      "Epoch 19/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2178565.5596 - mae: 621.8677 - val_loss: 2266758.5000 - val_mae: 679.9714\n",
      "Epoch 20/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2388564.6346 - mae: 749.7729 - val_loss: 2157771.2500 - val_mae: 639.9207\n",
      "Epoch 21/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2098027.2077 - mae: 622.2467 - val_loss: 2130492.5000 - val_mae: 613.7079\n",
      "Epoch 22/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2043677.5000 - mae: 600.2325 - val_loss: 2115908.5000 - val_mae: 608.8483\n",
      "Epoch 23/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2037299.7654 - mae: 594.1697 - val_loss: 2135073.5000 - val_mae: 611.3055\n",
      "Epoch 24/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1987414.3269 - mae: 588.9931 - val_loss: 2138774.7500 - val_mae: 611.9526\n",
      "Epoch 25/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2081044.8692 - mae: 603.5268 - val_loss: 2092939.5000 - val_mae: 596.3580\n",
      "Epoch 26/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2029249.3058 - mae: 583.7195 - val_loss: 2118576.2500 - val_mae: 615.6255\n",
      "Epoch 27/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2022357.9231 - mae: 603.3612 - val_loss: 2693115.7500 - val_mae: 739.6074\n",
      "Epoch 28/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2482230.7615 - mae: 693.3548 - val_loss: 2206506.7500 - val_mae: 630.7281\n",
      "Epoch 29/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2182200.6942 - mae: 614.1549 - val_loss: 2086714.8750 - val_mae: 599.6439\n",
      "Epoch 30/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2028822.2654 - mae: 593.2721 - val_loss: 2275847.0000 - val_mae: 651.6617\n",
      "Epoch 31/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2090816.2962 - mae: 611.5115 - val_loss: 2332990.0000 - val_mae: 652.5051\n",
      "Epoch 32/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2013528.2538 - mae: 600.1240 - val_loss: 2073106.6250 - val_mae: 596.9418\n",
      "Epoch 33/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2042969.3096 - mae: 589.0816 - val_loss: 2123122.2500 - val_mae: 606.9092\n",
      "Epoch 34/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2021447.9673 - mae: 588.2070 - val_loss: 2224591.5000 - val_mae: 623.8171\n",
      "Epoch 35/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2088456.2885 - mae: 617.7533 - val_loss: 2096642.7500 - val_mae: 628.6743\n",
      "Epoch 36/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2037673.3385 - mae: 606.9742 - val_loss: 2075886.1250 - val_mae: 612.7036\n",
      "Epoch 37/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2031948.3058 - mae: 595.2411 - val_loss: 2070608.1250 - val_mae: 590.8561\n",
      "Epoch 38/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2039607.3692 - mae: 598.6394 - val_loss: 2064335.7500 - val_mae: 592.9169\n",
      "Epoch 39/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2031113.4885 - mae: 588.6993 - val_loss: 2081939.8750 - val_mae: 594.3839\n",
      "Epoch 40/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1979980.1615 - mae: 576.8329 - val_loss: 2041424.7500 - val_mae: 592.3373\n",
      "Epoch 41/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1995833.3038 - mae: 594.8743 - val_loss: 2045501.0000 - val_mae: 586.0954\n",
      "Epoch 42/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2069243.1673 - mae: 592.8025 - val_loss: 2053115.0000 - val_mae: 590.8995\n",
      "Epoch 43/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1989828.2019 - mae: 580.1354 - val_loss: 2133332.0000 - val_mae: 636.4352\n",
      "Epoch 44/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2024970.2808 - mae: 597.0710 - val_loss: 2031513.6250 - val_mae: 586.1504\n",
      "Epoch 45/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1992669.8712 - mae: 589.0015 - val_loss: 2038880.1250 - val_mae: 594.9835\n",
      "Epoch 46/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1987176.8827 - mae: 581.2741 - val_loss: 2068060.8750 - val_mae: 595.9435\n",
      "Epoch 47/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2003449.0596 - mae: 584.8616 - val_loss: 2032355.2500 - val_mae: 595.1464\n",
      "Epoch 48/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1934080.1135 - mae: 587.5136 - val_loss: 2100917.7500 - val_mae: 600.2174\n",
      "Epoch 49/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2065359.5596 - mae: 588.7629 - val_loss: 2037111.6250 - val_mae: 590.5295\n",
      "Epoch 50/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1974500.2981 - mae: 591.9114 - val_loss: 2085487.6250 - val_mae: 619.2198\n",
      "Epoch 51/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1995928.8712 - mae: 594.1752 - val_loss: 2041989.7500 - val_mae: 590.8085\n",
      "Epoch 52/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1954494.7500 - mae: 583.4973 - val_loss: 2096147.0000 - val_mae: 610.0421\n",
      "Epoch 53/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1910187.6442 - mae: 581.2208 - val_loss: 2085252.1250 - val_mae: 601.4281\n",
      "Epoch 54/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2023046.3923 - mae: 603.4353 - val_loss: 2020497.8750 - val_mae: 587.5861\n",
      "Epoch 55/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1958894.5038 - mae: 582.0581 - val_loss: 3084686.0000 - val_mae: 817.1791\n",
      "Epoch 56/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2368279.4038 - mae: 685.4568 - val_loss: 2157164.7500 - val_mae: 637.2579\n",
      "Epoch 57/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2085748.4673 - mae: 619.3323 - val_loss: 2012217.6250 - val_mae: 598.6175\n",
      "Epoch 58/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1956557.3635 - mae: 584.7903 - val_loss: 2021325.5000 - val_mae: 594.2789\n",
      "Epoch 59/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2006052.2731 - mae: 588.4722 - val_loss: 2073412.3750 - val_mae: 612.0231\n",
      "Epoch 60/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1993288.2846 - mae: 588.5668 - val_loss: 2013912.3750 - val_mae: 608.1221\n",
      "Epoch 61/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1935982.8750 - mae: 583.3753 - val_loss: 2086475.5000 - val_mae: 621.4572\n",
      "Epoch 62/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1997936.2423 - mae: 589.6988 - val_loss: 2233272.7500 - val_mae: 678.8066\n",
      "Epoch 63/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2159748.9712 - mae: 640.3189 - val_loss: 2021577.0000 - val_mae: 594.5745\n",
      "Epoch 64/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2008141.8538 - mae: 596.7368 - val_loss: 2005617.3750 - val_mae: 584.4317\n",
      "Epoch 65/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1924029.4615 - mae: 571.2661 - val_loss: 2142895.2500 - val_mae: 648.2142\n",
      "Epoch 66/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2082940.2769 - mae: 612.0428 - val_loss: 2142723.5000 - val_mae: 619.9589\n",
      "Epoch 67/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1980212.7212 - mae: 585.0638 - val_loss: 2492445.0000 - val_mae: 715.5331\n",
      "Epoch 68/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2193026.2923 - mae: 633.5675 - val_loss: 2062343.6250 - val_mae: 597.7432\n",
      "Epoch 69/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1957855.9827 - mae: 586.3941 - val_loss: 2026008.0000 - val_mae: 592.9256\n",
      "Epoch 70/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1967586.2904 - mae: 578.1958 - val_loss: 2029751.6250 - val_mae: 597.2512\n",
      "Epoch 71/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1955197.6596 - mae: 588.4641 - val_loss: 2057442.3750 - val_mae: 615.6510\n",
      "Epoch 72/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1974698.9538 - mae: 594.4105 - val_loss: 2021463.6250 - val_mae: 602.0333\n",
      "Epoch 73/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1949547.1308 - mae: 587.5213 - val_loss: 2016900.2500 - val_mae: 600.0260\n",
      "Epoch 74/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1970733.0538 - mae: 585.6741 - val_loss: 2215139.0000 - val_mae: 653.9857\n",
      "Epoch 75/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2084936.4096 - mae: 619.8607 - val_loss: 2156160.5000 - val_mae: 632.6976\n",
      "Epoch 76/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1930338.4058 - mae: 591.5214 - val_loss: 2011470.7500 - val_mae: 590.0504\n",
      "Epoch 77/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1909220.0558 - mae: 578.3745 - val_loss: 2167453.7500 - val_mae: 618.3364\n",
      "Epoch 78/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2032176.6404 - mae: 599.2835 - val_loss: 2011657.5000 - val_mae: 600.1067\n",
      "Epoch 79/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1956826.0096 - mae: 588.0289 - val_loss: 2004596.0000 - val_mae: 585.5292\n",
      "Epoch 80/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2052383.8673 - mae: 599.7377 - val_loss: 2060613.6250 - val_mae: 615.1584\n",
      "Epoch 81/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1955575.4904 - mae: 597.4126 - val_loss: 2028569.3750 - val_mae: 596.6550\n",
      "Epoch 82/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1928799.6173 - mae: 593.3136 - val_loss: 1991250.7500 - val_mae: 581.8195\n",
      "Epoch 83/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1970232.5750 - mae: 585.4409 - val_loss: 1995834.2500 - val_mae: 603.2257\n",
      "Epoch 84/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1928694.9365 - mae: 581.3600 - val_loss: 2038402.2500 - val_mae: 622.7629\n",
      "Epoch 85/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1879981.7538 - mae: 574.4248 - val_loss: 2156000.7500 - val_mae: 638.3724\n",
      "Epoch 86/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2137870.5154 - mae: 634.2512 - val_loss: 2105309.7500 - val_mae: 622.6634\n",
      "Epoch 87/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1968752.8231 - mae: 603.5171 - val_loss: 2067185.3750 - val_mae: 603.4293\n",
      "Epoch 88/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1997931.5462 - mae: 591.6691 - val_loss: 2024418.1250 - val_mae: 605.2286\n",
      "Epoch 89/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1991695.1846 - mae: 598.8301 - val_loss: 2125535.2500 - val_mae: 641.8123\n",
      "Epoch 90/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2021714.4115 - mae: 615.1882 - val_loss: 2011371.7500 - val_mae: 605.1898\n",
      "Epoch 91/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1958368.4019 - mae: 603.7313 - val_loss: 2070439.8750 - val_mae: 609.6262\n",
      "Epoch 92/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1957677.6077 - mae: 590.5311 - val_loss: 2047388.7500 - val_mae: 588.1498\n",
      "Epoch 93/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1997182.2385 - mae: 602.2172 - val_loss: 2185882.0000 - val_mae: 650.9750\n",
      "Epoch 94/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2008081.4231 - mae: 603.1118 - val_loss: 2089738.6250 - val_mae: 608.5200\n",
      "Epoch 95/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1961473.0346 - mae: 586.8189 - val_loss: 1986290.5000 - val_mae: 590.6489\n",
      "Epoch 96/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1962534.1442 - mae: 586.7662 - val_loss: 2017676.3750 - val_mae: 613.3419\n",
      "Epoch 97/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1906037.3885 - mae: 584.2163 - val_loss: 1983816.0000 - val_mae: 590.9180\n",
      "Epoch 98/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1879593.5654 - mae: 571.6607 - val_loss: 2076253.3750 - val_mae: 620.9813\n",
      "Epoch 99/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1894003.9942 - mae: 589.7611 - val_loss: 2108525.5000 - val_mae: 631.2695\n",
      "Epoch 100/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2031218.8038 - mae: 601.8061 - val_loss: 1996272.5000 - val_mae: 608.6219\n",
      "Epoch 101/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1939258.0981 - mae: 585.4990 - val_loss: 1996119.8750 - val_mae: 591.1108\n",
      "Epoch 102/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1930088.9769 - mae: 580.2067 - val_loss: 2134965.5000 - val_mae: 662.9438\n",
      "Epoch 103/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1931507.4731 - mae: 599.7822 - val_loss: 1982673.8750 - val_mae: 597.3924\n",
      "Epoch 104/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1910401.8885 - mae: 578.6619 - val_loss: 2015053.5000 - val_mae: 589.4208\n",
      "Epoch 105/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1895486.7288 - mae: 581.9996 - val_loss: 2142047.2500 - val_mae: 629.4617\n",
      "Epoch 106/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2206630.9096 - mae: 652.7826 - val_loss: 1998244.0000 - val_mae: 593.7187\n",
      "Epoch 107/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1882320.9000 - mae: 578.2593 - val_loss: 2107281.5000 - val_mae: 643.3677\n",
      "Epoch 108/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1984665.7788 - mae: 613.5085 - val_loss: 2161989.5000 - val_mae: 640.9481\n",
      "Epoch 109/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2011008.4519 - mae: 615.1387 - val_loss: 2438830.2500 - val_mae: 685.0593\n",
      "Epoch 110/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2040581.6365 - mae: 611.3237 - val_loss: 1993649.3750 - val_mae: 595.8714\n",
      "Epoch 111/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1898496.5385 - mae: 578.7329 - val_loss: 2146604.7500 - val_mae: 626.6335\n",
      "Epoch 112/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1942246.0000 - mae: 591.3336 - val_loss: 1994691.0000 - val_mae: 596.4645\n",
      "Epoch 113/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1910440.8654 - mae: 584.0955 - val_loss: 2010294.5000 - val_mae: 587.6335\n",
      "Epoch 114/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1939962.8212 - mae: 584.6027 - val_loss: 2165888.2500 - val_mae: 658.2664\n",
      "Epoch 115/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2056385.2519 - mae: 635.3622 - val_loss: 1974347.6250 - val_mae: 594.3615\n",
      "Epoch 116/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1969115.7596 - mae: 601.0054 - val_loss: 1977357.7500 - val_mae: 589.1749\n",
      "Epoch 117/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1936405.5808 - mae: 584.9371 - val_loss: 2413122.0000 - val_mae: 743.5551\n",
      "Epoch 118/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2109231.1769 - mae: 649.1184 - val_loss: 2694504.2500 - val_mae: 740.4954\n",
      "Epoch 119/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2268334.8058 - mae: 675.0938 - val_loss: 1997278.2500 - val_mae: 592.9619\n",
      "Epoch 120/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1868225.5885 - mae: 581.0576 - val_loss: 2022328.5000 - val_mae: 601.1355\n",
      "Epoch 121/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1930781.2962 - mae: 582.4568 - val_loss: 1970379.2500 - val_mae: 586.8979\n",
      "Epoch 122/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1880130.1173 - mae: 576.6060 - val_loss: 2092591.7500 - val_mae: 621.6498\n",
      "Epoch 123/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1935656.8000 - mae: 588.5172 - val_loss: 2075690.8750 - val_mae: 633.5139\n",
      "Epoch 124/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1877208.2712 - mae: 587.6480 - val_loss: 1989530.6250 - val_mae: 595.7145\n",
      "Epoch 125/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1907392.1692 - mae: 583.2570 - val_loss: 2031020.1250 - val_mae: 602.6937\n",
      "Epoch 126/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1925750.6462 - mae: 585.5574 - val_loss: 1974282.8750 - val_mae: 588.4602\n",
      "Epoch 127/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1868147.7365 - mae: 578.5962 - val_loss: 2175409.2500 - val_mae: 655.0031\n",
      "Epoch 128/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1981024.9712 - mae: 609.3854 - val_loss: 2301198.5000 - val_mae: 708.2869\n",
      "Epoch 129/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2031261.1788 - mae: 624.8068 - val_loss: 2001998.5000 - val_mae: 591.9036\n",
      "Epoch 130/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1997753.2462 - mae: 602.5234 - val_loss: 1973459.3750 - val_mae: 589.4254\n",
      "Epoch 131/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1890170.2385 - mae: 575.2728 - val_loss: 1989665.3750 - val_mae: 610.5457\n",
      "Epoch 132/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1879720.1115 - mae: 580.6341 - val_loss: 1999665.8750 - val_mae: 608.8942\n",
      "Epoch 133/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1937760.2346 - mae: 591.9997 - val_loss: 1982864.6250 - val_mae: 598.6937\n",
      "Epoch 134/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1866651.5654 - mae: 583.0976 - val_loss: 2008642.5000 - val_mae: 599.8786\n",
      "Epoch 135/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1923541.4558 - mae: 603.3412 - val_loss: 2004466.8750 - val_mae: 594.9375\n",
      "Epoch 136/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1868786.9481 - mae: 581.5198 - val_loss: 2363595.7500 - val_mae: 670.3905\n",
      "Epoch 137/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2162533.5058 - mae: 651.7619 - val_loss: 2497063.2500 - val_mae: 752.3302\n",
      "Epoch 138/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2123770.0135 - mae: 645.7530 - val_loss: 1961812.7500 - val_mae: 596.8333\n",
      "Epoch 139/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1861833.0577 - mae: 573.5436 - val_loss: 1964695.7500 - val_mae: 585.2147\n",
      "Epoch 140/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1878961.3135 - mae: 585.3168 - val_loss: 2122486.7500 - val_mae: 627.1861\n",
      "Epoch 141/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1995101.2000 - mae: 603.8417 - val_loss: 2014834.7500 - val_mae: 596.3235\n",
      "Epoch 142/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1913538.7115 - mae: 581.8028 - val_loss: 2145427.5000 - val_mae: 673.4712\n",
      "Epoch 143/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1939884.9058 - mae: 606.5438 - val_loss: 2006833.5000 - val_mae: 608.9428\n",
      "Epoch 144/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1972481.5962 - mae: 606.4655 - val_loss: 2095946.5000 - val_mae: 661.7325\n",
      "Epoch 145/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1856538.7019 - mae: 591.6524 - val_loss: 1967963.5000 - val_mae: 594.5857\n",
      "Epoch 146/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1875628.1365 - mae: 584.7341 - val_loss: 1957602.7500 - val_mae: 588.7036\n",
      "Epoch 147/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1916003.5288 - mae: 602.5796 - val_loss: 1964952.5000 - val_mae: 594.4498\n",
      "Epoch 148/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1886187.4288 - mae: 588.7148 - val_loss: 2059589.5000 - val_mae: 635.2094\n",
      "Epoch 149/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1874307.8558 - mae: 581.7379 - val_loss: 2226623.2500 - val_mae: 639.7836\n",
      "Epoch 150/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1948547.7385 - mae: 606.4380 - val_loss: 2202303.2500 - val_mae: 640.1271\n",
      "Epoch 151/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1942453.1788 - mae: 585.8721 - val_loss: 1990808.3750 - val_mae: 599.0699\n",
      "Epoch 152/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1910621.2692 - mae: 593.2179 - val_loss: 1999814.6250 - val_mae: 591.5711\n",
      "Epoch 153/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1871089.0808 - mae: 579.7767 - val_loss: 1953425.3750 - val_mae: 584.1507\n",
      "Epoch 154/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1849863.4885 - mae: 582.9732 - val_loss: 1999194.7500 - val_mae: 604.8215\n",
      "Epoch 155/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1876529.7500 - mae: 587.4773 - val_loss: 1957152.5000 - val_mae: 585.8774\n",
      "Epoch 156/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1909610.8404 - mae: 587.2954 - val_loss: 2110527.2500 - val_mae: 639.2137\n",
      "Epoch 157/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1913040.9577 - mae: 590.0429 - val_loss: 2927807.7500 - val_mae: 764.0825\n",
      "Epoch 158/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2316434.9769 - mae: 720.3320 - val_loss: 1986574.8750 - val_mae: 610.3875\n",
      "Epoch 159/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1923773.6635 - mae: 599.8322 - val_loss: 2011339.1250 - val_mae: 597.3927\n",
      "Epoch 160/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1810933.6096 - mae: 567.7981 - val_loss: 2016260.0000 - val_mae: 618.5460\n",
      "Epoch 161/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2114289.0827 - mae: 640.0189 - val_loss: 1973952.8750 - val_mae: 595.7905\n",
      "Epoch 162/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1877844.3538 - mae: 575.4219 - val_loss: 1955584.6250 - val_mae: 592.9120\n",
      "Epoch 163/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1937858.4462 - mae: 597.1834 - val_loss: 1952204.5000 - val_mae: 584.0903\n",
      "Epoch 164/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1826994.3712 - mae: 565.9512 - val_loss: 1965762.6250 - val_mae: 584.1240\n",
      "Epoch 165/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1865197.2654 - mae: 578.7553 - val_loss: 1956851.2500 - val_mae: 593.6096\n",
      "Epoch 166/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1849589.9500 - mae: 568.7771 - val_loss: 1964271.5000 - val_mae: 604.7676\n",
      "Epoch 167/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1881122.2212 - mae: 576.2590 - val_loss: 4443985.0000 - val_mae: 1024.6367\n",
      "Epoch 168/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3473218.5846 - mae: 873.4801 - val_loss: 1972040.2500 - val_mae: 587.7544\n",
      "Epoch 169/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1901319.5654 - mae: 583.1027 - val_loss: 1988694.3750 - val_mae: 606.1891\n",
      "Epoch 170/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1848431.8442 - mae: 581.6668 - val_loss: 1958706.6250 - val_mae: 599.1646\n",
      "Epoch 171/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1849927.0192 - mae: 573.9936 - val_loss: 1945044.6250 - val_mae: 586.2499\n",
      "Epoch 172/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1862377.9558 - mae: 575.4563 - val_loss: 2039055.7500 - val_mae: 620.4187\n",
      "Epoch 173/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2005868.8077 - mae: 624.6956 - val_loss: 2139631.0000 - val_mae: 618.2776\n",
      "Epoch 174/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1942333.8577 - mae: 595.5258 - val_loss: 2018221.8750 - val_mae: 591.7293\n",
      "Epoch 175/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1948655.3346 - mae: 596.3703 - val_loss: 1961764.5000 - val_mae: 606.3926\n",
      "Epoch 176/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1857313.8731 - mae: 575.1964 - val_loss: 1952983.8750 - val_mae: 586.3116\n",
      "Epoch 177/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1887043.9827 - mae: 588.2478 - val_loss: 2035870.8750 - val_mae: 606.2566\n",
      "Epoch 178/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1886387.1288 - mae: 583.4397 - val_loss: 1943279.7500 - val_mae: 600.5788\n",
      "Epoch 179/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1861994.6269 - mae: 580.3337 - val_loss: 1986543.6250 - val_mae: 591.9730\n",
      "Epoch 180/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1839101.3269 - mae: 579.9402 - val_loss: 1943925.6250 - val_mae: 587.8477\n",
      "Epoch 181/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1886632.1654 - mae: 577.4054 - val_loss: 1987142.3750 - val_mae: 610.7280\n",
      "Epoch 182/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1892837.1577 - mae: 594.5919 - val_loss: 1934362.6250 - val_mae: 595.8189\n",
      "Epoch 183/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1894334.2904 - mae: 588.4235 - val_loss: 1961977.0000 - val_mae: 595.4642\n",
      "Epoch 184/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1850557.6865 - mae: 582.9879 - val_loss: 1976807.1250 - val_mae: 609.9604\n",
      "Epoch 185/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1884758.9962 - mae: 592.0062 - val_loss: 1940440.2500 - val_mae: 580.3957\n",
      "Epoch 186/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1799765.2731 - mae: 571.3070 - val_loss: 1972860.5000 - val_mae: 601.0754\n",
      "Epoch 187/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1886955.7769 - mae: 589.5476 - val_loss: 1991768.3750 - val_mae: 607.2491\n",
      "Epoch 188/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1829545.6365 - mae: 580.4250 - val_loss: 1938014.3750 - val_mae: 585.9174\n",
      "Epoch 189/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1818603.1481 - mae: 577.6579 - val_loss: 1946216.8750 - val_mae: 605.6606\n",
      "Epoch 190/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1852154.7154 - mae: 583.1761 - val_loss: 1960838.1250 - val_mae: 611.7906\n",
      "Epoch 191/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1962474.5788 - mae: 588.4377 - val_loss: 2121889.0000 - val_mae: 648.5986\n",
      "Epoch 192/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1940935.5788 - mae: 603.9897 - val_loss: 1951982.8750 - val_mae: 620.1513\n",
      "Epoch 193/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1972419.5173 - mae: 592.6458 - val_loss: 1934245.2500 - val_mae: 600.6734\n",
      "Epoch 194/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1891676.0038 - mae: 583.7519 - val_loss: 2009371.5000 - val_mae: 598.7654\n",
      "Epoch 195/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1825445.3135 - mae: 579.9681 - val_loss: 1937998.5000 - val_mae: 601.3536\n",
      "Epoch 196/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1799228.7288 - mae: 577.2720 - val_loss: 1943353.2500 - val_mae: 602.0168\n",
      "Epoch 197/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1857599.7596 - mae: 587.3146 - val_loss: 1999291.8750 - val_mae: 599.6709\n",
      "Epoch 198/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1867883.0462 - mae: 586.0882 - val_loss: 1945574.7500 - val_mae: 605.5092\n",
      "Epoch 199/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1872433.9846 - mae: 582.4127 - val_loss: 1969317.0000 - val_mae: 608.8837\n",
      "Epoch 200/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1985921.1788 - mae: 622.7454 - val_loss: 2027534.0000 - val_mae: 604.5774\n",
      "Epoch 201/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1862194.8712 - mae: 577.3304 - val_loss: 2048103.7500 - val_mae: 609.9314\n",
      "Epoch 202/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1872669.1731 - mae: 589.9008 - val_loss: 1959469.0000 - val_mae: 612.8041\n",
      "Epoch 203/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1836234.5635 - mae: 587.7605 - val_loss: 2175094.5000 - val_mae: 636.5370\n",
      "Epoch 204/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1914362.5865 - mae: 613.8665 - val_loss: 1955356.0000 - val_mae: 607.7052\n",
      "Epoch 205/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1792196.3615 - mae: 582.5422 - val_loss: 2043488.2500 - val_mae: 609.6209\n",
      "Epoch 206/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1873599.3173 - mae: 616.9027 - val_loss: 3088957.2500 - val_mae: 826.5892\n",
      "Epoch 207/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2461950.0692 - mae: 717.3207 - val_loss: 2050959.0000 - val_mae: 606.0834\n",
      "Epoch 208/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1945062.4000 - mae: 596.7682 - val_loss: 1922359.7500 - val_mae: 597.1025\n",
      "Epoch 209/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1854159.3231 - mae: 581.7264 - val_loss: 2229582.2500 - val_mae: 641.9738\n",
      "Epoch 210/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1981695.3673 - mae: 620.3052 - val_loss: 1937954.5000 - val_mae: 587.7753\n",
      "Epoch 211/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1870880.6019 - mae: 581.6217 - val_loss: 2007158.6250 - val_mae: 602.0864\n",
      "Epoch 212/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1882850.8596 - mae: 589.6518 - val_loss: 1916368.3750 - val_mae: 607.5456\n",
      "Epoch 213/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1825988.8731 - mae: 582.7654 - val_loss: 1912103.8750 - val_mae: 601.2877\n",
      "Epoch 214/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1783990.7750 - mae: 571.6109 - val_loss: 1986997.1250 - val_mae: 642.4129\n",
      "Epoch 215/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1842344.4635 - mae: 595.1233 - val_loss: 1908097.0000 - val_mae: 595.3903\n",
      "Epoch 216/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1840077.9731 - mae: 583.9669 - val_loss: 1937482.5000 - val_mae: 616.3078\n",
      "Epoch 217/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1833633.3423 - mae: 582.4406 - val_loss: 2109522.0000 - val_mae: 615.2393\n",
      "Epoch 218/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1952991.9288 - mae: 603.5903 - val_loss: 1947364.2500 - val_mae: 614.5302\n",
      "Epoch 219/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1835411.3923 - mae: 584.0320 - val_loss: 1986607.0000 - val_mae: 624.5181\n",
      "Epoch 220/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1853811.5481 - mae: 592.9966 - val_loss: 2146360.2500 - val_mae: 626.8821\n",
      "Epoch 221/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1973992.3731 - mae: 607.6114 - val_loss: 1906635.3750 - val_mae: 591.3453\n",
      "Epoch 222/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1846941.4154 - mae: 583.1781 - val_loss: 1897529.6250 - val_mae: 590.9660\n",
      "Epoch 223/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1756203.6538 - mae: 572.3195 - val_loss: 1905691.0000 - val_mae: 588.2211\n",
      "Epoch 224/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1809975.6635 - mae: 584.6516 - val_loss: 1992994.1250 - val_mae: 629.6945\n",
      "Epoch 225/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1894131.3462 - mae: 602.2357 - val_loss: 1914266.8750 - val_mae: 594.2828\n",
      "Epoch 226/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1879482.9846 - mae: 599.1136 - val_loss: 1969753.6250 - val_mae: 644.2089\n",
      "Epoch 227/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1784136.8846 - mae: 581.8504 - val_loss: 1927842.6250 - val_mae: 621.0529\n",
      "Epoch 228/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1834536.4731 - mae: 588.1360 - val_loss: 2388957.5000 - val_mae: 729.7898\n",
      "Epoch 229/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2336348.6712 - mae: 709.5626 - val_loss: 2014920.6250 - val_mae: 603.1784\n",
      "Epoch 230/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1866012.2212 - mae: 589.2373 - val_loss: 2306783.5000 - val_mae: 675.6381\n",
      "Epoch 231/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1954976.1327 - mae: 621.4242 - val_loss: 1926405.3750 - val_mae: 596.7203\n",
      "Epoch 232/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1785702.0712 - mae: 572.9787 - val_loss: 1995326.2500 - val_mae: 594.4136\n",
      "Epoch 233/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1838091.4596 - mae: 583.6998 - val_loss: 1906898.6250 - val_mae: 588.1472\n",
      "Epoch 234/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1787473.2538 - mae: 576.2943 - val_loss: 2444498.5000 - val_mae: 684.2259\n",
      "Epoch 235/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2147223.3519 - mae: 650.2637 - val_loss: 1917591.0000 - val_mae: 619.5982\n",
      "Epoch 236/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1917476.6096 - mae: 600.3636 - val_loss: 2044586.7500 - val_mae: 640.2677\n",
      "Epoch 237/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1877413.7212 - mae: 591.8232 - val_loss: 1913266.0000 - val_mae: 608.3076\n",
      "Epoch 238/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1852969.1269 - mae: 585.8492 - val_loss: 1994724.0000 - val_mae: 626.1213\n",
      "Epoch 239/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1904345.9481 - mae: 607.8115 - val_loss: 1941923.5000 - val_mae: 624.0461\n",
      "Epoch 240/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1855129.9096 - mae: 603.9588 - val_loss: 2248039.0000 - val_mae: 716.8044\n",
      "Epoch 241/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1972409.7500 - mae: 632.7003 - val_loss: 1899902.5000 - val_mae: 599.7962\n",
      "Epoch 242/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1882163.8192 - mae: 599.1482 - val_loss: 1900297.3750 - val_mae: 594.4044\n",
      "Epoch 243/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1909264.7808 - mae: 594.4969 - val_loss: 1923582.5000 - val_mae: 603.2500\n",
      "Epoch 244/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1852781.0404 - mae: 584.1955 - val_loss: 1932084.3750 - val_mae: 622.9620\n",
      "Epoch 245/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1870634.0808 - mae: 592.7736 - val_loss: 1887885.0000 - val_mae: 591.7075\n",
      "Epoch 246/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1775343.4462 - mae: 568.5111 - val_loss: 1928173.5000 - val_mae: 592.6841\n",
      "Epoch 247/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1830477.7519 - mae: 593.8597 - val_loss: 1896029.1250 - val_mae: 590.9355\n",
      "Epoch 248/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1894393.1538 - mae: 587.9074 - val_loss: 1910336.5000 - val_mae: 594.7039\n",
      "Epoch 249/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1874348.3962 - mae: 589.0397 - val_loss: 1900970.3750 - val_mae: 592.0583\n",
      "Epoch 250/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1801267.5538 - mae: 578.5263 - val_loss: 1889771.0000 - val_mae: 593.4644\n",
      "Epoch 251/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1791579.5423 - mae: 578.0932 - val_loss: 2289652.5000 - val_mae: 658.1729\n",
      "Epoch 252/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2109608.9269 - mae: 645.6636 - val_loss: 1893247.3750 - val_mae: 595.1924\n",
      "Epoch 253/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1752860.1212 - mae: 577.2133 - val_loss: 1986588.1250 - val_mae: 594.8307\n",
      "Epoch 254/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1773881.9596 - mae: 573.4104 - val_loss: 1940521.0000 - val_mae: 615.4653\n",
      "Epoch 255/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1873969.2365 - mae: 590.4685 - val_loss: 1949990.3750 - val_mae: 595.1045\n",
      "Epoch 256/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1851526.7635 - mae: 589.3619 - val_loss: 1902789.1250 - val_mae: 603.3555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbbe884a390>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras import Model\n",
    "from datetime import datetime\n",
    "\n",
    "weights = tf.constant(df.weight.to_numpy(dtype=np.float32))\n",
    "\n",
    "inp = Input(shape=(36,))\n",
    "x = Dense(16, activation=\"relu\")(inp)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "out = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "logdir = \"logs/scalars/relu-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X[train], Y[train], validation_data=(X[val], Y[val]), batch_size=1024, epochs=256, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6db1614a-6c2a-40a6-a03f-3715651852a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from microdf import MicroDataFrame\n",
    "\n",
    "testing = MicroDataFrame(dict(\n",
    "    actual=Y[test],\n",
    "    predicted=model.predict(X[test]).squeeze(),\n",
    "), weights=weights[test].numpy())\n",
    "testing[\"error\"] = testing.predicted - testing.actual\n",
    "testing[\"abs_error\"] = np.abs(testing.error)\n",
    "testing[\"abs_rel_error\"] = testing.abs_error / (testing.actual + 1e-3)\n",
    "testing[\"white\"] = X[test].T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b38a2909-1307-49b2-a198-75b8ff77065b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "      <th>70</th>\n",
       "      <th>80</th>\n",
       "      <th>90</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abs_error</th>\n",
       "      <td>0.001311</td>\n",
       "      <td>6.303428</td>\n",
       "      <td>15.777587</td>\n",
       "      <td>27.886304</td>\n",
       "      <td>51.002919</td>\n",
       "      <td>89.591276</td>\n",
       "      <td>187.003720</td>\n",
       "      <td>345.691483</td>\n",
       "      <td>594.137226</td>\n",
       "      <td>1867.597391</td>\n",
       "      <td>1.637814e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_rel_error</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.036765</td>\n",
       "      <td>0.098943</td>\n",
       "      <td>0.228441</td>\n",
       "      <td>0.629130</td>\n",
       "      <td>2645.945601</td>\n",
       "      <td>11877.423533</td>\n",
       "      <td>22627.213975</td>\n",
       "      <td>44220.083091</td>\n",
       "      <td>90456.149325</td>\n",
       "      <td>9.481946e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         10         20         30         40   \\\n",
       "abs_error      0.001311  6.303428  15.777587  27.886304  51.002919   \n",
       "abs_rel_error  0.000004  0.036765   0.098943   0.228441   0.629130   \n",
       "\n",
       "                       50            60            70            80   \\\n",
       "abs_error        89.591276    187.003720    345.691483    594.137226   \n",
       "abs_rel_error  2645.945601  11877.423533  22627.213975  44220.083091   \n",
       "\n",
       "                        90            100  \n",
       "abs_error       1867.597391  1.637814e+04  \n",
       "abs_rel_error  90456.149325  9.481946e+06  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = testing[[\"abs_error\", \"abs_rel_error\"]].quantile(np.linspace(0, 1, 11))\n",
    "x.columns = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
