{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c0f7ff-5a5d-4182-8bef-f9164ebf2739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df = df[df.tax < 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed1fbc30-da28-4056-ad07-514181ab2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"tax\", \"weight\", \"white\", \"asian\", \"black\", \"male\", \"Gender\", \"Race\", \"industry\"], axis=1).to_numpy(dtype=np.float32)\n",
    "Y = df.tax.to_numpy(dtype=np.float32)\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "random = np.random.rand(len(X))\n",
    "train = random < TRAIN_SPLIT\n",
    "val = (random >= TRAIN_SPLIT) * (random < TRAIN_SPLIT + VAL_SPLIT)\n",
    "test = random >= TRAIN_SPLIT + VAL_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d9bcb7-492d-4146-9167-c6866b13514f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5f01a552e09424a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5f01a552e09424a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8698c7-7a53-4a21-9ac0-529085c3efa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 11655706.2308 - mae: 1803.4596 - val_loss: 7722334.5000 - val_mae: 1425.0189\n",
      "Epoch 2/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7101617.2077 - mae: 1356.3129 - val_loss: 6535898.5000 - val_mae: 1273.6306\n",
      "Epoch 3/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6020165.6154 - mae: 1244.3972 - val_loss: 5681065.5000 - val_mae: 1199.4958\n",
      "Epoch 4/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5224369.3538 - mae: 1139.0921 - val_loss: 4964055.0000 - val_mae: 1062.4174\n",
      "Epoch 5/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 4594464.3692 - mae: 1017.3828 - val_loss: 4347363.5000 - val_mae: 973.5231\n",
      "Epoch 6/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3864537.1500 - mae: 919.6065 - val_loss: 3815487.2500 - val_mae: 919.2795\n",
      "Epoch 7/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3491576.6846 - mae: 871.7864 - val_loss: 3520265.0000 - val_mae: 887.1760\n",
      "Epoch 8/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 3266115.9962 - mae: 827.8817 - val_loss: 3252784.0000 - val_mae: 816.3121\n",
      "Epoch 9/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3084417.5308 - mae: 786.8364 - val_loss: 3119998.2500 - val_mae: 783.7292\n",
      "Epoch 10/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2998267.4577 - mae: 757.7183 - val_loss: 3014148.2500 - val_mae: 773.9194\n",
      "Epoch 11/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2904028.9615 - mae: 750.8974 - val_loss: 2961849.5000 - val_mae: 785.9017\n",
      "Epoch 12/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2792173.3385 - mae: 735.8621 - val_loss: 2844929.5000 - val_mae: 735.0042\n",
      "Epoch 13/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2672048.5962 - mae: 703.0235 - val_loss: 2805272.5000 - val_mae: 757.7576\n",
      "Epoch 14/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2709044.6077 - mae: 708.4948 - val_loss: 2702269.2500 - val_mae: 714.5818\n",
      "Epoch 15/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2458213.8346 - mae: 670.7046 - val_loss: 2673718.5000 - val_mae: 717.8285\n",
      "Epoch 16/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2516611.2192 - mae: 677.5506 - val_loss: 2576851.5000 - val_mae: 707.7809\n",
      "Epoch 17/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2416927.3731 - mae: 663.2435 - val_loss: 2619744.2500 - val_mae: 732.7343\n",
      "Epoch 18/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2332270.1808 - mae: 666.7820 - val_loss: 2475006.2500 - val_mae: 681.2945\n",
      "Epoch 19/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2296851.7308 - mae: 649.0925 - val_loss: 2488060.0000 - val_mae: 684.3857\n",
      "Epoch 20/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2250045.1923 - mae: 649.0581 - val_loss: 2424609.5000 - val_mae: 668.2494\n",
      "Epoch 21/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2246574.6000 - mae: 644.1091 - val_loss: 2397228.2500 - val_mae: 665.7983\n",
      "Epoch 22/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2193320.8654 - mae: 628.2182 - val_loss: 2448353.0000 - val_mae: 669.6872\n",
      "Epoch 23/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2117079.1827 - mae: 619.4176 - val_loss: 2451707.7500 - val_mae: 674.7471\n",
      "Epoch 24/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2166319.4231 - mae: 623.6118 - val_loss: 2363498.5000 - val_mae: 663.5204\n",
      "Epoch 25/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2195850.3115 - mae: 628.0852 - val_loss: 2340668.2500 - val_mae: 653.1346\n",
      "Epoch 26/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2107068.3173 - mae: 612.3719 - val_loss: 2355274.5000 - val_mae: 647.5745\n",
      "Epoch 27/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2140391.6288 - mae: 623.5969 - val_loss: 2311032.5000 - val_mae: 635.4404\n",
      "Epoch 28/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2076614.3212 - mae: 602.4212 - val_loss: 2344989.0000 - val_mae: 662.9402\n",
      "Epoch 29/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2085891.9288 - mae: 616.5233 - val_loss: 2445797.2500 - val_mae: 685.0939\n",
      "Epoch 30/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2108986.7712 - mae: 616.3613 - val_loss: 2566179.0000 - val_mae: 705.4098\n",
      "Epoch 31/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2148472.8942 - mae: 624.9222 - val_loss: 2380518.0000 - val_mae: 645.0662\n",
      "Epoch 32/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2074624.1365 - mae: 608.6977 - val_loss: 2350032.2500 - val_mae: 645.1819\n",
      "Epoch 33/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2009519.2077 - mae: 592.0008 - val_loss: 2328806.2500 - val_mae: 643.7428\n",
      "Epoch 34/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2097983.2558 - mae: 614.8436 - val_loss: 2335990.0000 - val_mae: 668.5642\n",
      "Epoch 35/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2077436.7154 - mae: 606.8506 - val_loss: 2276221.5000 - val_mae: 635.5717\n",
      "Epoch 36/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2021349.9077 - mae: 589.1292 - val_loss: 2284056.2500 - val_mae: 641.6226\n",
      "Epoch 37/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2015940.7865 - mae: 601.4246 - val_loss: 2341019.7500 - val_mae: 636.4927\n",
      "Epoch 38/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1964711.3962 - mae: 582.8143 - val_loss: 2287135.2500 - val_mae: 631.1375\n",
      "Epoch 39/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2016503.2115 - mae: 594.8732 - val_loss: 2301943.2500 - val_mae: 630.8229\n",
      "Epoch 40/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2046307.3385 - mae: 597.2953 - val_loss: 2246192.7500 - val_mae: 624.3066\n",
      "Epoch 41/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2028040.2038 - mae: 597.6119 - val_loss: 2246081.5000 - val_mae: 618.4547\n",
      "Epoch 42/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1984009.4865 - mae: 585.5891 - val_loss: 2302350.5000 - val_mae: 649.3137\n",
      "Epoch 43/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2046170.1558 - mae: 604.8068 - val_loss: 2352462.0000 - val_mae: 636.7454\n",
      "Epoch 44/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2009245.9019 - mae: 601.2952 - val_loss: 2247747.0000 - val_mae: 624.0212\n",
      "Epoch 45/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1947503.7115 - mae: 583.7919 - val_loss: 2228292.7500 - val_mae: 621.0363\n",
      "Epoch 46/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2058081.6058 - mae: 599.0889 - val_loss: 2266567.7500 - val_mae: 635.3812\n",
      "Epoch 47/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1973529.0462 - mae: 597.6459 - val_loss: 2378066.2500 - val_mae: 662.8442\n",
      "Epoch 48/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2009458.7346 - mae: 601.6784 - val_loss: 2238711.7500 - val_mae: 634.1748\n",
      "Epoch 49/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2047799.4192 - mae: 605.4445 - val_loss: 2257832.5000 - val_mae: 627.2518\n",
      "Epoch 50/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1975159.4981 - mae: 585.9163 - val_loss: 2275197.5000 - val_mae: 630.5587\n",
      "Epoch 51/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1986980.0538 - mae: 590.9053 - val_loss: 2227541.5000 - val_mae: 616.3832\n",
      "Epoch 52/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2033064.8788 - mae: 600.3371 - val_loss: 2250132.7500 - val_mae: 628.5614\n",
      "Epoch 53/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1957686.6885 - mae: 594.2388 - val_loss: 2346007.5000 - val_mae: 643.3561\n",
      "Epoch 54/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2041113.5596 - mae: 595.5179 - val_loss: 2272363.7500 - val_mae: 647.4457\n",
      "Epoch 55/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2054740.2058 - mae: 602.8682 - val_loss: 2203924.7500 - val_mae: 618.8144\n",
      "Epoch 56/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1974109.0558 - mae: 587.1914 - val_loss: 2223510.2500 - val_mae: 632.6938\n",
      "Epoch 57/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1930094.7923 - mae: 589.1854 - val_loss: 2213406.0000 - val_mae: 619.3102\n",
      "Epoch 58/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2005222.3500 - mae: 604.5212 - val_loss: 2332236.5000 - val_mae: 645.0315\n",
      "Epoch 59/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1951392.1500 - mae: 594.2679 - val_loss: 2310047.7500 - val_mae: 656.2920\n",
      "Epoch 60/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2046519.3712 - mae: 606.6917 - val_loss: 2201013.0000 - val_mae: 619.2798\n",
      "Epoch 61/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1970472.1808 - mae: 589.0006 - val_loss: 2196591.7500 - val_mae: 618.7652\n",
      "Epoch 62/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1995995.0692 - mae: 591.1530 - val_loss: 2211229.2500 - val_mae: 640.1335\n",
      "Epoch 63/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2021857.4788 - mae: 601.6759 - val_loss: 2219656.7500 - val_mae: 637.1339\n",
      "Epoch 64/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1969987.8846 - mae: 593.2460 - val_loss: 2251755.5000 - val_mae: 629.7608\n",
      "Epoch 65/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1904007.5615 - mae: 587.9773 - val_loss: 2192895.5000 - val_mae: 638.5109\n",
      "Epoch 66/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1912961.4346 - mae: 591.3477 - val_loss: 2179488.7500 - val_mae: 615.0493\n",
      "Epoch 67/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1986383.0019 - mae: 591.7729 - val_loss: 2221247.5000 - val_mae: 639.0157\n",
      "Epoch 68/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1945258.3635 - mae: 590.1303 - val_loss: 2190589.2500 - val_mae: 619.3423\n",
      "Epoch 69/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1974394.3346 - mae: 591.3458 - val_loss: 2186624.2500 - val_mae: 616.4868\n",
      "Epoch 70/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1973027.9192 - mae: 590.4820 - val_loss: 2212231.2500 - val_mae: 634.3939\n",
      "Epoch 71/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1921553.4596 - mae: 589.2086 - val_loss: 2173415.7500 - val_mae: 616.0097\n",
      "Epoch 72/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1981350.2904 - mae: 591.8448 - val_loss: 2230946.0000 - val_mae: 644.8374\n",
      "Epoch 73/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1926559.0154 - mae: 587.7034 - val_loss: 2262311.5000 - val_mae: 638.9595\n",
      "Epoch 74/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1980122.9769 - mae: 612.1869 - val_loss: 2256311.5000 - val_mae: 644.4230\n",
      "Epoch 75/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1947490.7538 - mae: 592.5305 - val_loss: 2201748.5000 - val_mae: 622.2309\n",
      "Epoch 76/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1905244.3250 - mae: 581.0991 - val_loss: 2257920.5000 - val_mae: 627.5720\n",
      "Epoch 77/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1881251.1846 - mae: 584.3880 - val_loss: 2349482.2500 - val_mae: 660.6958\n",
      "Epoch 78/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1981593.1827 - mae: 592.1970 - val_loss: 2195369.0000 - val_mae: 624.6003\n",
      "Epoch 79/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1959210.7135 - mae: 595.0406 - val_loss: 2219806.5000 - val_mae: 640.7199\n",
      "Epoch 80/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1963616.0192 - mae: 596.7015 - val_loss: 2193136.5000 - val_mae: 627.8804\n",
      "Epoch 81/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1889968.7846 - mae: 580.5519 - val_loss: 2177860.2500 - val_mae: 613.2313\n",
      "Epoch 82/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1944231.9096 - mae: 590.8304 - val_loss: 2193539.7500 - val_mae: 618.7788\n",
      "Epoch 83/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1886227.5519 - mae: 585.1545 - val_loss: 2238050.0000 - val_mae: 632.5530\n",
      "Epoch 84/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1906501.1538 - mae: 590.1547 - val_loss: 2161617.7500 - val_mae: 619.2213\n",
      "Epoch 85/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1984136.4269 - mae: 597.7532 - val_loss: 2198355.7500 - val_mae: 625.9337\n",
      "Epoch 86/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1942038.9135 - mae: 587.1539 - val_loss: 2166544.5000 - val_mae: 617.0591\n",
      "Epoch 87/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1914110.5231 - mae: 592.0560 - val_loss: 2153468.2500 - val_mae: 616.7186\n",
      "Epoch 88/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1957297.9212 - mae: 587.7658 - val_loss: 2137830.7500 - val_mae: 623.7074\n",
      "Epoch 89/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1901140.7519 - mae: 592.9710 - val_loss: 2175693.5000 - val_mae: 617.1377\n",
      "Epoch 90/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1929970.1308 - mae: 596.1290 - val_loss: 2161532.0000 - val_mae: 615.2991\n",
      "Epoch 91/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1933846.7942 - mae: 588.9286 - val_loss: 2172032.7500 - val_mae: 638.1966\n",
      "Epoch 92/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1867815.7404 - mae: 580.7172 - val_loss: 2141879.2500 - val_mae: 612.0162\n",
      "Epoch 93/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1939793.3385 - mae: 585.7312 - val_loss: 2150409.2500 - val_mae: 618.9805\n",
      "Epoch 94/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1936263.0231 - mae: 586.6752 - val_loss: 2157551.5000 - val_mae: 610.2423\n",
      "Epoch 95/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1952863.8577 - mae: 588.6867 - val_loss: 2165778.2500 - val_mae: 617.4609\n",
      "Epoch 96/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1902879.4981 - mae: 586.8781 - val_loss: 2148738.7500 - val_mae: 615.5139\n",
      "Epoch 97/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1886957.9962 - mae: 579.2871 - val_loss: 2195026.5000 - val_mae: 626.4888\n",
      "Epoch 98/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1983247.3923 - mae: 598.6545 - val_loss: 2169404.5000 - val_mae: 642.4720\n",
      "Epoch 99/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1921775.0519 - mae: 594.5904 - val_loss: 2166253.2500 - val_mae: 618.2352\n",
      "Epoch 100/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1917157.2635 - mae: 583.5277 - val_loss: 2152123.5000 - val_mae: 619.0781\n",
      "Epoch 101/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1882641.5692 - mae: 587.4124 - val_loss: 2175451.5000 - val_mae: 644.6683\n",
      "Epoch 102/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1843089.8885 - mae: 590.5267 - val_loss: 2165521.5000 - val_mae: 629.5692\n",
      "Epoch 103/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1947008.1096 - mae: 592.7437 - val_loss: 2159421.5000 - val_mae: 624.9870\n",
      "Epoch 104/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1940885.5212 - mae: 592.6817 - val_loss: 2415528.0000 - val_mae: 689.0624\n",
      "Epoch 105/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1951649.3942 - mae: 606.1118 - val_loss: 2133275.2500 - val_mae: 611.3101\n",
      "Epoch 106/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1875093.5615 - mae: 588.7765 - val_loss: 2142490.0000 - val_mae: 613.4022\n",
      "Epoch 107/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1961377.2135 - mae: 598.1718 - val_loss: 2157253.0000 - val_mae: 623.8400\n",
      "Epoch 108/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1919892.3481 - mae: 592.9003 - val_loss: 2149703.2500 - val_mae: 619.0782\n",
      "Epoch 109/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1948107.5808 - mae: 609.2958 - val_loss: 2143396.5000 - val_mae: 613.6224\n",
      "Epoch 110/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1918072.2385 - mae: 589.5515 - val_loss: 2146925.2500 - val_mae: 617.3574\n",
      "Epoch 111/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1873724.7808 - mae: 584.3540 - val_loss: 2174603.7500 - val_mae: 623.2527\n",
      "Epoch 112/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1883909.6577 - mae: 588.8689 - val_loss: 2147895.2500 - val_mae: 619.8296\n",
      "Epoch 113/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1906949.5827 - mae: 593.7680 - val_loss: 2131056.0000 - val_mae: 610.1374\n",
      "Epoch 114/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1888694.1038 - mae: 585.4527 - val_loss: 2182596.5000 - val_mae: 658.2148\n",
      "Epoch 115/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1907445.6077 - mae: 595.5753 - val_loss: 2231881.2500 - val_mae: 638.6638\n",
      "Epoch 116/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1987870.3365 - mae: 606.2565 - val_loss: 2123561.7500 - val_mae: 616.5034\n",
      "Epoch 117/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1913763.5731 - mae: 593.9947 - val_loss: 2204919.5000 - val_mae: 644.7950\n",
      "Epoch 118/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1899709.5904 - mae: 587.8609 - val_loss: 2268509.0000 - val_mae: 651.3134\n",
      "Epoch 119/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1939743.5981 - mae: 599.5341 - val_loss: 2161586.5000 - val_mae: 620.4299\n",
      "Epoch 120/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1897850.4462 - mae: 587.3465 - val_loss: 2292339.2500 - val_mae: 657.8043\n",
      "Epoch 121/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1881971.3077 - mae: 591.4573 - val_loss: 2163719.2500 - val_mae: 618.7109\n",
      "Epoch 122/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1894369.0173 - mae: 590.7678 - val_loss: 2170642.5000 - val_mae: 667.1261\n",
      "Epoch 123/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1962601.0288 - mae: 600.5651 - val_loss: 2118534.7500 - val_mae: 615.4291\n",
      "Epoch 124/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1873251.6942 - mae: 593.1861 - val_loss: 2113822.2500 - val_mae: 617.2577\n",
      "Epoch 125/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1885942.1288 - mae: 593.8084 - val_loss: 2154232.2500 - val_mae: 631.6729\n",
      "Epoch 126/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1911207.1923 - mae: 602.0681 - val_loss: 2149440.5000 - val_mae: 620.8539\n",
      "Epoch 127/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1890932.3462 - mae: 589.4113 - val_loss: 2121127.2500 - val_mae: 614.4844\n",
      "Epoch 128/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1870964.8231 - mae: 587.5719 - val_loss: 2169186.7500 - val_mae: 642.5273\n",
      "Epoch 129/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1892842.4577 - mae: 592.3580 - val_loss: 2138744.5000 - val_mae: 617.4798\n",
      "Epoch 130/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1913461.6712 - mae: 592.1530 - val_loss: 2118446.0000 - val_mae: 615.8652\n",
      "Epoch 131/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1938636.5635 - mae: 591.1412 - val_loss: 2245695.5000 - val_mae: 660.2887\n",
      "Epoch 132/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1954793.3750 - mae: 595.8976 - val_loss: 2111425.0000 - val_mae: 609.6254\n",
      "Epoch 133/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1967447.1865 - mae: 598.8892 - val_loss: 2134241.5000 - val_mae: 629.4666\n",
      "Epoch 134/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1900757.5038 - mae: 593.6613 - val_loss: 2101859.0000 - val_mae: 622.1830\n",
      "Epoch 135/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1840979.6750 - mae: 587.3301 - val_loss: 2146220.2500 - val_mae: 635.2565\n",
      "Epoch 136/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1901958.7788 - mae: 588.0938 - val_loss: 2170498.5000 - val_mae: 637.4858\n",
      "Epoch 137/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1889162.1173 - mae: 591.8185 - val_loss: 2124157.2500 - val_mae: 620.4048\n",
      "Epoch 138/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1874084.8673 - mae: 585.1043 - val_loss: 2118482.2500 - val_mae: 618.3110\n",
      "Epoch 139/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1912490.0115 - mae: 598.3722 - val_loss: 2114488.5000 - val_mae: 612.6476\n",
      "Epoch 140/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1872771.4538 - mae: 581.3290 - val_loss: 2117094.0000 - val_mae: 616.7537\n",
      "Epoch 141/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1973751.4615 - mae: 603.5660 - val_loss: 2105726.2500 - val_mae: 630.4113\n",
      "Epoch 142/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1870532.3750 - mae: 590.6245 - val_loss: 2095419.1250 - val_mae: 610.6542\n",
      "Epoch 143/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1904343.7231 - mae: 588.6440 - val_loss: 2114953.7500 - val_mae: 611.5026\n",
      "Epoch 144/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1826258.2462 - mae: 577.8526 - val_loss: 2144437.5000 - val_mae: 632.3771\n",
      "Epoch 145/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1891526.0885 - mae: 590.5834 - val_loss: 2195041.2500 - val_mae: 672.4884\n",
      "Epoch 146/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1928085.7904 - mae: 597.7644 - val_loss: 2133778.5000 - val_mae: 635.0660\n",
      "Epoch 147/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1857313.7731 - mae: 593.1920 - val_loss: 2243755.5000 - val_mae: 651.2831\n",
      "Epoch 148/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1950727.8615 - mae: 613.8077 - val_loss: 2116368.2500 - val_mae: 641.1856\n",
      "Epoch 149/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1858073.8385 - mae: 588.9704 - val_loss: 2300010.2500 - val_mae: 671.4706\n",
      "Epoch 150/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1893086.8577 - mae: 599.7787 - val_loss: 2104268.0000 - val_mae: 620.8741\n",
      "Epoch 151/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1863394.5000 - mae: 585.9476 - val_loss: 2229824.5000 - val_mae: 694.0633\n",
      "Epoch 152/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1947261.0942 - mae: 613.2070 - val_loss: 2106418.0000 - val_mae: 616.6858\n",
      "Epoch 153/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1887257.6981 - mae: 587.4252 - val_loss: 2084466.5000 - val_mae: 613.9980\n",
      "Epoch 154/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1928355.4038 - mae: 589.0112 - val_loss: 2104503.5000 - val_mae: 625.7691\n",
      "Epoch 155/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1816944.2192 - mae: 586.5537 - val_loss: 2112926.7500 - val_mae: 615.5121\n",
      "Epoch 156/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1837209.0712 - mae: 580.7770 - val_loss: 2090227.8750 - val_mae: 629.8046\n",
      "Epoch 157/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1899170.1058 - mae: 599.9821 - val_loss: 2110579.7500 - val_mae: 643.2820\n",
      "Epoch 158/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1851403.8981 - mae: 592.4444 - val_loss: 2106286.0000 - val_mae: 627.9078\n",
      "Epoch 159/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1928270.2885 - mae: 593.3737 - val_loss: 2079017.5000 - val_mae: 606.5759\n",
      "Epoch 160/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1895314.4096 - mae: 600.4385 - val_loss: 2089232.5000 - val_mae: 614.3049\n",
      "Epoch 161/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1889146.0615 - mae: 595.5355 - val_loss: 2082290.6250 - val_mae: 639.0703\n",
      "Epoch 162/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1915989.3654 - mae: 608.2503 - val_loss: 2080425.1250 - val_mae: 619.4901\n",
      "Epoch 163/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1840166.2192 - mae: 586.0274 - val_loss: 2099445.7500 - val_mae: 612.7507\n",
      "Epoch 164/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1862645.3962 - mae: 591.4282 - val_loss: 2098744.5000 - val_mae: 616.0817\n",
      "Epoch 165/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1933061.5558 - mae: 598.3082 - val_loss: 2072713.0000 - val_mae: 609.8091\n",
      "Epoch 166/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1876697.4712 - mae: 593.2835 - val_loss: 2089573.8750 - val_mae: 616.6285\n",
      "Epoch 167/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1787095.5577 - mae: 577.8968 - val_loss: 2089983.7500 - val_mae: 625.5114\n",
      "Epoch 168/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1866792.2096 - mae: 589.2839 - val_loss: 2066609.1250 - val_mae: 610.3986\n",
      "Epoch 169/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1870934.0192 - mae: 593.1323 - val_loss: 2119818.0000 - val_mae: 627.9129\n",
      "Epoch 170/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1809923.1827 - mae: 586.2872 - val_loss: 2184720.2500 - val_mae: 644.2790\n",
      "Epoch 171/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1832760.2558 - mae: 595.1223 - val_loss: 2124842.2500 - val_mae: 636.5770\n",
      "Epoch 172/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1889848.5865 - mae: 590.3222 - val_loss: 2083055.1250 - val_mae: 616.4577\n",
      "Epoch 173/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1813178.4308 - mae: 586.2121 - val_loss: 2076011.8750 - val_mae: 630.3746\n",
      "Epoch 174/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1890870.6423 - mae: 594.0195 - val_loss: 2081056.0000 - val_mae: 624.1597\n",
      "Epoch 175/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1795499.6231 - mae: 585.7863 - val_loss: 2102425.0000 - val_mae: 618.7919\n",
      "Epoch 176/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1913319.0654 - mae: 604.3646 - val_loss: 2081522.2500 - val_mae: 619.7394\n",
      "Epoch 177/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1906169.9769 - mae: 599.0112 - val_loss: 2070256.5000 - val_mae: 610.5541\n",
      "Epoch 178/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1853509.8750 - mae: 592.4653 - val_loss: 2130570.2500 - val_mae: 657.6422\n",
      "Epoch 179/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1864504.2712 - mae: 596.3545 - val_loss: 2069442.1250 - val_mae: 618.4715\n",
      "Epoch 180/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1784640.2519 - mae: 585.9289 - val_loss: 2120391.0000 - val_mae: 626.8992\n",
      "Epoch 181/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1783580.9654 - mae: 586.3155 - val_loss: 2079862.8750 - val_mae: 610.9208\n",
      "Epoch 182/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1831162.7327 - mae: 586.3503 - val_loss: 2076256.5000 - val_mae: 617.1180\n",
      "Epoch 183/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1850359.6019 - mae: 586.6164 - val_loss: 2099052.0000 - val_mae: 641.9564\n",
      "Epoch 184/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1899376.9231 - mae: 604.1311 - val_loss: 2077614.8750 - val_mae: 625.8304\n",
      "Epoch 185/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1779718.2808 - mae: 587.8809 - val_loss: 2075478.2500 - val_mae: 614.3749\n",
      "Epoch 186/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1794779.7404 - mae: 579.4818 - val_loss: 2067123.5000 - val_mae: 611.9721\n",
      "Epoch 187/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1813539.9346 - mae: 588.7931 - val_loss: 2063413.6250 - val_mae: 622.3865\n",
      "Epoch 188/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1850165.1596 - mae: 589.1370 - val_loss: 2077607.1250 - val_mae: 621.1221\n",
      "Epoch 189/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1823679.0981 - mae: 585.3462 - val_loss: 2070914.6250 - val_mae: 621.6002\n",
      "Epoch 190/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1797835.9808 - mae: 590.9811 - val_loss: 2082292.3750 - val_mae: 635.5530\n",
      "Epoch 191/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1834234.5288 - mae: 593.5667 - val_loss: 2181953.5000 - val_mae: 635.7207\n",
      "Epoch 192/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1883055.3692 - mae: 608.0141 - val_loss: 2061071.3750 - val_mae: 618.7916\n",
      "Epoch 193/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1769645.8423 - mae: 585.9932 - val_loss: 2098462.5000 - val_mae: 625.4915\n",
      "Epoch 194/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1819022.3135 - mae: 588.3696 - val_loss: 2120054.5000 - val_mae: 637.2021\n",
      "Epoch 195/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1792273.1481 - mae: 587.1713 - val_loss: 2103137.7500 - val_mae: 631.8886\n",
      "Epoch 196/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1860881.9788 - mae: 593.4692 - val_loss: 2046898.3750 - val_mae: 613.2189\n",
      "Epoch 197/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1769043.3808 - mae: 579.7408 - val_loss: 2063266.8750 - val_mae: 616.1874\n",
      "Epoch 198/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1828948.7308 - mae: 592.8367 - val_loss: 2094955.8750 - val_mae: 642.0102\n",
      "Epoch 199/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1860398.8231 - mae: 601.0755 - val_loss: 2053583.0000 - val_mae: 616.4062\n",
      "Epoch 200/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1811895.3327 - mae: 591.4126 - val_loss: 2065424.5000 - val_mae: 617.2668\n",
      "Epoch 201/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1849621.4327 - mae: 597.8856 - val_loss: 2080418.1250 - val_mae: 618.7950\n",
      "Epoch 202/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1780972.2981 - mae: 585.9525 - val_loss: 2061685.8750 - val_mae: 618.6000\n",
      "Epoch 203/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1764003.2538 - mae: 579.5856 - val_loss: 2065619.1250 - val_mae: 618.9531\n",
      "Epoch 204/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1812531.2558 - mae: 592.4977 - val_loss: 2078687.8750 - val_mae: 634.7014\n",
      "Epoch 205/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1830458.9692 - mae: 592.9080 - val_loss: 2100287.7500 - val_mae: 647.2931\n",
      "Epoch 206/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1854872.5135 - mae: 598.7261 - val_loss: 2091529.6250 - val_mae: 645.4873\n",
      "Epoch 207/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1831352.0000 - mae: 587.6227 - val_loss: 2046645.2500 - val_mae: 623.5528\n",
      "Epoch 208/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1837724.2481 - mae: 595.5247 - val_loss: 2071709.0000 - val_mae: 645.6036\n",
      "Epoch 209/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1888902.0962 - mae: 602.7243 - val_loss: 2152813.5000 - val_mae: 651.2539\n",
      "Epoch 210/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1890825.1250 - mae: 606.0111 - val_loss: 2083784.5000 - val_mae: 632.0437\n",
      "Epoch 211/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1839725.7865 - mae: 602.5223 - val_loss: 2072331.5000 - val_mae: 616.5825\n",
      "Epoch 212/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1834568.3038 - mae: 598.6273 - val_loss: 2102407.5000 - val_mae: 649.9232\n",
      "Epoch 213/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1823710.8096 - mae: 598.3685 - val_loss: 2043913.7500 - val_mae: 613.8392\n",
      "Epoch 214/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1827981.5096 - mae: 598.1630 - val_loss: 2059283.0000 - val_mae: 623.8685\n",
      "Epoch 215/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1818976.2250 - mae: 589.6736 - val_loss: 2046857.0000 - val_mae: 613.2270\n",
      "Epoch 216/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1851272.7538 - mae: 597.7476 - val_loss: 2147593.7500 - val_mae: 671.9944\n",
      "Epoch 217/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1897358.3442 - mae: 622.8325 - val_loss: 2216966.7500 - val_mae: 662.0093\n",
      "Epoch 218/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1809243.0635 - mae: 594.5504 - val_loss: 2029759.1250 - val_mae: 615.1930\n",
      "Epoch 219/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1847891.9038 - mae: 598.5379 - val_loss: 2072193.6250 - val_mae: 622.6774\n",
      "Epoch 220/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1776072.3654 - mae: 581.4392 - val_loss: 2048285.8750 - val_mae: 618.3738\n",
      "Epoch 221/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1815218.6462 - mae: 591.0648 - val_loss: 2039940.3750 - val_mae: 626.2330\n",
      "Epoch 222/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1774876.8365 - mae: 585.2166 - val_loss: 2073827.1250 - val_mae: 642.8651\n",
      "Epoch 223/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1814150.5865 - mae: 594.4607 - val_loss: 2027159.3750 - val_mae: 616.9778\n",
      "Epoch 224/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1761167.0558 - mae: 581.3230 - val_loss: 2047078.3750 - val_mae: 622.5931\n",
      "Epoch 225/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1819938.0962 - mae: 592.2425 - val_loss: 2086493.8750 - val_mae: 626.9210\n",
      "Epoch 226/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1826393.0692 - mae: 593.8596 - val_loss: 2012965.8750 - val_mae: 613.0717\n",
      "Epoch 227/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1755902.2827 - mae: 581.5532 - val_loss: 2084994.5000 - val_mae: 637.5546\n",
      "Epoch 228/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1826898.5942 - mae: 599.6139 - val_loss: 2034652.3750 - val_mae: 618.2814\n",
      "Epoch 229/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1828230.7962 - mae: 594.4485 - val_loss: 2042644.8750 - val_mae: 619.5239\n",
      "Epoch 230/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1862130.3192 - mae: 593.9578 - val_loss: 2044737.2500 - val_mae: 626.4001\n",
      "Epoch 231/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1792377.9154 - mae: 588.0599 - val_loss: 2060702.7500 - val_mae: 633.1329\n",
      "Epoch 232/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1795302.6135 - mae: 592.3253 - val_loss: 2059609.7500 - val_mae: 622.9600\n",
      "Epoch 233/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1790857.9962 - mae: 588.3368 - val_loss: 2167434.5000 - val_mae: 670.3208\n",
      "Epoch 234/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1783619.4192 - mae: 592.3333 - val_loss: 2059427.6250 - val_mae: 623.0285\n",
      "Epoch 235/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1782000.7115 - mae: 590.3181 - val_loss: 2026396.8750 - val_mae: 617.1019\n",
      "Epoch 236/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1758701.4750 - mae: 596.4124 - val_loss: 2028917.5000 - val_mae: 620.7084\n",
      "Epoch 237/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1758191.0769 - mae: 584.1723 - val_loss: 2058398.5000 - val_mae: 634.5981\n",
      "Epoch 238/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1821267.2865 - mae: 591.7773 - val_loss: 2086031.5000 - val_mae: 664.0454\n",
      "Epoch 239/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1861114.2154 - mae: 607.1334 - val_loss: 2061051.8750 - val_mae: 631.8322\n",
      "Epoch 240/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1796813.6192 - mae: 587.5376 - val_loss: 2022125.8750 - val_mae: 622.4151\n",
      "Epoch 241/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1817977.5096 - mae: 600.2387 - val_loss: 2029821.0000 - val_mae: 617.3707\n",
      "Epoch 242/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1790534.8846 - mae: 589.7384 - val_loss: 2060070.1250 - val_mae: 641.7229\n",
      "Epoch 243/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1736888.0038 - mae: 587.8273 - val_loss: 2100752.5000 - val_mae: 669.2122\n",
      "Epoch 244/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1813975.9981 - mae: 608.1079 - val_loss: 2021676.3750 - val_mae: 631.5560\n",
      "Epoch 245/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1828894.9077 - mae: 600.5015 - val_loss: 2012851.2500 - val_mae: 612.5367\n",
      "Epoch 246/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1760685.4038 - mae: 588.0488 - val_loss: 2054953.3750 - val_mae: 620.9760\n",
      "Epoch 247/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1864374.9635 - mae: 595.9778 - val_loss: 2013023.5000 - val_mae: 606.2869\n",
      "Epoch 248/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1790764.3192 - mae: 588.8631 - val_loss: 2029252.2500 - val_mae: 615.4891\n",
      "Epoch 249/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1828381.0481 - mae: 599.7971 - val_loss: 2022876.1250 - val_mae: 611.4460\n",
      "Epoch 250/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1785559.9712 - mae: 588.1982 - val_loss: 2054291.3750 - val_mae: 621.7734\n",
      "Epoch 251/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1835348.9288 - mae: 596.6785 - val_loss: 2034220.2500 - val_mae: 619.3812\n",
      "Epoch 252/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1777516.2481 - mae: 584.8238 - val_loss: 2016491.3750 - val_mae: 617.6894\n",
      "Epoch 253/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1807083.5808 - mae: 591.4346 - val_loss: 1994705.3750 - val_mae: 621.4630\n",
      "Epoch 254/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1786405.4154 - mae: 593.3731 - val_loss: 2030394.2500 - val_mae: 613.5422\n",
      "Epoch 255/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1815404.6615 - mae: 594.1698 - val_loss: 2053700.7500 - val_mae: 616.4419\n",
      "Epoch 256/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1815309.4769 - mae: 595.0427 - val_loss: 2019200.2500 - val_mae: 630.5997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd4eeefcd50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras import Model\n",
    "from datetime import datetime\n",
    "\n",
    "weights = tf.constant(df.weight.to_numpy(dtype=np.float32))\n",
    "\n",
    "inp = Input(shape=(32,))\n",
    "num_vars = Dense(8, activation=\"relu\")(inp)\n",
    "bool_vars = Dense(8, activation=\"sigmoid\")(inp)\n",
    "concat = Concatenate()([num_vars, bool_vars])\n",
    "final = Dense(16, activation=\"relu\")(concat)\n",
    "out = Dense(1, activation=\"linear\")(final)\n",
    "\n",
    "logdir = \"logs/scalars/hybrid-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X[train], Y[train], validation_data=(X[val], Y[val]), batch_size=1024, epochs=256, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebf03a16-447c-4472-a189-1383160f23fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 157031020.3077 - mae: 5844.1468 - val_loss: 67695216.0000 - val_mae: 4241.5435\n",
      "Epoch 2/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 72219149.3538 - mae: 3947.3580 - val_loss: 32634902.0000 - val_mae: 3099.1421\n",
      "Epoch 3/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 37459175.1385 - mae: 2902.8519 - val_loss: 22337108.0000 - val_mae: 2538.8806\n",
      "Epoch 4/256\n",
      "64/64 [==============================] - 0s 987us/step - loss: 33422532.1846 - mae: 2474.5018 - val_loss: 17602230.0000 - val_mae: 2254.6772\n",
      "Epoch 5/256\n",
      "64/64 [==============================] - 0s 976us/step - loss: 20671357.7077 - mae: 2163.9712 - val_loss: 14352344.0000 - val_mae: 2020.3097\n",
      "Epoch 6/256\n",
      "64/64 [==============================] - 0s 987us/step - loss: 15884182.7846 - mae: 1924.6532 - val_loss: 11748338.0000 - val_mae: 1831.3433\n",
      "Epoch 7/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 15271714.7385 - mae: 1761.8402 - val_loss: 9805835.0000 - val_mae: 1627.7426\n",
      "Epoch 8/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 12100937.9692 - mae: 1550.7438 - val_loss: 8402743.0000 - val_mae: 1448.4585\n",
      "Epoch 9/256\n",
      "64/64 [==============================] - 0s 999us/step - loss: 11247252.6231 - mae: 1413.3807 - val_loss: 7346436.0000 - val_mae: 1335.2670\n",
      "Epoch 10/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 8760459.9615 - mae: 1300.0212 - val_loss: 6525590.5000 - val_mae: 1256.3359\n",
      "Epoch 11/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 7590788.7538 - mae: 1194.5636 - val_loss: 5899463.0000 - val_mae: 1169.2430\n",
      "Epoch 12/256\n",
      "64/64 [==============================] - 0s 988us/step - loss: 6805699.8154 - mae: 1125.7386 - val_loss: 5424938.0000 - val_mae: 1126.6422\n",
      "Epoch 13/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 6179521.5154 - mae: 1077.3084 - val_loss: 5055344.0000 - val_mae: 1081.8159\n",
      "Epoch 14/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5185806.1692 - mae: 1031.3155 - val_loss: 4771267.0000 - val_mae: 1047.5822\n",
      "Epoch 15/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5060891.0538 - mae: 1013.1835 - val_loss: 4525865.0000 - val_mae: 1017.8488\n",
      "Epoch 16/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4797573.1846 - mae: 981.7992 - val_loss: 4329328.0000 - val_mae: 1005.7076\n",
      "Epoch 17/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4395394.3538 - mae: 964.8974 - val_loss: 4174298.0000 - val_mae: 996.1272\n",
      "Epoch 18/256\n",
      "64/64 [==============================] - 0s 998us/step - loss: 3989652.8000 - mae: 959.4928 - val_loss: 4046997.5000 - val_mae: 992.4810\n",
      "Epoch 19/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3861944.9769 - mae: 956.6979 - val_loss: 3945772.7500 - val_mae: 979.6468\n",
      "Epoch 20/256\n",
      "64/64 [==============================] - 0s 990us/step - loss: 4277011.6462 - mae: 958.1587 - val_loss: 3846897.2500 - val_mae: 981.9678\n",
      "Epoch 21/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3680585.1500 - mae: 941.4630 - val_loss: 3784565.7500 - val_mae: 967.6366\n",
      "Epoch 22/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3784999.7500 - mae: 938.6095 - val_loss: 3721884.0000 - val_mae: 963.3307\n",
      "Epoch 23/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3630054.2577 - mae: 936.1630 - val_loss: 3682479.5000 - val_mae: 960.5856\n",
      "Epoch 24/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3601686.7808 - mae: 939.8498 - val_loss: 3636957.2500 - val_mae: 975.6577\n",
      "Epoch 25/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3441212.1769 - mae: 931.3774 - val_loss: 3605787.5000 - val_mae: 958.0479\n",
      "Epoch 26/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3531934.8538 - mae: 936.2848 - val_loss: 3566961.2500 - val_mae: 961.1631\n",
      "Epoch 27/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3408013.9615 - mae: 927.0158 - val_loss: 3545398.5000 - val_mae: 958.2112\n",
      "Epoch 28/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3409635.2923 - mae: 924.5253 - val_loss: 3525339.2500 - val_mae: 961.2930\n",
      "Epoch 29/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3455926.6308 - mae: 932.7193 - val_loss: 3510716.7500 - val_mae: 960.3732\n",
      "Epoch 30/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3391536.9923 - mae: 928.7760 - val_loss: 3513068.0000 - val_mae: 970.8254\n",
      "Epoch 31/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3374369.3423 - mae: 930.1098 - val_loss: 3479271.5000 - val_mae: 950.5763\n",
      "Epoch 32/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3275913.4154 - mae: 916.2251 - val_loss: 3470976.2500 - val_mae: 956.5243\n",
      "Epoch 33/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3345029.7269 - mae: 923.4920 - val_loss: 3477657.0000 - val_mae: 947.3858\n",
      "Epoch 34/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3248573.5615 - mae: 917.8044 - val_loss: 3486261.2500 - val_mae: 946.2859\n",
      "Epoch 35/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3371033.7346 - mae: 915.9670 - val_loss: 3451538.5000 - val_mae: 946.1041\n",
      "Epoch 36/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3333677.2000 - mae: 913.4652 - val_loss: 3440470.5000 - val_mae: 951.3176\n",
      "Epoch 37/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3267930.5308 - mae: 914.4577 - val_loss: 3472461.2500 - val_mae: 969.2812\n",
      "Epoch 38/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3403987.3538 - mae: 929.7021 - val_loss: 3499049.5000 - val_mae: 942.6926\n",
      "Epoch 39/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3356665.2654 - mae: 919.2228 - val_loss: 3424489.2500 - val_mae: 948.4042\n",
      "Epoch 40/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3129186.2577 - mae: 900.9588 - val_loss: 3427288.2500 - val_mae: 942.7922\n",
      "Epoch 41/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3227306.2846 - mae: 907.1654 - val_loss: 3424745.2500 - val_mae: 953.9191\n",
      "Epoch 42/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3332070.1423 - mae: 922.9208 - val_loss: 3412542.2500 - val_mae: 944.8675\n",
      "Epoch 43/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3236842.9231 - mae: 911.4609 - val_loss: 3431661.2500 - val_mae: 939.4125\n",
      "Epoch 44/256\n",
      "64/64 [==============================] - 0s 976us/step - loss: 3163302.2346 - mae: 899.0660 - val_loss: 3416739.2500 - val_mae: 954.7950\n",
      "Epoch 45/256\n",
      "64/64 [==============================] - 0s 989us/step - loss: 3184086.8654 - mae: 904.8234 - val_loss: 3404501.7500 - val_mae: 948.1767\n",
      "Epoch 46/256\n",
      "64/64 [==============================] - 0s 995us/step - loss: 3205556.8692 - mae: 908.8485 - val_loss: 3433167.0000 - val_mae: 940.5890\n",
      "Epoch 47/256\n",
      "64/64 [==============================] - 0s 947us/step - loss: 3399284.9462 - mae: 926.4652 - val_loss: 3411927.2500 - val_mae: 957.5912\n",
      "Epoch 48/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3207442.6962 - mae: 908.5272 - val_loss: 3396909.0000 - val_mae: 941.5076\n",
      "Epoch 49/256\n",
      "64/64 [==============================] - 0s 928us/step - loss: 3240354.0038 - mae: 915.5090 - val_loss: 3395508.7500 - val_mae: 947.0222\n",
      "Epoch 50/256\n",
      "64/64 [==============================] - 0s 896us/step - loss: 3280821.2500 - mae: 915.8752 - val_loss: 3395013.2500 - val_mae: 946.1797\n",
      "Epoch 51/256\n",
      "64/64 [==============================] - 0s 957us/step - loss: 3240347.5692 - mae: 910.7094 - val_loss: 3389339.7500 - val_mae: 943.6276\n",
      "Epoch 52/256\n",
      "64/64 [==============================] - 0s 933us/step - loss: 3153151.5038 - mae: 901.2455 - val_loss: 3389285.5000 - val_mae: 943.6481\n",
      "Epoch 53/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3370656.4615 - mae: 926.9853 - val_loss: 3387659.7500 - val_mae: 943.9201\n",
      "Epoch 54/256\n",
      "64/64 [==============================] - 0s 966us/step - loss: 3282040.3538 - mae: 922.0978 - val_loss: 3396520.2500 - val_mae: 937.7175\n",
      "Epoch 55/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3316789.7231 - mae: 918.1021 - val_loss: 3394405.5000 - val_mae: 940.5336\n",
      "Epoch 56/256\n",
      "64/64 [==============================] - 0s 964us/step - loss: 3195226.3577 - mae: 900.5341 - val_loss: 3384148.2500 - val_mae: 939.0441\n",
      "Epoch 57/256\n",
      "64/64 [==============================] - 0s 935us/step - loss: 3256927.1000 - mae: 913.3508 - val_loss: 3379303.0000 - val_mae: 939.9545\n",
      "Epoch 58/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3212684.2654 - mae: 907.2447 - val_loss: 3386622.0000 - val_mae: 937.2886\n",
      "Epoch 59/256\n",
      "64/64 [==============================] - 0s 957us/step - loss: 3301129.2346 - mae: 923.2074 - val_loss: 3380446.7500 - val_mae: 950.0405\n",
      "Epoch 60/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3153598.3462 - mae: 908.7464 - val_loss: 3458457.5000 - val_mae: 977.0966\n",
      "Epoch 61/256\n",
      "64/64 [==============================] - 0s 918us/step - loss: 3302365.3385 - mae: 923.3594 - val_loss: 3384187.0000 - val_mae: 936.0223\n",
      "Epoch 62/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3244076.3962 - mae: 906.2739 - val_loss: 3376647.0000 - val_mae: 945.4408\n",
      "Epoch 63/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3237263.8962 - mae: 917.1072 - val_loss: 3385691.0000 - val_mae: 941.3001\n",
      "Epoch 64/256\n",
      "64/64 [==============================] - 0s 949us/step - loss: 3170288.7000 - mae: 905.6849 - val_loss: 3377257.7500 - val_mae: 948.8776\n",
      "Epoch 65/256\n",
      "64/64 [==============================] - 0s 991us/step - loss: 3205703.4731 - mae: 924.7100 - val_loss: 3416826.7500 - val_mae: 961.6526\n",
      "Epoch 66/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3242077.1538 - mae: 916.6213 - val_loss: 3375134.5000 - val_mae: 943.1620\n",
      "Epoch 67/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3239459.9308 - mae: 916.7516 - val_loss: 3382879.2500 - val_mae: 939.3250\n",
      "Epoch 68/256\n",
      "64/64 [==============================] - 0s 930us/step - loss: 3252454.2538 - mae: 912.3961 - val_loss: 3369548.5000 - val_mae: 944.1552\n",
      "Epoch 69/256\n",
      "64/64 [==============================] - 0s 997us/step - loss: 3181917.6077 - mae: 907.2737 - val_loss: 3376308.0000 - val_mae: 944.7139\n",
      "Epoch 70/256\n",
      "64/64 [==============================] - 0s 923us/step - loss: 3178507.9962 - mae: 909.8747 - val_loss: 3375320.2500 - val_mae: 944.8903\n",
      "Epoch 71/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3193446.7692 - mae: 909.9878 - val_loss: 3375629.5000 - val_mae: 951.5251\n",
      "Epoch 72/256\n",
      "64/64 [==============================] - 0s 949us/step - loss: 3150356.8423 - mae: 911.3793 - val_loss: 3384159.0000 - val_mae: 955.3167\n",
      "Epoch 73/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3182725.6769 - mae: 917.0588 - val_loss: 3366988.0000 - val_mae: 947.8209\n",
      "Epoch 74/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3158406.5154 - mae: 914.1175 - val_loss: 3374885.5000 - val_mae: 941.7452\n",
      "Epoch 75/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3283782.7077 - mae: 916.2051 - val_loss: 3381394.5000 - val_mae: 955.1941\n",
      "Epoch 76/256\n",
      "64/64 [==============================] - 0s 981us/step - loss: 3170480.8808 - mae: 913.6471 - val_loss: 3370490.2500 - val_mae: 954.6267\n",
      "Epoch 77/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3214961.7962 - mae: 918.4631 - val_loss: 3375526.0000 - val_mae: 945.8593\n",
      "Epoch 78/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3295403.2192 - mae: 925.5568 - val_loss: 3371777.7500 - val_mae: 947.3391\n",
      "Epoch 79/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3213798.7077 - mae: 917.1776 - val_loss: 3365516.7500 - val_mae: 952.5642\n",
      "Epoch 80/256\n",
      "64/64 [==============================] - 0s 918us/step - loss: 3170106.1385 - mae: 920.5028 - val_loss: 3382037.0000 - val_mae: 961.6061\n",
      "Epoch 81/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3208092.6577 - mae: 916.3991 - val_loss: 3362476.7500 - val_mae: 950.1932\n",
      "Epoch 82/256\n",
      "64/64 [==============================] - 0s 935us/step - loss: 3175182.3731 - mae: 915.6013 - val_loss: 3364783.7500 - val_mae: 947.4688\n",
      "Epoch 83/256\n",
      "64/64 [==============================] - 0s 964us/step - loss: 3110901.4077 - mae: 907.3459 - val_loss: 3362840.0000 - val_mae: 948.9641\n",
      "Epoch 84/256\n",
      "64/64 [==============================] - 0s 944us/step - loss: 3148204.5808 - mae: 915.9859 - val_loss: 3406770.2500 - val_mae: 968.6403\n",
      "Epoch 85/256\n",
      "64/64 [==============================] - 0s 988us/step - loss: 3196788.8308 - mae: 922.1323 - val_loss: 3383828.7500 - val_mae: 947.3075\n",
      "Epoch 86/256\n",
      "64/64 [==============================] - 0s 966us/step - loss: 3286292.4500 - mae: 925.1017 - val_loss: 3372165.2500 - val_mae: 949.5516\n",
      "Epoch 87/256\n",
      "64/64 [==============================] - 0s 958us/step - loss: 3312419.5577 - mae: 928.2996 - val_loss: 3360926.2500 - val_mae: 952.5880\n",
      "Epoch 88/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3185325.6077 - mae: 920.9739 - val_loss: 3359638.5000 - val_mae: 949.9949\n",
      "Epoch 89/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3216459.9423 - mae: 923.7199 - val_loss: 3363821.2500 - val_mae: 960.6958\n",
      "Epoch 90/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3247514.6500 - mae: 927.1357 - val_loss: 3359192.7500 - val_mae: 952.8408\n",
      "Epoch 91/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3173840.1308 - mae: 922.2376 - val_loss: 3363554.0000 - val_mae: 953.9841\n",
      "Epoch 92/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3201278.2231 - mae: 928.9683 - val_loss: 3364367.7500 - val_mae: 958.5820\n",
      "Epoch 93/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3216452.4808 - mae: 924.0049 - val_loss: 3364833.2500 - val_mae: 961.0305\n",
      "Epoch 94/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3251662.2846 - mae: 930.5714 - val_loss: 3356171.0000 - val_mae: 956.1782\n",
      "Epoch 95/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3169848.3654 - mae: 922.1636 - val_loss: 3358933.5000 - val_mae: 954.1360\n",
      "Epoch 96/256\n",
      "64/64 [==============================] - 0s 959us/step - loss: 3233725.8231 - mae: 922.5066 - val_loss: 3355879.7500 - val_mae: 953.2231\n",
      "Epoch 97/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3127589.3154 - mae: 918.0826 - val_loss: 3357082.0000 - val_mae: 957.6342\n",
      "Epoch 98/256\n",
      "64/64 [==============================] - 0s 990us/step - loss: 3147412.4346 - mae: 923.6727 - val_loss: 3371373.2500 - val_mae: 956.9603\n",
      "Epoch 99/256\n",
      "64/64 [==============================] - 0s 954us/step - loss: 3244036.5962 - mae: 937.2355 - val_loss: 3372946.5000 - val_mae: 965.6802\n",
      "Epoch 100/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3177881.3769 - mae: 924.7757 - val_loss: 3374221.7500 - val_mae: 967.4656\n",
      "Epoch 101/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3227445.7192 - mae: 936.8214 - val_loss: 3359259.0000 - val_mae: 956.8403\n",
      "Epoch 102/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3175918.3423 - mae: 921.1860 - val_loss: 3375392.0000 - val_mae: 958.5320\n",
      "Epoch 103/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3118460.7077 - mae: 925.0744 - val_loss: 3354444.7500 - val_mae: 956.4303\n",
      "Epoch 104/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3122537.1269 - mae: 923.8079 - val_loss: 3358336.5000 - val_mae: 963.9108\n",
      "Epoch 105/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3123941.6808 - mae: 920.3926 - val_loss: 3354097.7500 - val_mae: 960.8319\n",
      "Epoch 106/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3182716.2731 - mae: 928.3394 - val_loss: 3356979.0000 - val_mae: 962.2182\n",
      "Epoch 107/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3195132.7154 - mae: 937.8862 - val_loss: 3359654.2500 - val_mae: 959.8883\n",
      "Epoch 108/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3093520.0808 - mae: 918.0273 - val_loss: 3390827.2500 - val_mae: 977.1234\n",
      "Epoch 109/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3177747.1231 - mae: 937.7772 - val_loss: 3405408.7500 - val_mae: 963.2294\n",
      "Epoch 110/256\n",
      "64/64 [==============================] - 0s 952us/step - loss: 3142998.4077 - mae: 927.3296 - val_loss: 3352606.5000 - val_mae: 963.0288\n",
      "Epoch 111/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3122122.7077 - mae: 923.1083 - val_loss: 3360772.7500 - val_mae: 965.2972\n",
      "Epoch 112/256\n",
      "64/64 [==============================] - 0s 980us/step - loss: 3208288.0038 - mae: 929.4018 - val_loss: 3357550.7500 - val_mae: 963.6098\n",
      "Epoch 113/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3191322.0885 - mae: 933.9285 - val_loss: 3373131.0000 - val_mae: 973.3109\n",
      "Epoch 114/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3180377.5846 - mae: 935.9514 - val_loss: 3429961.2500 - val_mae: 991.5171\n",
      "Epoch 115/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3266604.6846 - mae: 943.8641 - val_loss: 3353760.5000 - val_mae: 967.9770\n",
      "Epoch 116/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3278603.0231 - mae: 939.5158 - val_loss: 3355954.7500 - val_mae: 966.3633\n",
      "Epoch 117/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3127719.6000 - mae: 931.3021 - val_loss: 3352148.7500 - val_mae: 964.8634\n",
      "Epoch 118/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3246227.1000 - mae: 945.5536 - val_loss: 3350362.2500 - val_mae: 966.6861\n",
      "Epoch 119/256\n",
      "64/64 [==============================] - 0s 944us/step - loss: 3219642.0000 - mae: 938.7427 - val_loss: 3350994.0000 - val_mae: 965.2442\n",
      "Epoch 120/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3103970.7923 - mae: 933.4486 - val_loss: 3351809.7500 - val_mae: 965.6939\n",
      "Epoch 121/256\n",
      "64/64 [==============================] - 0s 989us/step - loss: 3158995.3231 - mae: 931.2693 - val_loss: 3363953.2500 - val_mae: 969.9526\n",
      "Epoch 122/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3211251.5000 - mae: 935.0309 - val_loss: 3351492.0000 - val_mae: 968.3958\n",
      "Epoch 123/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3156786.2808 - mae: 931.2894 - val_loss: 3350394.0000 - val_mae: 971.6531\n",
      "Epoch 124/256\n",
      "64/64 [==============================] - 0s 986us/step - loss: 3104218.8846 - mae: 929.8334 - val_loss: 3374973.2500 - val_mae: 979.6042\n",
      "Epoch 125/256\n",
      "64/64 [==============================] - 0s 993us/step - loss: 3238758.0154 - mae: 946.2318 - val_loss: 3380474.2500 - val_mae: 979.0629\n",
      "Epoch 126/256\n",
      "64/64 [==============================] - 0s 976us/step - loss: 3203049.9654 - mae: 943.5508 - val_loss: 3364619.5000 - val_mae: 977.6440\n",
      "Epoch 127/256\n",
      "64/64 [==============================] - 0s 991us/step - loss: 3168863.4577 - mae: 938.4547 - val_loss: 3354307.2500 - val_mae: 964.0325\n",
      "Epoch 128/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3174167.2000 - mae: 932.1184 - val_loss: 3365505.0000 - val_mae: 966.0392\n",
      "Epoch 129/256\n",
      "64/64 [==============================] - 0s 996us/step - loss: 3197592.0615 - mae: 940.4240 - val_loss: 3349920.7500 - val_mae: 969.0290\n",
      "Epoch 130/256\n",
      "64/64 [==============================] - 0s 971us/step - loss: 3262879.5500 - mae: 944.9342 - val_loss: 3350372.0000 - val_mae: 973.9620\n",
      "Epoch 131/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3072951.6000 - mae: 935.2176 - val_loss: 3382095.7500 - val_mae: 968.8655\n",
      "Epoch 132/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3175762.1577 - mae: 942.8258 - val_loss: 3348986.2500 - val_mae: 969.4376\n",
      "Epoch 133/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3140822.4885 - mae: 939.0427 - val_loss: 3381089.5000 - val_mae: 975.9388\n",
      "Epoch 134/256\n",
      "64/64 [==============================] - 0s 928us/step - loss: 3212282.8231 - mae: 941.8209 - val_loss: 3369833.7500 - val_mae: 980.2925\n",
      "Epoch 135/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3177694.4038 - mae: 944.5697 - val_loss: 3347719.7500 - val_mae: 974.3240\n",
      "Epoch 136/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3181651.3769 - mae: 943.1686 - val_loss: 3353932.2500 - val_mae: 967.7806\n",
      "Epoch 137/256\n",
      "64/64 [==============================] - 0s 984us/step - loss: 3222766.6731 - mae: 946.9099 - val_loss: 3379088.5000 - val_mae: 969.7205\n",
      "Epoch 138/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3177782.1308 - mae: 939.4877 - val_loss: 3347504.5000 - val_mae: 976.5869\n",
      "Epoch 139/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3285739.2885 - mae: 956.6191 - val_loss: 3374370.7500 - val_mae: 983.3131\n",
      "Epoch 140/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3043611.8731 - mae: 932.3641 - val_loss: 3384381.0000 - val_mae: 988.0248\n",
      "Epoch 141/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3254321.1538 - mae: 954.6082 - val_loss: 3352275.7500 - val_mae: 969.1342\n",
      "Epoch 142/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3216877.9654 - mae: 948.0289 - val_loss: 3346554.0000 - val_mae: 977.0779\n",
      "Epoch 143/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3184279.2231 - mae: 950.0712 - val_loss: 3349750.2500 - val_mae: 977.7001\n",
      "Epoch 144/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3139418.2692 - mae: 941.7192 - val_loss: 3343557.7500 - val_mae: 978.0391\n",
      "Epoch 145/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3158854.4423 - mae: 945.4140 - val_loss: 3341733.2500 - val_mae: 977.6636\n",
      "Epoch 146/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3107885.8731 - mae: 941.1257 - val_loss: 3348819.7500 - val_mae: 978.1035\n",
      "Epoch 147/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3170849.8192 - mae: 952.3813 - val_loss: 3365329.5000 - val_mae: 973.3632\n",
      "Epoch 148/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3158901.6192 - mae: 940.9389 - val_loss: 3381467.7500 - val_mae: 989.2074\n",
      "Epoch 149/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3182106.3192 - mae: 945.2880 - val_loss: 3353018.7500 - val_mae: 978.7482\n",
      "Epoch 150/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3225770.7192 - mae: 951.1873 - val_loss: 3350489.5000 - val_mae: 981.3563\n",
      "Epoch 151/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3149909.0962 - mae: 950.1423 - val_loss: 3365631.0000 - val_mae: 985.9710\n",
      "Epoch 152/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3226855.8038 - mae: 955.1463 - val_loss: 3348234.0000 - val_mae: 973.7147\n",
      "Epoch 153/256\n",
      "64/64 [==============================] - 0s 964us/step - loss: 3063409.3038 - mae: 941.0225 - val_loss: 3369025.2500 - val_mae: 984.1245\n",
      "Epoch 154/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3161179.3038 - mae: 944.5780 - val_loss: 3343105.7500 - val_mae: 980.8352\n",
      "Epoch 155/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3109183.3423 - mae: 944.8049 - val_loss: 3367951.0000 - val_mae: 987.6404\n",
      "Epoch 156/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3168120.5269 - mae: 950.8675 - val_loss: 3346821.7500 - val_mae: 976.6793\n",
      "Epoch 157/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3141137.9038 - mae: 940.1052 - val_loss: 3357534.7500 - val_mae: 980.1176\n",
      "Epoch 158/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3197246.2269 - mae: 954.0590 - val_loss: 3362073.0000 - val_mae: 977.1486\n",
      "Epoch 159/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3155190.3846 - mae: 948.7281 - val_loss: 3367537.7500 - val_mae: 983.2003\n",
      "Epoch 160/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3125978.2000 - mae: 947.3310 - val_loss: 3358980.2500 - val_mae: 979.6396\n",
      "Epoch 161/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3081258.6808 - mae: 942.2548 - val_loss: 3344644.0000 - val_mae: 983.7188\n",
      "Epoch 162/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3148380.8846 - mae: 947.6545 - val_loss: 3353445.7500 - val_mae: 976.8256\n",
      "Epoch 163/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3134614.7538 - mae: 946.8969 - val_loss: 3340550.7500 - val_mae: 984.5304\n",
      "Epoch 164/256\n",
      "64/64 [==============================] - 0s 974us/step - loss: 3225689.2423 - mae: 956.7060 - val_loss: 3351360.2500 - val_mae: 978.5557\n",
      "Epoch 165/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3199836.5231 - mae: 956.2518 - val_loss: 3396693.2500 - val_mae: 979.1104\n",
      "Epoch 166/256\n",
      "64/64 [==============================] - 0s 882us/step - loss: 3184855.6000 - mae: 952.2002 - val_loss: 3353680.2500 - val_mae: 984.9827\n",
      "Epoch 167/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3163656.8077 - mae: 949.9806 - val_loss: 3356096.2500 - val_mae: 983.6294\n",
      "Epoch 168/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3162419.3154 - mae: 950.4904 - val_loss: 3344729.2500 - val_mae: 982.8330\n",
      "Epoch 169/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3157154.3654 - mae: 954.4476 - val_loss: 3394008.7500 - val_mae: 995.0435\n",
      "Epoch 170/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3258165.2154 - mae: 963.7763 - val_loss: 3354956.2500 - val_mae: 982.6263\n",
      "Epoch 171/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3139961.2308 - mae: 952.9223 - val_loss: 3347761.0000 - val_mae: 986.4543\n",
      "Epoch 172/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3206883.2154 - mae: 955.9512 - val_loss: 3343987.0000 - val_mae: 982.6992\n",
      "Epoch 173/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3181624.6500 - mae: 959.1282 - val_loss: 3341388.2500 - val_mae: 982.3388\n",
      "Epoch 174/256\n",
      "64/64 [==============================] - 0s 984us/step - loss: 3186294.1231 - mae: 948.2576 - val_loss: 3345786.0000 - val_mae: 983.5774\n",
      "Epoch 175/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3226509.8346 - mae: 955.5595 - val_loss: 3413796.0000 - val_mae: 985.5397\n",
      "Epoch 176/256\n",
      "64/64 [==============================] - 0s 970us/step - loss: 3211121.9115 - mae: 955.0356 - val_loss: 3349817.7500 - val_mae: 986.5317\n",
      "Epoch 177/256\n",
      "64/64 [==============================] - 0s 967us/step - loss: 3170225.3462 - mae: 955.4510 - val_loss: 3342254.0000 - val_mae: 982.3872\n",
      "Epoch 178/256\n",
      "64/64 [==============================] - 0s 958us/step - loss: 3118179.1615 - mae: 946.9416 - val_loss: 3338405.2500 - val_mae: 981.8053\n",
      "Epoch 179/256\n",
      "64/64 [==============================] - 0s 975us/step - loss: 3116092.4462 - mae: 946.3654 - val_loss: 3344154.0000 - val_mae: 981.0912\n",
      "Epoch 180/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3193676.6077 - mae: 956.4087 - val_loss: 3341836.2500 - val_mae: 983.9674\n",
      "Epoch 181/256\n",
      "64/64 [==============================] - 0s 943us/step - loss: 3216930.3692 - mae: 962.6941 - val_loss: 3352927.0000 - val_mae: 986.0846\n",
      "Epoch 182/256\n",
      "64/64 [==============================] - 0s 964us/step - loss: 3156368.4654 - mae: 956.8055 - val_loss: 3374407.7500 - val_mae: 983.5617\n",
      "Epoch 183/256\n",
      "64/64 [==============================] - 0s 929us/step - loss: 3352407.1885 - mae: 971.6777 - val_loss: 3338584.0000 - val_mae: 988.1234\n",
      "Epoch 184/256\n",
      "64/64 [==============================] - 0s 998us/step - loss: 3146525.2577 - mae: 951.0603 - val_loss: 3342404.0000 - val_mae: 979.4470\n",
      "Epoch 185/256\n",
      "64/64 [==============================] - 0s 974us/step - loss: 3098981.2154 - mae: 950.0951 - val_loss: 3367773.0000 - val_mae: 991.8055\n",
      "Epoch 186/256\n",
      "64/64 [==============================] - 0s 999us/step - loss: 3210393.1615 - mae: 964.7689 - val_loss: 3337824.7500 - val_mae: 985.5306\n",
      "Epoch 187/256\n",
      "64/64 [==============================] - 0s 998us/step - loss: 3153154.0038 - mae: 960.4394 - val_loss: 3342533.5000 - val_mae: 991.4755\n",
      "Epoch 188/256\n",
      "64/64 [==============================] - 0s 984us/step - loss: 3150786.2385 - mae: 953.9075 - val_loss: 3346115.5000 - val_mae: 983.9308\n",
      "Epoch 189/256\n",
      "64/64 [==============================] - 0s 984us/step - loss: 3124886.2769 - mae: 950.3493 - val_loss: 3370071.5000 - val_mae: 995.3501\n",
      "Epoch 190/256\n",
      "64/64 [==============================] - 0s 961us/step - loss: 3223582.1231 - mae: 958.6641 - val_loss: 3339794.0000 - val_mae: 985.4499\n",
      "Epoch 191/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3083611.6577 - mae: 950.2297 - val_loss: 3364260.7500 - val_mae: 995.2902\n",
      "Epoch 192/256\n",
      "64/64 [==============================] - 0s 995us/step - loss: 3167765.3577 - mae: 960.3305 - val_loss: 3353673.7500 - val_mae: 986.1688\n",
      "Epoch 193/256\n",
      "64/64 [==============================] - 0s 971us/step - loss: 3106201.3962 - mae: 946.9065 - val_loss: 3363985.5000 - val_mae: 996.2943\n",
      "Epoch 194/256\n",
      "64/64 [==============================] - 0s 954us/step - loss: 3151131.7731 - mae: 961.5492 - val_loss: 3372308.2500 - val_mae: 988.9164\n",
      "Epoch 195/256\n",
      "64/64 [==============================] - 0s 971us/step - loss: 3171352.2769 - mae: 957.5801 - val_loss: 3352188.5000 - val_mae: 992.1306\n",
      "Epoch 196/256\n",
      "64/64 [==============================] - 0s 924us/step - loss: 3159064.0423 - mae: 958.7873 - val_loss: 3345343.5000 - val_mae: 983.4125\n",
      "Epoch 197/256\n",
      "64/64 [==============================] - 0s 959us/step - loss: 3174429.7846 - mae: 958.5979 - val_loss: 3359998.0000 - val_mae: 987.4813\n",
      "Epoch 198/256\n",
      "64/64 [==============================] - 0s 955us/step - loss: 3223534.5385 - mae: 968.0336 - val_loss: 3359936.0000 - val_mae: 989.0928\n",
      "Epoch 199/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3226309.1000 - mae: 964.2859 - val_loss: 3338395.7500 - val_mae: 986.7421\n",
      "Epoch 200/256\n",
      "64/64 [==============================] - 0s 953us/step - loss: 3192393.4577 - mae: 959.3027 - val_loss: 3336052.5000 - val_mae: 986.6116\n",
      "Epoch 201/256\n",
      "64/64 [==============================] - 0s 955us/step - loss: 3275675.8423 - mae: 965.8693 - val_loss: 3369678.0000 - val_mae: 991.0543\n",
      "Epoch 202/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3140878.4769 - mae: 954.0471 - val_loss: 3342744.0000 - val_mae: 987.5528\n",
      "Epoch 203/256\n",
      "64/64 [==============================] - 0s 908us/step - loss: 3171832.8692 - mae: 959.2139 - val_loss: 3345858.0000 - val_mae: 985.8182\n",
      "Epoch 204/256\n",
      "64/64 [==============================] - 0s 923us/step - loss: 3084663.3308 - mae: 946.0988 - val_loss: 3396883.2500 - val_mae: 1002.1778\n",
      "Epoch 205/256\n",
      "64/64 [==============================] - 0s 987us/step - loss: 3245672.9385 - mae: 975.4067 - val_loss: 3344403.5000 - val_mae: 983.4178\n",
      "Epoch 206/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3170153.4538 - mae: 951.4875 - val_loss: 3339588.7500 - val_mae: 987.1047\n",
      "Epoch 207/256\n",
      "64/64 [==============================] - 0s 965us/step - loss: 3136514.4577 - mae: 956.1254 - val_loss: 3349979.2500 - val_mae: 989.7479\n",
      "Epoch 208/256\n",
      "64/64 [==============================] - 0s 879us/step - loss: 3140095.6231 - mae: 957.4583 - val_loss: 3337627.5000 - val_mae: 989.9803\n",
      "Epoch 209/256\n",
      "64/64 [==============================] - 0s 971us/step - loss: 3303059.5077 - mae: 965.8386 - val_loss: 3340158.7500 - val_mae: 989.6306\n",
      "Epoch 210/256\n",
      "64/64 [==============================] - 0s 924us/step - loss: 3242883.6962 - mae: 962.3546 - val_loss: 3367735.2500 - val_mae: 998.5834\n",
      "Epoch 211/256\n",
      "64/64 [==============================] - 0s 961us/step - loss: 3277075.1346 - mae: 968.6485 - val_loss: 3337573.5000 - val_mae: 990.9397\n",
      "Epoch 212/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3238738.9731 - mae: 960.2711 - val_loss: 3333353.5000 - val_mae: 987.4056\n",
      "Epoch 213/256\n",
      "64/64 [==============================] - 0s 978us/step - loss: 3133282.2115 - mae: 952.9209 - val_loss: 3338686.5000 - val_mae: 984.9242\n",
      "Epoch 214/256\n",
      "64/64 [==============================] - 0s 921us/step - loss: 3139237.5731 - mae: 955.6782 - val_loss: 3404473.5000 - val_mae: 998.9505\n",
      "Epoch 215/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3194328.2500 - mae: 958.9377 - val_loss: 3336926.2500 - val_mae: 989.0242\n",
      "Epoch 216/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3202301.1154 - mae: 962.1045 - val_loss: 3334325.0000 - val_mae: 988.3514\n",
      "Epoch 217/256\n",
      "64/64 [==============================] - 0s 965us/step - loss: 3116434.5769 - mae: 956.8341 - val_loss: 3346219.5000 - val_mae: 994.2263\n",
      "Epoch 218/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3066801.6231 - mae: 944.5583 - val_loss: 3365790.5000 - val_mae: 996.2808\n",
      "Epoch 219/256\n",
      "64/64 [==============================] - 0s 943us/step - loss: 3161119.1808 - mae: 963.1898 - val_loss: 3340583.0000 - val_mae: 996.3085\n",
      "Epoch 220/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3194910.8538 - mae: 961.3439 - val_loss: 3339312.5000 - val_mae: 989.8416\n",
      "Epoch 221/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3262029.3846 - mae: 967.9308 - val_loss: 3361323.2500 - val_mae: 994.8542\n",
      "Epoch 222/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3184674.0269 - mae: 966.8718 - val_loss: 3343992.7500 - val_mae: 993.2027\n",
      "Epoch 223/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3132100.1923 - mae: 959.2876 - val_loss: 3338698.5000 - val_mae: 989.1569\n",
      "Epoch 224/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3221246.8577 - mae: 966.3454 - val_loss: 3343678.5000 - val_mae: 992.7928\n",
      "Epoch 225/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3154223.5000 - mae: 954.0147 - val_loss: 3344036.5000 - val_mae: 995.6096\n",
      "Epoch 226/256\n",
      "64/64 [==============================] - 0s 989us/step - loss: 3262797.6385 - mae: 963.2347 - val_loss: 3345191.0000 - val_mae: 988.7047\n",
      "Epoch 227/256\n",
      "64/64 [==============================] - 0s 994us/step - loss: 3178982.4154 - mae: 953.8288 - val_loss: 3335266.0000 - val_mae: 992.8333\n",
      "Epoch 228/256\n",
      "64/64 [==============================] - 0s 976us/step - loss: 3141716.2808 - mae: 959.9694 - val_loss: 3332355.5000 - val_mae: 989.5901\n",
      "Epoch 229/256\n",
      "64/64 [==============================] - 0s 972us/step - loss: 3108046.7077 - mae: 957.9599 - val_loss: 3333064.2500 - val_mae: 989.4196\n",
      "Epoch 230/256\n",
      "64/64 [==============================] - 0s 971us/step - loss: 3174150.3192 - mae: 964.8302 - val_loss: 3380932.0000 - val_mae: 999.7506\n",
      "Epoch 231/256\n",
      "64/64 [==============================] - 0s 953us/step - loss: 3134016.6846 - mae: 959.2438 - val_loss: 3334920.2500 - val_mae: 996.0341\n",
      "Epoch 232/256\n",
      "64/64 [==============================] - 0s 971us/step - loss: 3178354.5038 - mae: 959.0027 - val_loss: 3359915.2500 - val_mae: 997.5927\n",
      "Epoch 233/256\n",
      "64/64 [==============================] - 0s 893us/step - loss: 3104852.3423 - mae: 956.6877 - val_loss: 3337432.5000 - val_mae: 994.4102\n",
      "Epoch 234/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3210315.4577 - mae: 966.3672 - val_loss: 3332470.2500 - val_mae: 988.4921\n",
      "Epoch 235/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3192421.5385 - mae: 967.1126 - val_loss: 3334056.5000 - val_mae: 989.5602\n",
      "Epoch 236/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3187683.0692 - mae: 956.4085 - val_loss: 3337157.7500 - val_mae: 989.4005\n",
      "Epoch 237/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3103270.9923 - mae: 955.5862 - val_loss: 3343948.0000 - val_mae: 989.7042\n",
      "Epoch 238/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3156667.3423 - mae: 962.9029 - val_loss: 3338687.0000 - val_mae: 992.1229\n",
      "Epoch 239/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3176333.1154 - mae: 961.9251 - val_loss: 3337027.2500 - val_mae: 987.0808\n",
      "Epoch 240/256\n",
      "64/64 [==============================] - 0s 973us/step - loss: 3115779.6462 - mae: 956.4333 - val_loss: 3332350.2500 - val_mae: 988.6786\n",
      "Epoch 241/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3194615.6231 - mae: 965.4365 - val_loss: 3333849.2500 - val_mae: 988.8672\n",
      "Epoch 242/256\n",
      "64/64 [==============================] - 0s 932us/step - loss: 3145577.8500 - mae: 961.7323 - val_loss: 3335624.2500 - val_mae: 987.8173\n",
      "Epoch 243/256\n",
      "64/64 [==============================] - 0s 907us/step - loss: 3081010.2769 - mae: 951.9018 - val_loss: 3367457.0000 - val_mae: 1006.1126\n",
      "Epoch 244/256\n",
      "64/64 [==============================] - 0s 899us/step - loss: 3098812.5000 - mae: 959.0364 - val_loss: 3387663.5000 - val_mae: 1004.1921\n",
      "Epoch 245/256\n",
      "64/64 [==============================] - 0s 922us/step - loss: 3180157.5923 - mae: 969.5894 - val_loss: 3346809.2500 - val_mae: 990.7595\n",
      "Epoch 246/256\n",
      "64/64 [==============================] - 0s 996us/step - loss: 3130568.6846 - mae: 956.5217 - val_loss: 3337658.7500 - val_mae: 991.4973\n",
      "Epoch 247/256\n",
      "64/64 [==============================] - 0s 981us/step - loss: 3066517.0231 - mae: 954.2677 - val_loss: 3349039.7500 - val_mae: 996.0862\n",
      "Epoch 248/256\n",
      "64/64 [==============================] - 0s 961us/step - loss: 3119019.5192 - mae: 958.9669 - val_loss: 3353772.7500 - val_mae: 991.7662\n",
      "Epoch 249/256\n",
      "64/64 [==============================] - 0s 975us/step - loss: 3237193.1500 - mae: 964.0550 - val_loss: 3342248.0000 - val_mae: 989.9350\n",
      "Epoch 250/256\n",
      "64/64 [==============================] - 0s 912us/step - loss: 3142755.9962 - mae: 965.4960 - val_loss: 3368041.5000 - val_mae: 1002.6744\n",
      "Epoch 251/256\n",
      "64/64 [==============================] - 0s 977us/step - loss: 3136468.7231 - mae: 958.8632 - val_loss: 3336246.2500 - val_mae: 989.7734\n",
      "Epoch 252/256\n",
      "64/64 [==============================] - 0s 914us/step - loss: 3144475.7615 - mae: 961.5726 - val_loss: 3334264.0000 - val_mae: 993.1628\n",
      "Epoch 253/256\n",
      "64/64 [==============================] - 0s 974us/step - loss: 3167931.8846 - mae: 964.4588 - val_loss: 3338979.2500 - val_mae: 996.1323\n",
      "Epoch 254/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3161459.8962 - mae: 960.6985 - val_loss: 3343445.2500 - val_mae: 999.1196\n",
      "Epoch 255/256\n",
      "64/64 [==============================] - 0s 970us/step - loss: 3173789.0462 - mae: 971.5266 - val_loss: 3343391.5000 - val_mae: 988.3348\n",
      "Epoch 256/256\n",
      "64/64 [==============================] - 0s 997us/step - loss: 3131596.2269 - mae: 956.1531 - val_loss: 3334665.0000 - val_mae: 990.4553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd4ec5362d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras import Model\n",
    "from datetime import datetime\n",
    "\n",
    "weights = tf.constant(df.weight.to_numpy(dtype=np.float32))\n",
    "\n",
    "inp = Input(shape=(32,))\n",
    "out = Dense(1, activation=\"linear\")(inp)\n",
    "\n",
    "logdir = \"logs/scalars/linear-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X[train], Y[train], validation_data=(X[val], Y[val]), batch_size=1024, epochs=256, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "378a3e23-6fed-4fe1-94e5-f39fe3e56648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 886924266.8308 - mae: 16963.7884 - val_loss: 15419597.0000 - val_mae: 2385.0229\n",
      "Epoch 2/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 10863450.4923 - mae: 1953.0507 - val_loss: 5616393.5000 - val_mae: 1257.8107\n",
      "Epoch 3/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 5132920.8077 - mae: 1173.9135 - val_loss: 4869248.5000 - val_mae: 1136.0292\n",
      "Epoch 4/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4610910.3462 - mae: 1092.0388 - val_loss: 4576461.0000 - val_mae: 1094.9310\n",
      "Epoch 5/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4208299.3462 - mae: 1035.7893 - val_loss: 4350384.5000 - val_mae: 1049.9189\n",
      "Epoch 6/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 4023445.9462 - mae: 1004.0371 - val_loss: 4184804.2500 - val_mae: 1008.2059\n",
      "Epoch 7/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3890871.4962 - mae: 968.6000 - val_loss: 3856796.7500 - val_mae: 945.9319\n",
      "Epoch 8/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3537880.7308 - mae: 908.3259 - val_loss: 3600511.0000 - val_mae: 893.7473\n",
      "Epoch 9/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3270085.2962 - mae: 852.2231 - val_loss: 3347982.7500 - val_mae: 862.0735\n",
      "Epoch 10/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3057850.7385 - mae: 811.5769 - val_loss: 3209143.5000 - val_mae: 832.4272\n",
      "Epoch 11/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 3049322.7654 - mae: 797.2391 - val_loss: 3095974.2500 - val_mae: 798.8539\n",
      "Epoch 12/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2888286.7038 - mae: 765.5422 - val_loss: 2992332.7500 - val_mae: 789.4061\n",
      "Epoch 13/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2782710.2423 - mae: 755.1552 - val_loss: 2905114.2500 - val_mae: 762.1070\n",
      "Epoch 14/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2706374.3385 - mae: 725.8176 - val_loss: 2839945.2500 - val_mae: 750.0208\n",
      "Epoch 15/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2578508.7385 - mae: 714.2648 - val_loss: 2808573.7500 - val_mae: 771.1616\n",
      "Epoch 16/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2589849.7192 - mae: 711.0220 - val_loss: 2755470.5000 - val_mae: 725.3504\n",
      "Epoch 17/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2578626.3846 - mae: 692.1051 - val_loss: 2715970.7500 - val_mae: 717.4284\n",
      "Epoch 18/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2517363.4346 - mae: 687.8225 - val_loss: 2676762.5000 - val_mae: 712.6959\n",
      "Epoch 19/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2519381.3115 - mae: 681.0673 - val_loss: 2612110.2500 - val_mae: 702.0997\n",
      "Epoch 20/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2368947.1942 - mae: 668.4063 - val_loss: 2624432.0000 - val_mae: 705.6436\n",
      "Epoch 21/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2337686.1654 - mae: 671.8695 - val_loss: 2582102.0000 - val_mae: 697.7459\n",
      "Epoch 22/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2273956.9327 - mae: 655.3941 - val_loss: 2550288.0000 - val_mae: 691.9946\n",
      "Epoch 23/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2309584.7577 - mae: 652.2229 - val_loss: 2516226.7500 - val_mae: 684.4063\n",
      "Epoch 24/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2253050.8981 - mae: 647.8544 - val_loss: 2503110.2500 - val_mae: 680.7720\n",
      "Epoch 25/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2303527.4577 - mae: 644.8461 - val_loss: 2483985.7500 - val_mae: 677.1066\n",
      "Epoch 26/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2230977.8192 - mae: 638.0950 - val_loss: 2514116.7500 - val_mae: 675.5673\n",
      "Epoch 27/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2225865.1404 - mae: 637.3142 - val_loss: 2446107.5000 - val_mae: 662.5563\n",
      "Epoch 28/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2229271.6308 - mae: 632.6729 - val_loss: 2462004.2500 - val_mae: 670.6867\n",
      "Epoch 29/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2184914.6288 - mae: 623.4001 - val_loss: 2428647.7500 - val_mae: 661.5401\n",
      "Epoch 30/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2143924.7712 - mae: 618.8637 - val_loss: 2417165.5000 - val_mae: 664.7675\n",
      "Epoch 31/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2231449.2500 - mae: 632.1862 - val_loss: 2407210.5000 - val_mae: 655.5352\n",
      "Epoch 32/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2142483.9385 - mae: 616.9189 - val_loss: 2386453.7500 - val_mae: 655.0339\n",
      "Epoch 33/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2098137.8404 - mae: 616.4801 - val_loss: 2404186.2500 - val_mae: 657.3444\n",
      "Epoch 34/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2145510.1654 - mae: 615.7306 - val_loss: 2396650.2500 - val_mae: 656.6628\n",
      "Epoch 35/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2106250.6750 - mae: 615.0346 - val_loss: 2380513.0000 - val_mae: 657.2487\n",
      "Epoch 36/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2141743.9577 - mae: 617.1792 - val_loss: 2381410.5000 - val_mae: 661.2216\n",
      "Epoch 37/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2118187.3769 - mae: 613.3250 - val_loss: 2423254.7500 - val_mae: 661.2675\n",
      "Epoch 38/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2130774.8981 - mae: 615.9616 - val_loss: 2352809.0000 - val_mae: 646.3007\n",
      "Epoch 39/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2162969.0673 - mae: 615.5412 - val_loss: 2341010.5000 - val_mae: 645.9863\n",
      "Epoch 40/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2214135.2308 - mae: 626.4840 - val_loss: 2504813.5000 - val_mae: 693.4088\n",
      "Epoch 41/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2136267.8846 - mae: 629.2494 - val_loss: 2331129.2500 - val_mae: 646.9958\n",
      "Epoch 42/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2097724.6250 - mae: 607.6820 - val_loss: 2372399.2500 - val_mae: 673.6855\n",
      "Epoch 43/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2102021.8923 - mae: 624.1843 - val_loss: 2323734.7500 - val_mae: 645.1919\n",
      "Epoch 44/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1988050.8885 - mae: 597.0396 - val_loss: 2320533.2500 - val_mae: 639.8672\n",
      "Epoch 45/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2064195.7308 - mae: 598.4728 - val_loss: 2325091.0000 - val_mae: 649.2733\n",
      "Epoch 46/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2041311.8923 - mae: 607.9107 - val_loss: 2346284.0000 - val_mae: 660.4639\n",
      "Epoch 47/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2011931.1308 - mae: 597.6211 - val_loss: 2300943.7500 - val_mae: 639.8636\n",
      "Epoch 48/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2025473.7346 - mae: 599.2367 - val_loss: 2300387.5000 - val_mae: 645.3123\n",
      "Epoch 49/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2048671.6192 - mae: 609.6381 - val_loss: 2351002.7500 - val_mae: 669.1974\n",
      "Epoch 50/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2040559.6942 - mae: 614.2158 - val_loss: 2317437.7500 - val_mae: 656.0229\n",
      "Epoch 51/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2010488.7077 - mae: 598.2580 - val_loss: 2326758.2500 - val_mae: 658.9870\n",
      "Epoch 52/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2025444.3365 - mae: 609.0394 - val_loss: 2301214.0000 - val_mae: 647.5322\n",
      "Epoch 53/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1985698.1327 - mae: 596.8026 - val_loss: 2332240.7500 - val_mae: 644.2816\n",
      "Epoch 54/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2013482.2577 - mae: 596.0652 - val_loss: 2294526.0000 - val_mae: 637.4371\n",
      "Epoch 55/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1968813.8135 - mae: 597.4788 - val_loss: 2314033.0000 - val_mae: 642.9997\n",
      "Epoch 56/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2067682.5692 - mae: 605.1625 - val_loss: 2296057.5000 - val_mae: 640.0753\n",
      "Epoch 57/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2010097.5731 - mae: 609.0891 - val_loss: 2274644.2500 - val_mae: 649.9548\n",
      "Epoch 58/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2036022.8808 - mae: 603.9477 - val_loss: 2276271.5000 - val_mae: 635.4039\n",
      "Epoch 59/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2046232.6173 - mae: 602.2265 - val_loss: 2289043.7500 - val_mae: 656.1262\n",
      "Epoch 60/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1986113.7173 - mae: 608.2096 - val_loss: 2325988.7500 - val_mae: 664.9751\n",
      "Epoch 61/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2008806.9981 - mae: 602.4219 - val_loss: 2273734.0000 - val_mae: 647.7795\n",
      "Epoch 62/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2043625.7558 - mae: 601.3234 - val_loss: 2315330.0000 - val_mae: 670.5232\n",
      "Epoch 63/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1996201.5942 - mae: 604.5421 - val_loss: 2405317.0000 - val_mae: 705.4561\n",
      "Epoch 64/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2021774.2115 - mae: 605.3964 - val_loss: 2273158.5000 - val_mae: 638.6821\n",
      "Epoch 65/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1980606.2442 - mae: 601.9557 - val_loss: 2284486.5000 - val_mae: 635.2913\n",
      "Epoch 66/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2032715.3442 - mae: 605.8416 - val_loss: 2296734.2500 - val_mae: 643.8180\n",
      "Epoch 67/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1968808.3577 - mae: 600.6475 - val_loss: 2281891.2500 - val_mae: 632.1817\n",
      "Epoch 68/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1942386.5885 - mae: 589.1445 - val_loss: 2277954.0000 - val_mae: 635.8835\n",
      "Epoch 69/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1919421.0500 - mae: 589.3552 - val_loss: 2274582.2500 - val_mae: 642.7244\n",
      "Epoch 70/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1956776.8385 - mae: 600.3176 - val_loss: 2370992.5000 - val_mae: 685.1536\n",
      "Epoch 71/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1968804.8212 - mae: 605.0116 - val_loss: 2330159.7500 - val_mae: 677.8042\n",
      "Epoch 72/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1904908.9173 - mae: 596.0418 - val_loss: 2309363.5000 - val_mae: 638.4904\n",
      "Epoch 73/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1925758.3365 - mae: 586.5954 - val_loss: 2262524.0000 - val_mae: 643.2383\n",
      "Epoch 74/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1909944.7327 - mae: 598.3771 - val_loss: 2275998.0000 - val_mae: 648.3528\n",
      "Epoch 75/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1946095.2788 - mae: 602.3020 - val_loss: 2266280.5000 - val_mae: 632.2898\n",
      "Epoch 76/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1978088.8788 - mae: 599.7941 - val_loss: 2255042.5000 - val_mae: 638.3811\n",
      "Epoch 77/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1980479.2385 - mae: 595.4963 - val_loss: 2259648.2500 - val_mae: 637.1359\n",
      "Epoch 78/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2011146.2577 - mae: 598.0603 - val_loss: 2286378.5000 - val_mae: 656.9030\n",
      "Epoch 79/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1952427.7385 - mae: 603.5624 - val_loss: 2279460.0000 - val_mae: 640.2125\n",
      "Epoch 80/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2014011.4096 - mae: 595.7159 - val_loss: 2282406.0000 - val_mae: 647.2348\n",
      "Epoch 81/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2034680.9712 - mae: 599.0774 - val_loss: 2262920.7500 - val_mae: 642.0391\n",
      "Epoch 82/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1964241.1904 - mae: 600.0091 - val_loss: 2276806.0000 - val_mae: 645.1097\n",
      "Epoch 83/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1951214.1154 - mae: 602.4680 - val_loss: 2256846.7500 - val_mae: 660.2726\n",
      "Epoch 84/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1949608.4058 - mae: 602.3999 - val_loss: 2259190.5000 - val_mae: 653.3076\n",
      "Epoch 85/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1950212.5462 - mae: 595.0558 - val_loss: 2307749.0000 - val_mae: 675.6762\n",
      "Epoch 86/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1998558.0269 - mae: 612.4591 - val_loss: 2267279.7500 - val_mae: 637.8920\n",
      "Epoch 87/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1980454.9077 - mae: 604.3958 - val_loss: 2281634.5000 - val_mae: 639.6829\n",
      "Epoch 88/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1907877.3538 - mae: 594.4156 - val_loss: 2299972.7500 - val_mae: 654.5623\n",
      "Epoch 89/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1866239.9000 - mae: 587.6592 - val_loss: 2235876.7500 - val_mae: 629.1918\n",
      "Epoch 90/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1996409.1154 - mae: 592.0038 - val_loss: 2421559.0000 - val_mae: 691.7630\n",
      "Epoch 91/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2002597.0423 - mae: 613.3065 - val_loss: 2470373.7500 - val_mae: 717.9745\n",
      "Epoch 92/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2012335.1673 - mae: 609.4733 - val_loss: 2244997.2500 - val_mae: 630.4733\n",
      "Epoch 93/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1937277.8288 - mae: 585.4408 - val_loss: 2267929.0000 - val_mae: 631.7111\n",
      "Epoch 94/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1895610.0481 - mae: 591.7408 - val_loss: 2248268.7500 - val_mae: 628.8315\n",
      "Epoch 95/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1975344.0404 - mae: 606.9258 - val_loss: 2258330.0000 - val_mae: 633.9372\n",
      "Epoch 96/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1926091.8885 - mae: 596.4626 - val_loss: 2291662.5000 - val_mae: 636.4944\n",
      "Epoch 97/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1969528.2519 - mae: 598.8435 - val_loss: 2305283.7500 - val_mae: 650.0775\n",
      "Epoch 98/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1945144.4250 - mae: 596.3531 - val_loss: 2337241.7500 - val_mae: 682.3750\n",
      "Epoch 99/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1986486.2769 - mae: 596.8324 - val_loss: 2282932.7500 - val_mae: 644.9528\n",
      "Epoch 100/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 2009849.6096 - mae: 600.7243 - val_loss: 2253213.2500 - val_mae: 627.3105\n",
      "Epoch 101/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1956178.4538 - mae: 587.8528 - val_loss: 2253302.2500 - val_mae: 629.5450\n",
      "Epoch 102/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1955564.2865 - mae: 593.4940 - val_loss: 2256242.0000 - val_mae: 622.1442\n",
      "Epoch 103/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1906859.5346 - mae: 589.5244 - val_loss: 2261836.7500 - val_mae: 630.8881\n",
      "Epoch 104/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2027315.3731 - mae: 600.6933 - val_loss: 2243458.0000 - val_mae: 621.7964\n",
      "Epoch 105/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2026786.8096 - mae: 610.6821 - val_loss: 2268574.2500 - val_mae: 644.5597\n",
      "Epoch 106/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1935900.2981 - mae: 597.7726 - val_loss: 2252998.0000 - val_mae: 652.6572\n",
      "Epoch 107/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1969161.7442 - mae: 587.6236 - val_loss: 2304726.0000 - val_mae: 645.9550\n",
      "Epoch 108/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2037929.7269 - mae: 613.9012 - val_loss: 2211222.5000 - val_mae: 618.4469\n",
      "Epoch 109/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1924936.5769 - mae: 586.3515 - val_loss: 2229033.7500 - val_mae: 643.9395\n",
      "Epoch 110/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1955603.3019 - mae: 602.5061 - val_loss: 2259660.5000 - val_mae: 643.6976\n",
      "Epoch 111/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1915553.6135 - mae: 592.5519 - val_loss: 2265322.7500 - val_mae: 648.8179\n",
      "Epoch 112/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1964392.3865 - mae: 597.0217 - val_loss: 2239235.5000 - val_mae: 626.2831\n",
      "Epoch 113/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1922773.5885 - mae: 596.7335 - val_loss: 2227458.2500 - val_mae: 661.8729\n",
      "Epoch 114/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1961646.8481 - mae: 608.0979 - val_loss: 2277751.2500 - val_mae: 642.8993\n",
      "Epoch 115/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2053048.3250 - mae: 615.8676 - val_loss: 2228697.7500 - val_mae: 631.2820\n",
      "Epoch 116/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1957270.9635 - mae: 589.0023 - val_loss: 2258767.2500 - val_mae: 633.9435\n",
      "Epoch 117/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1947076.9788 - mae: 593.4370 - val_loss: 2239468.5000 - val_mae: 630.6834\n",
      "Epoch 118/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1934533.5692 - mae: 605.9320 - val_loss: 2303993.5000 - val_mae: 653.6957\n",
      "Epoch 119/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1960795.8596 - mae: 599.3165 - val_loss: 2215257.5000 - val_mae: 625.9402\n",
      "Epoch 120/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1837113.0673 - mae: 580.9199 - val_loss: 2217380.0000 - val_mae: 622.4401\n",
      "Epoch 121/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1939765.6269 - mae: 595.0070 - val_loss: 2213709.2500 - val_mae: 625.1700\n",
      "Epoch 122/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1861028.6365 - mae: 581.3124 - val_loss: 2230056.0000 - val_mae: 618.9317\n",
      "Epoch 123/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1951315.4615 - mae: 590.4029 - val_loss: 2196318.7500 - val_mae: 618.4650\n",
      "Epoch 124/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1925609.1962 - mae: 588.4070 - val_loss: 2240129.2500 - val_mae: 651.1330\n",
      "Epoch 125/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1899813.7135 - mae: 595.4913 - val_loss: 2257322.0000 - val_mae: 628.7225\n",
      "Epoch 126/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1940485.2250 - mae: 589.6239 - val_loss: 2224864.5000 - val_mae: 631.4919\n",
      "Epoch 127/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1981554.5154 - mae: 604.8089 - val_loss: 2259287.0000 - val_mae: 667.3164\n",
      "Epoch 128/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1895977.4904 - mae: 596.1344 - val_loss: 2216643.5000 - val_mae: 619.3481\n",
      "Epoch 129/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1885411.1019 - mae: 579.0831 - val_loss: 2274549.7500 - val_mae: 647.3297\n",
      "Epoch 130/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 2007882.2019 - mae: 622.1624 - val_loss: 2281780.5000 - val_mae: 658.8465\n",
      "Epoch 131/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1891985.1019 - mae: 583.6479 - val_loss: 2220667.2500 - val_mae: 631.2653\n",
      "Epoch 132/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1901061.2846 - mae: 582.5543 - val_loss: 2240348.0000 - val_mae: 637.6816\n",
      "Epoch 133/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1902403.1365 - mae: 584.2003 - val_loss: 2241193.2500 - val_mae: 667.5770\n",
      "Epoch 134/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1901218.3442 - mae: 595.4112 - val_loss: 2229537.5000 - val_mae: 622.6976\n",
      "Epoch 135/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1930598.3596 - mae: 584.8077 - val_loss: 2281361.2500 - val_mae: 652.0385\n",
      "Epoch 136/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1960510.8577 - mae: 599.6664 - val_loss: 2253175.7500 - val_mae: 635.7419\n",
      "Epoch 137/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1937687.3058 - mae: 610.5106 - val_loss: 2213075.2500 - val_mae: 627.6686\n",
      "Epoch 138/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1893146.5019 - mae: 590.4038 - val_loss: 2189797.7500 - val_mae: 614.6561\n",
      "Epoch 139/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1897062.2231 - mae: 580.7731 - val_loss: 2290618.0000 - val_mae: 679.0702\n",
      "Epoch 140/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1967847.1423 - mae: 615.5362 - val_loss: 2224496.7500 - val_mae: 628.6830\n",
      "Epoch 141/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1956143.9058 - mae: 588.4855 - val_loss: 2210187.2500 - val_mae: 658.5255\n",
      "Epoch 142/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1936467.0423 - mae: 603.1641 - val_loss: 2203014.7500 - val_mae: 616.9252\n",
      "Epoch 143/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1904337.2250 - mae: 593.4032 - val_loss: 2236278.2500 - val_mae: 634.0765\n",
      "Epoch 144/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1907210.1346 - mae: 599.2186 - val_loss: 2216408.7500 - val_mae: 627.4685\n",
      "Epoch 145/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1895627.5212 - mae: 592.5952 - val_loss: 2263605.0000 - val_mae: 643.7939\n",
      "Epoch 146/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1867621.8788 - mae: 584.8856 - val_loss: 2231350.2500 - val_mae: 631.8598\n",
      "Epoch 147/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1875472.0135 - mae: 587.7483 - val_loss: 2187906.7500 - val_mae: 622.8470\n",
      "Epoch 148/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1840420.4788 - mae: 586.6268 - val_loss: 2187201.2500 - val_mae: 626.4307\n",
      "Epoch 149/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1891203.3481 - mae: 591.0305 - val_loss: 2264705.5000 - val_mae: 647.9422\n",
      "Epoch 150/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1844822.2058 - mae: 581.6909 - val_loss: 2208082.0000 - val_mae: 663.5677\n",
      "Epoch 151/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1948956.5077 - mae: 614.4533 - val_loss: 2193910.2500 - val_mae: 625.3875\n",
      "Epoch 152/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1964677.1500 - mae: 598.0507 - val_loss: 2183125.0000 - val_mae: 632.5123\n",
      "Epoch 153/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1863103.0115 - mae: 580.0656 - val_loss: 2265287.0000 - val_mae: 643.8934\n",
      "Epoch 154/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1930936.3000 - mae: 609.1152 - val_loss: 2188670.7500 - val_mae: 645.8527\n",
      "Epoch 155/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1920321.9731 - mae: 593.5870 - val_loss: 2192721.0000 - val_mae: 626.4073\n",
      "Epoch 156/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1881823.6481 - mae: 590.8148 - val_loss: 2199124.2500 - val_mae: 636.3901\n",
      "Epoch 157/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1881500.4712 - mae: 580.5033 - val_loss: 2189986.2500 - val_mae: 630.6998\n",
      "Epoch 158/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1891148.9481 - mae: 588.6637 - val_loss: 2235611.5000 - val_mae: 649.8936\n",
      "Epoch 159/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1955310.5423 - mae: 597.4182 - val_loss: 2343849.0000 - val_mae: 663.3117\n",
      "Epoch 160/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1988893.7654 - mae: 617.2987 - val_loss: 2218164.7500 - val_mae: 641.3434\n",
      "Epoch 161/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1877609.7365 - mae: 584.8876 - val_loss: 2212745.5000 - val_mae: 627.8049\n",
      "Epoch 162/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1860039.3327 - mae: 579.3031 - val_loss: 2204566.7500 - val_mae: 651.4578\n",
      "Epoch 163/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1858689.3327 - mae: 593.7212 - val_loss: 2242106.7500 - val_mae: 663.7409\n",
      "Epoch 164/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1825079.5173 - mae: 581.6166 - val_loss: 2172544.5000 - val_mae: 614.1312\n",
      "Epoch 165/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1951955.3115 - mae: 590.2155 - val_loss: 2145023.0000 - val_mae: 622.9631\n",
      "Epoch 166/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1878815.4846 - mae: 587.3651 - val_loss: 2193181.5000 - val_mae: 619.7916\n",
      "Epoch 167/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1925836.1596 - mae: 585.0724 - val_loss: 2218794.7500 - val_mae: 648.2256\n",
      "Epoch 168/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1845445.1269 - mae: 574.6372 - val_loss: 2194831.5000 - val_mae: 647.2974\n",
      "Epoch 169/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1892727.9635 - mae: 602.6321 - val_loss: 2170854.0000 - val_mae: 619.3085\n",
      "Epoch 170/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1904328.4808 - mae: 601.1554 - val_loss: 2257389.2500 - val_mae: 693.1985\n",
      "Epoch 171/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1890241.0519 - mae: 596.8514 - val_loss: 2278834.7500 - val_mae: 650.2483\n",
      "Epoch 172/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1880459.5288 - mae: 595.5066 - val_loss: 2198450.5000 - val_mae: 640.5135\n",
      "Epoch 173/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1879955.0558 - mae: 585.3108 - val_loss: 2166054.7500 - val_mae: 623.8787\n",
      "Epoch 174/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1886979.5365 - mae: 587.9422 - val_loss: 2186266.2500 - val_mae: 632.6681\n",
      "Epoch 175/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1916371.2942 - mae: 600.5152 - val_loss: 2169360.2500 - val_mae: 623.2371\n",
      "Epoch 176/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1934144.1019 - mae: 598.0186 - val_loss: 2169996.5000 - val_mae: 635.3483\n",
      "Epoch 177/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1909030.7885 - mae: 607.4676 - val_loss: 2184901.0000 - val_mae: 659.3561\n",
      "Epoch 178/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1915154.3538 - mae: 597.0738 - val_loss: 2169590.2500 - val_mae: 620.9515\n",
      "Epoch 179/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1837260.3423 - mae: 576.2437 - val_loss: 2181475.0000 - val_mae: 625.4684\n",
      "Epoch 180/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1858140.6096 - mae: 580.8375 - val_loss: 2139259.2500 - val_mae: 615.0833\n",
      "Epoch 181/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1861645.4212 - mae: 576.2765 - val_loss: 2134911.2500 - val_mae: 621.6692\n",
      "Epoch 182/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1853915.2923 - mae: 582.3152 - val_loss: 2129618.7500 - val_mae: 629.2496\n",
      "Epoch 183/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1875958.4615 - mae: 582.0451 - val_loss: 2138222.5000 - val_mae: 620.6590\n",
      "Epoch 184/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1850783.5981 - mae: 578.4717 - val_loss: 2141655.2500 - val_mae: 615.2621\n",
      "Epoch 185/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1760786.7654 - mae: 563.1529 - val_loss: 2426930.0000 - val_mae: 745.3658\n",
      "Epoch 186/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1875649.3327 - mae: 607.1853 - val_loss: 2158647.7500 - val_mae: 612.7772\n",
      "Epoch 187/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1883461.6519 - mae: 594.9506 - val_loss: 2154265.7500 - val_mae: 630.9487\n",
      "Epoch 188/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1821958.4404 - mae: 580.2605 - val_loss: 2168831.2500 - val_mae: 612.4269\n",
      "Epoch 189/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1827850.8962 - mae: 586.5456 - val_loss: 2155076.2500 - val_mae: 611.2080\n",
      "Epoch 190/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1930394.5231 - mae: 593.0724 - val_loss: 2174631.2500 - val_mae: 617.1506\n",
      "Epoch 191/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1848976.1385 - mae: 586.2863 - val_loss: 2268018.7500 - val_mae: 657.4257\n",
      "Epoch 192/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1889739.7115 - mae: 608.2213 - val_loss: 2168592.0000 - val_mae: 637.4429\n",
      "Epoch 193/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1854083.7885 - mae: 582.9106 - val_loss: 2194975.5000 - val_mae: 648.6443\n",
      "Epoch 194/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1870236.4442 - mae: 579.2850 - val_loss: 2121081.2500 - val_mae: 620.9136\n",
      "Epoch 195/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1830694.2577 - mae: 580.7042 - val_loss: 2148682.5000 - val_mae: 608.1943\n",
      "Epoch 196/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1873615.1423 - mae: 591.6100 - val_loss: 2215388.7500 - val_mae: 657.9852\n",
      "Epoch 197/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1807220.0000 - mae: 569.8012 - val_loss: 2120675.5000 - val_mae: 612.3574\n",
      "Epoch 198/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1795729.6346 - mae: 569.9670 - val_loss: 2220361.7500 - val_mae: 662.8785\n",
      "Epoch 199/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1881125.3019 - mae: 597.7361 - val_loss: 2165971.7500 - val_mae: 649.1006\n",
      "Epoch 200/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1867043.9827 - mae: 593.5482 - val_loss: 2181526.7500 - val_mae: 621.2808\n",
      "Epoch 201/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1859765.8654 - mae: 593.1165 - val_loss: 2120620.0000 - val_mae: 612.9216\n",
      "Epoch 202/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1832664.8500 - mae: 582.7910 - val_loss: 2137791.5000 - val_mae: 619.6130\n",
      "Epoch 203/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1811632.0519 - mae: 575.4400 - val_loss: 2109675.0000 - val_mae: 616.3182\n",
      "Epoch 204/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1814361.4404 - mae: 577.8561 - val_loss: 2155625.7500 - val_mae: 628.0845\n",
      "Epoch 205/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1768646.7038 - mae: 572.2573 - val_loss: 2123973.0000 - val_mae: 613.5434\n",
      "Epoch 206/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1762863.9385 - mae: 571.9018 - val_loss: 2136569.5000 - val_mae: 615.5397\n",
      "Epoch 207/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1870890.2269 - mae: 593.4024 - val_loss: 2130797.7500 - val_mae: 621.5179\n",
      "Epoch 208/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1874297.0904 - mae: 586.3501 - val_loss: 2116936.0000 - val_mae: 624.1003\n",
      "Epoch 209/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1834420.3865 - mae: 586.5471 - val_loss: 2107971.5000 - val_mae: 611.2040\n",
      "Epoch 210/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1821269.0442 - mae: 586.1464 - val_loss: 2128117.7500 - val_mae: 633.6234\n",
      "Epoch 211/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1852516.3365 - mae: 588.0415 - val_loss: 2244536.5000 - val_mae: 679.0406\n",
      "Epoch 212/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1974515.7615 - mae: 626.3368 - val_loss: 2109825.7500 - val_mae: 627.0172\n",
      "Epoch 213/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1855535.6500 - mae: 585.7202 - val_loss: 2104348.2500 - val_mae: 607.7159\n",
      "Epoch 214/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1863448.0731 - mae: 586.8486 - val_loss: 2134632.0000 - val_mae: 620.7621\n",
      "Epoch 215/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1800811.9538 - mae: 573.5067 - val_loss: 2102630.5000 - val_mae: 621.2455\n",
      "Epoch 216/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1882852.2308 - mae: 592.5726 - val_loss: 2093046.8750 - val_mae: 606.8542\n",
      "Epoch 217/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1822867.2135 - mae: 574.9849 - val_loss: 2148939.5000 - val_mae: 663.5122\n",
      "Epoch 218/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1844743.6077 - mae: 604.4335 - val_loss: 2120839.0000 - val_mae: 614.5146\n",
      "Epoch 219/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1856830.3577 - mae: 588.2611 - val_loss: 2097919.0000 - val_mae: 620.3173\n",
      "Epoch 220/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1838351.9981 - mae: 587.0699 - val_loss: 2147333.0000 - val_mae: 630.1409\n",
      "Epoch 221/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1759536.3192 - mae: 577.8022 - val_loss: 2119297.0000 - val_mae: 624.7714\n",
      "Epoch 222/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1899299.7500 - mae: 612.1872 - val_loss: 2153054.5000 - val_mae: 648.7388\n",
      "Epoch 223/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1804419.9404 - mae: 587.9776 - val_loss: 2156215.7500 - val_mae: 638.7148\n",
      "Epoch 224/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1836305.4596 - mae: 599.2191 - val_loss: 2097426.2500 - val_mae: 614.1896\n",
      "Epoch 225/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1792902.9596 - mae: 584.8578 - val_loss: 2114932.5000 - val_mae: 643.0491\n",
      "Epoch 226/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1810088.2115 - mae: 579.6327 - val_loss: 2099402.5000 - val_mae: 625.7458\n",
      "Epoch 227/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1811252.1058 - mae: 581.5999 - val_loss: 2100131.2500 - val_mae: 635.5677\n",
      "Epoch 228/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1844185.3865 - mae: 593.5280 - val_loss: 2126285.7500 - val_mae: 632.2684\n",
      "Epoch 229/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1835041.9038 - mae: 590.3662 - val_loss: 2124945.5000 - val_mae: 643.4236\n",
      "Epoch 230/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1790641.7288 - mae: 581.3782 - val_loss: 2080219.8750 - val_mae: 612.4067\n",
      "Epoch 231/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1753764.4038 - mae: 570.4230 - val_loss: 2080022.8750 - val_mae: 615.8571\n",
      "Epoch 232/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1793748.7423 - mae: 587.1961 - val_loss: 2092928.5000 - val_mae: 622.7770\n",
      "Epoch 233/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1905237.3692 - mae: 597.9989 - val_loss: 2100169.2500 - val_mae: 628.1078\n",
      "Epoch 234/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1841156.0077 - mae: 593.0646 - val_loss: 2086464.0000 - val_mae: 619.0187\n",
      "Epoch 235/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1854603.7500 - mae: 597.0898 - val_loss: 2146181.2500 - val_mae: 630.7369\n",
      "Epoch 236/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1852940.2096 - mae: 584.8120 - val_loss: 2080403.1250 - val_mae: 616.2175\n",
      "Epoch 237/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1759615.9654 - mae: 568.2951 - val_loss: 2066722.6250 - val_mae: 608.5001\n",
      "Epoch 238/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1848000.4750 - mae: 590.2667 - val_loss: 2086860.5000 - val_mae: 630.4856\n",
      "Epoch 239/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1783775.8423 - mae: 579.9730 - val_loss: 2106722.5000 - val_mae: 627.5850\n",
      "Epoch 240/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1791895.4731 - mae: 585.5672 - val_loss: 2124119.7500 - val_mae: 649.4681\n",
      "Epoch 241/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1791116.9019 - mae: 591.9822 - val_loss: 2088944.0000 - val_mae: 627.2408\n",
      "Epoch 242/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1846509.8692 - mae: 584.8426 - val_loss: 2213534.7500 - val_mae: 674.4354\n",
      "Epoch 243/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1796220.6827 - mae: 586.1517 - val_loss: 2289769.5000 - val_mae: 714.8960\n",
      "Epoch 244/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1822834.5962 - mae: 611.2366 - val_loss: 2066729.3750 - val_mae: 619.1387\n",
      "Epoch 245/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1755650.1769 - mae: 573.8727 - val_loss: 2117199.7500 - val_mae: 660.7167\n",
      "Epoch 246/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1846526.4538 - mae: 609.0304 - val_loss: 2105713.5000 - val_mae: 638.2684\n",
      "Epoch 247/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1817371.8846 - mae: 597.1118 - val_loss: 2089879.2500 - val_mae: 611.2237\n",
      "Epoch 248/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1844202.4827 - mae: 601.1263 - val_loss: 2101247.2500 - val_mae: 634.3555\n",
      "Epoch 249/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1796761.4385 - mae: 584.4773 - val_loss: 2249408.2500 - val_mae: 686.1812\n",
      "Epoch 250/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1790423.2846 - mae: 598.2136 - val_loss: 2114520.7500 - val_mae: 626.9037\n",
      "Epoch 251/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1798592.2769 - mae: 581.4403 - val_loss: 2158222.7500 - val_mae: 657.4937\n",
      "Epoch 252/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1834854.8673 - mae: 603.8551 - val_loss: 2067148.3750 - val_mae: 627.3740\n",
      "Epoch 253/256\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 1772209.0635 - mae: 591.0442 - val_loss: 2115524.0000 - val_mae: 634.8378\n",
      "Epoch 254/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1803624.1635 - mae: 594.5149 - val_loss: 2068333.8750 - val_mae: 620.1841\n",
      "Epoch 255/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1868984.3538 - mae: 597.6242 - val_loss: 2079988.5000 - val_mae: 622.9630\n",
      "Epoch 256/256\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1761581.1519 - mae: 581.1883 - val_loss: 2168127.0000 - val_mae: 637.7869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd4ec374f90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras import Model\n",
    "from datetime import datetime\n",
    "\n",
    "weights = tf.constant(df.weight.to_numpy(dtype=np.float32))\n",
    "\n",
    "inp = Input(shape=(32,))\n",
    "x = Dense(16, activation=\"relu\")(inp)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "out = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "logdir = \"logs/scalars/relu-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X[train], Y[train], validation_data=(X[val], Y[val]), batch_size=1024, epochs=256, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6db1614a-6c2a-40a6-a03f-3715651852a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from microdf import MicroDataFrame\n",
    "\n",
    "testing = MicroDataFrame(dict(\n",
    "    actual=Y[test],\n",
    "    predicted=model.predict(X[test]).squeeze(),\n",
    "), weights=weights[test].numpy())\n",
    "testing[\"error\"] = testing.predicted - testing.actual\n",
    "testing[\"abs_error\"] = np.abs(testing.error)\n",
    "testing[\"abs_rel_error\"] = testing.abs_error / (testing.actual + 1e-3)\n",
    "testing[\"white\"] = X[test].T[0]\n",
    "testing[\"tax_inc\"] = df[test].reset_index().taxable_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b38a2909-1307-49b2-a198-75b8ff77065b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "      <th>70</th>\n",
       "      <th>80</th>\n",
       "      <th>90</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abs_error</th>\n",
       "      <td>0.004935</td>\n",
       "      <td>8.302305</td>\n",
       "      <td>21.432205</td>\n",
       "      <td>35.928291</td>\n",
       "      <td>53.446816</td>\n",
       "      <td>102.479272</td>\n",
       "      <td>226.290370</td>\n",
       "      <td>411.741338</td>\n",
       "      <td>691.703704</td>\n",
       "      <td>1738.729088</td>\n",
       "      <td>1.709862e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_rel_error</th>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.048033</td>\n",
       "      <td>0.119921</td>\n",
       "      <td>0.251826</td>\n",
       "      <td>0.704660</td>\n",
       "      <td>4892.755199</td>\n",
       "      <td>16555.524066</td>\n",
       "      <td>31075.570550</td>\n",
       "      <td>47479.625011</td>\n",
       "      <td>100052.577113</td>\n",
       "      <td>1.139271e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         10         20         30         40   \\\n",
       "abs_error      0.004935  8.302305  21.432205  35.928291  53.446816   \n",
       "abs_rel_error  0.000145  0.048033   0.119921   0.251826   0.704660   \n",
       "\n",
       "                       50            60            70            80   \\\n",
       "abs_error       102.479272    226.290370    411.741338    691.703704   \n",
       "abs_rel_error  4892.755199  16555.524066  31075.570550  47479.625011   \n",
       "\n",
       "                         90            100  \n",
       "abs_error        1738.729088  1.709862e+04  \n",
       "abs_rel_error  100052.577113  1.139271e+07  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = testing[[\"abs_error\", \"abs_rel_error\"]].quantile(np.linspace(0, 1, 11))\n",
    "x.columns = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de616c63-bae8-4470-8841-dfa3ae133635",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce9491f5-5101-4311-a8a1-6fdddd9837b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "      <th>30</th>\n",
       "      <th>40</th>\n",
       "      <th>50</th>\n",
       "      <th>60</th>\n",
       "      <th>70</th>\n",
       "      <th>80</th>\n",
       "      <th>90</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abs_error</th>\n",
       "      <td>0.004935</td>\n",
       "      <td>8.610998</td>\n",
       "      <td>21.269027</td>\n",
       "      <td>35.871140</td>\n",
       "      <td>52.762369</td>\n",
       "      <td>97.037470</td>\n",
       "      <td>211.650115</td>\n",
       "      <td>413.817622</td>\n",
       "      <td>693.943017</td>\n",
       "      <td>1745.747474</td>\n",
       "      <td>1.709862e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_rel_error</th>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.049283</td>\n",
       "      <td>0.119320</td>\n",
       "      <td>0.258062</td>\n",
       "      <td>0.810084</td>\n",
       "      <td>5651.811035</td>\n",
       "      <td>17532.998427</td>\n",
       "      <td>31988.066096</td>\n",
       "      <td>48409.244919</td>\n",
       "      <td>101115.878768</td>\n",
       "      <td>1.139271e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         10         20         30         40   \\\n",
       "abs_error      0.004935  8.610998  21.269027  35.871140  52.762369   \n",
       "abs_rel_error  0.000145  0.049283   0.119320   0.258062   0.810084   \n",
       "\n",
       "                       50            60            70            80   \\\n",
       "abs_error        97.037470    211.650115    413.817622    693.943017   \n",
       "abs_rel_error  5651.811035  17532.998427  31988.066096  48409.244919   \n",
       "\n",
       "                         90            100  \n",
       "abs_error        1745.747474  1.709862e+04  \n",
       "abs_rel_error  101115.878768  1.139271e+07  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = testing[[\"abs_error\", \"abs_rel_error\"]].quantile(np.linspace(0, 1, 11))\n",
    "x.columns = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fecd60bc-42c4-4eef-9571-93101119fa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>error</th>\n",
       "      <th>abs_error</th>\n",
       "      <th>abs_rel_error</th>\n",
       "      <th>white</th>\n",
       "      <th>tax_inc</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>4342.0</td>\n",
       "      <td>4342.629395</td>\n",
       "      <td>0.629395</td>\n",
       "      <td>0.629395</td>\n",
       "      <td>1.449549e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37800</td>\n",
       "      <td>7114.779785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4425</th>\n",
       "      <td>3295.0</td>\n",
       "      <td>3295.562500</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.707131e-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34861</td>\n",
       "      <td>3783.729980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7777</th>\n",
       "      <td>10797.0</td>\n",
       "      <td>10799.560547</td>\n",
       "      <td>2.560547</td>\n",
       "      <td>2.560547</td>\n",
       "      <td>2.371535e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67900</td>\n",
       "      <td>4007.479980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>21129.0</td>\n",
       "      <td>21122.636719</td>\n",
       "      <td>-6.363281</td>\n",
       "      <td>6.363281</td>\n",
       "      <td>3.011634e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112311</td>\n",
       "      <td>1956.459961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>1758.0</td>\n",
       "      <td>1757.467163</td>\n",
       "      <td>-0.532837</td>\n",
       "      <td>0.532837</td>\n",
       "      <td>3.030925e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16265</td>\n",
       "      <td>3525.020020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4640</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5964.383301</td>\n",
       "      <td>5964.383301</td>\n",
       "      <td>5964.383301</td>\n",
       "      <td>5.964383e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49650</td>\n",
       "      <td>1963.680054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5966.557129</td>\n",
       "      <td>5966.557129</td>\n",
       "      <td>5966.557129</td>\n",
       "      <td>5.966557e+06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>53974</td>\n",
       "      <td>949.400024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7379.401855</td>\n",
       "      <td>7379.401855</td>\n",
       "      <td>7379.401855</td>\n",
       "      <td>7.379402e+06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71850</td>\n",
       "      <td>3543.719971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9235.776367</td>\n",
       "      <td>9235.776367</td>\n",
       "      <td>9235.776367</td>\n",
       "      <td>9.235776e+06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84800</td>\n",
       "      <td>3200.040039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11392.709961</td>\n",
       "      <td>11392.709961</td>\n",
       "      <td>11392.709961</td>\n",
       "      <td>1.139271e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97195</td>\n",
       "      <td>3746.489990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8029 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actual     predicted         error     abs_error  abs_rel_error  white  \\\n",
       "7548   4342.0   4342.629395      0.629395      0.629395   1.449549e-04    0.0   \n",
       "4425   3295.0   3295.562500      0.562500      0.562500   1.707131e-04    1.0   \n",
       "7777  10797.0  10799.560547      2.560547      2.560547   2.371535e-04    0.0   \n",
       "7001  21129.0  21122.636719     -6.363281      6.363281   3.011634e-04    0.0   \n",
       "2043   1758.0   1757.467163     -0.532837      0.532837   3.030925e-04    0.0   \n",
       "...       ...           ...           ...           ...            ...    ...   \n",
       "4640      0.0   5964.383301   5964.383301   5964.383301   5.964383e+06    0.0   \n",
       "2911      0.0   5966.557129   5966.557129   5966.557129   5.966557e+06    1.0   \n",
       "3608      0.0   7379.401855   7379.401855   7379.401855   7.379402e+06    1.0   \n",
       "2364      0.0   9235.776367   9235.776367   9235.776367   9.235776e+06    1.0   \n",
       "3243      0.0  11392.709961  11392.709961  11392.709961   1.139271e+07    1.0   \n",
       "\n",
       "      tax_inc       weight  \n",
       "7548    37800  7114.779785  \n",
       "4425    34861  3783.729980  \n",
       "7777    67900  4007.479980  \n",
       "7001   112311  1956.459961  \n",
       "2043    16265  3525.020020  \n",
       "...       ...          ...  \n",
       "4640    49650  1963.680054  \n",
       "2911    53974   949.400024  \n",
       "3608    71850  3543.719971  \n",
       "2364    84800  3200.040039  \n",
       "3243    97195  3746.489990  \n",
       "\n",
       "[8029 rows x 8 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.sort_values(\"abs_rel_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18cc11c0-bc1b-402d-9997-cc0133248fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[test].reset_index()\n",
    "testing[\"race\"] = df_test.Race\n",
    "testing[\"gender\"] = df_test.Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43dbac9d-a201-422e-8b40-8702ac69639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdbl import num\n",
    "populations = df.groupby([\"Race\", \"Gender\"]).taxable_income.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa8fd1e8-cdc0-4199-acb6-882a87fac084",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pop = df.taxable_income.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8186785d-fd1c-41df-8182-04bd08cca93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipliers = total_pop / populations / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7300a058-dfd9-4875-9fb3-646e9b79d3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>white</th>\n",
       "      <th>black</th>\n",
       "      <th>asian</th>\n",
       "      <th>male</th>\n",
       "      <th>married</th>\n",
       "      <th>age</th>\n",
       "      <th>weekly_hours</th>\n",
       "      <th>employment_income</th>\n",
       "      <th>self_employment_income</th>\n",
       "      <th>dividend_income</th>\n",
       "      <th>...</th>\n",
       "      <th>industry-retail</th>\n",
       "      <th>industry-utilities</th>\n",
       "      <th>industry-information</th>\n",
       "      <th>industry-financial</th>\n",
       "      <th>industry-business</th>\n",
       "      <th>industry-education-health</th>\n",
       "      <th>industry-hospitality</th>\n",
       "      <th>industry-other-services</th>\n",
       "      <th>industry-public-admin</th>\n",
       "      <th>industry-armed-forces</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Race</th>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">White</th>\n",
       "      <th>Female</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>63</td>\n",
       "      <td>40</td>\n",
       "      <td>52500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>67</td>\n",
       "      <td>40</td>\n",
       "      <td>56000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>64</td>\n",
       "      <td>40</td>\n",
       "      <td>34000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>40000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Other</th>\n",
       "      <th>Male</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>31</td>\n",
       "      <td>40</td>\n",
       "      <td>17860</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>57</td>\n",
       "      <td>50</td>\n",
       "      <td>84000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <th>Male</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>12500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80609 rows  39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              white  black  asian   male  married  age  weekly_hours  \\\n",
       "Race  Gender                                                           \n",
       "White Female   True  False  False  False     True   63            40   \n",
       "      Male     True  False  False   True     True   67            40   \n",
       "      Male     True  False  False   True     True   64            40   \n",
       "      Female   True  False  False  False     True   71             0   \n",
       "      Female   True  False  False  False    False   54            44   \n",
       "...             ...    ...    ...    ...      ...  ...           ...   \n",
       "Other Male    False  False  False   True    False   19             0   \n",
       "      Female  False  False  False  False    False   31            40   \n",
       "      Female  False  False  False  False    False   57            50   \n",
       "      Female  False  False  False  False    False   23             0   \n",
       "Asian Male    False  False   True   True    False   32            34   \n",
       "\n",
       "              employment_income  self_employment_income  dividend_income  ...  \\\n",
       "Race  Gender                                                              ...   \n",
       "White Female              52500                       0                0  ...   \n",
       "      Male                56000                       0                0  ...   \n",
       "      Male                34000                       0                0  ...   \n",
       "      Female                  0                       0                0  ...   \n",
       "      Female              40000                       0                0  ...   \n",
       "...                         ...                     ...              ...  ...   \n",
       "Other Male                    0                       0                0  ...   \n",
       "      Female              17860                       0                0  ...   \n",
       "      Female              84000                       0                0  ...   \n",
       "      Female                  0                       0                0  ...   \n",
       "Asian Male                12500                       0                0  ...   \n",
       "\n",
       "              industry-retail  industry-utilities  industry-information  \\\n",
       "Race  Gender                                                              \n",
       "White Female            False               False                 False   \n",
       "      Male              False               False                 False   \n",
       "      Male              False               False                 False   \n",
       "      Female            False               False                 False   \n",
       "      Female            False                True                 False   \n",
       "...                       ...                 ...                   ...   \n",
       "Other Male              False               False                 False   \n",
       "      Female            False               False                 False   \n",
       "      Female            False               False                 False   \n",
       "      Female            False               False                 False   \n",
       "Asian Male              False               False                  True   \n",
       "\n",
       "              industry-financial  industry-business  \\\n",
       "Race  Gender                                          \n",
       "White Female               False              False   \n",
       "      Male                 False              False   \n",
       "      Male                 False              False   \n",
       "      Female               False              False   \n",
       "      Female               False              False   \n",
       "...                          ...                ...   \n",
       "Other Male                 False              False   \n",
       "      Female               False              False   \n",
       "      Female               False              False   \n",
       "      Female               False              False   \n",
       "Asian Male                 False              False   \n",
       "\n",
       "              industry-education-health  industry-hospitality  \\\n",
       "Race  Gender                                                    \n",
       "White Female                      False                 False   \n",
       "      Male                        False                 False   \n",
       "      Male                         True                 False   \n",
       "      Female                      False                 False   \n",
       "      Female                      False                 False   \n",
       "...                                 ...                   ...   \n",
       "Other Male                        False                 False   \n",
       "      Female                      False                  True   \n",
       "      Female                      False                  True   \n",
       "      Female                      False                 False   \n",
       "Asian Male                        False                 False   \n",
       "\n",
       "              industry-other-services  industry-public-admin  \\\n",
       "Race  Gender                                                   \n",
       "White Female                    False                   True   \n",
       "      Male                      False                  False   \n",
       "      Male                      False                  False   \n",
       "      Female                    False                  False   \n",
       "      Female                    False                  False   \n",
       "...                               ...                    ...   \n",
       "Other Male                      False                  False   \n",
       "      Female                    False                  False   \n",
       "      Female                    False                  False   \n",
       "      Female                    False                  False   \n",
       "Asian Male                      False                  False   \n",
       "\n",
       "              industry-armed-forces  \n",
       "Race  Gender                         \n",
       "White Female                  False  \n",
       "      Male                    False  \n",
       "      Male                    False  \n",
       "      Female                  False  \n",
       "      Female                  False  \n",
       "...                             ...  \n",
       "Other Male                    False  \n",
       "      Female                  False  \n",
       "      Female                  False  \n",
       "      Female                  False  \n",
       "Asian Male                    False  \n",
       "\n",
       "[80609 rows x 39 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index([\"Race\", \"Gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3b3fa78-a31d-4287-9670-0b33dfdd0710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>multiplier</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Race</th>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Asian</th>\n",
       "      <th>Female</th>\n",
       "      <td>4.180965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>4.846621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Black</th>\n",
       "      <th>Female</th>\n",
       "      <td>2.191891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>2.941934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Other</th>\n",
       "      <th>Female</th>\n",
       "      <td>7.744908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>8.354996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">White</th>\n",
       "      <th>Female</th>\n",
       "      <td>0.296156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.319188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              multiplier\n",
       "Race  Gender            \n",
       "Asian Female    4.180965\n",
       "      Male      4.846621\n",
       "Black Female    2.191891\n",
       "      Male      2.941934\n",
       "Other Female    7.744908\n",
       "      Male      8.354996\n",
       "White Female    0.296156\n",
       "      Male      0.319188"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dict(multiplier=multipliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d1a0b4c-a0a3-4836-baab-5b3632b764f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_multipliers = df.set_index([\"Race\", \"Gender\"]).merge(pd.DataFrame(dict(multiplier=multipliers)), left_index=True, right_index=True).multiplier.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed1a9a47-33f6-49a6-a3a0-4035b81f208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = weights * weight_multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43fff54-5e14-4864-83b7-0f91b8e9e54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
