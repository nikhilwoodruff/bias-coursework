@Book{Bias,
    publisher = {Oxford University Press},
    author = {Bias},
    title = {{The Oxford English Dictionary}},
    year = {2021},
}
@Book{Discrimination,
    publisher = {Oxford University Press},
    author = {Discrimination},
    title = {{The Oxford English Dictionary}},
    year = {2021},
}
@article{DBLP:journals/corr/SchnabelSSCJ16,
  author    = {Tobias Schnabel and
               Adith Swaminathan and
               Ashudeep Singh and
               Navin Chandak and
               Thorsten Joachims},
  title     = {Recommendations as Treatments: Debiasing Learning and Evaluation},
  journal   = {CoRR},
  volume    = {abs/1602.05352},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.05352},
  archivePrefix = {arXiv},
  eprint    = {1602.05352},
  timestamp = {Mon, 13 Aug 2018 16:46:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchnabelSSCJ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{equality_act,
  author = {{Equality Act 2010, s. 19}},
  url = {https://www.legislation.gov.uk/ukpga/2010/15/section/19}
}
@Article{FitzGerald2019,
author={FitzGerald, Chlo{\"e}
and Martin, Angela
and Berner, Delphine
and Hurst, Samia},
title={Interventions designed to reduce implicit prejudices and implicit stereotypes in real world contexts: a systematic review},
journal={BMC Psychology},
year={2019},
month={May},
day={16},
volume={7},
number={1},
pages={29},
abstract={Implicit biases are present in the general population and among professionals in various domains, where they can lead to discrimination. Many interventions are used to reduce implicit bias. However, uncertainties remain as to their effectiveness.},
issn={2050-7283},
doi={10.1186/s40359-019-0299-7},
url={https://doi.org/10.1186/s40359-019-0299-7}
}
@techreport{NBERw22014,
 title = "Field Experiments on Discrimination",
 author = "Bertrand, Marianne and Duflo, Esther",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "22014",
 year = "2016",
 month = "February",
 doi = {10.3386/w22014},
 URL = "http://www.nber.org/papers/w22014",
 abstract = {This article reviews the existing field experimentation literature on the prevalence of discrimination, the consequences of such discrimination, and possible approaches to undermine it. We highlight key gaps in the literature and ripe opportunities for future field work. Section 1 reviews the various experimental methods that have been employed to measure the prevalence of discrimination, most notably audit and correspondence studies; it also describes several other measurement tools commonly used in lab-based work that deserve greater consideration in field research. Section 2 provides an overview of the literature on the costs of being stereotyped or discriminated against, with a focus on self-expectancy effects and self-fulfilling prophecies; section 2 also discusses the thin field-based literature on the consequences of limited diversity in organizations and groups. The final section of the paper, Section 3, reviews the evidence for policies and interventions aimed at weakening discrimination, covering role model and intergroup contact effects, as well as socio-cognitive and technological de-biasing strategies.},
}
@InProceedings{pmlr-v97-brunet19a,
  title = 	 {Understanding the Origins of Bias in Word Embeddings},
  author =       {Brunet, Marc-Etienne and Alkalay-Houlihan, Colleen and Anderson, Ashton and Zemel, Richard},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {803--811},
  year = 	 {2019},
  editor = 	 {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/brunet19a/brunet19a.pdf},
  url = 	 {
http://proceedings.mlr.press/v97/brunet19a.html
},
  abstract = 	 {Popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems can amplify stereotypes in important contexts. Although some methods have been developed to mitigate this problem, how word embedding biases arise during training is poorly understood. In this work we develop a technique to address this question. Given a word embedding, our method reveals how perturbing the training corpus would affect the resulting embedding bias. By tracing the origins of word embedding bias back to the original training documents, one can identify subsets of documents whose removal would most reduce bias. We demonstrate our methodology on Wikipedia and New York Times corpora, and find it to be very accurate.}
}
@article{article,
author = {Kodelja, Zdenko},
year = {2016},
month = {02},
pages = {9-24},
title = {Equality of Opportunity and Equality of Outcome},
volume = {6},
journal = {Center for Educational Policy Studies Journal}
}
@misc{lee_ashers,
author = {Lee v Ashers Baking Company Ltd},
year = {1998},
month = {October},
publisher = {UK Supreme Court},
url = {https://www.supremecourt.uk/cases/docs/uksc-2017-0020-judgment.pdf}
}
@Article{Garvey2018,
author={Garvey, Brian},
title={The evolution of morality and its rollback},
journal={History and philosophy of the life sciences},
year={2018},
month={Mar},
day={21},
publisher={Springer International Publishing},
volume={40},
number={2},
pages={26-26},
keywords={Cognitive modularity; Embedded cognition; Evolution of morality; Evolutionary psychology; Evolutionary rollback; *Biological Evolution; *Cognition; *Cultural Evolution; Humans; Models, Psychological; *Morals},
abstract={According to most Evolutionary Psychologists, human moral attitudes are rooted in cognitive modules that evolved in the Stone Age to solve problems of social interaction. A crucial component of their view is that such cognitive modules remain unchanged since the Stone Age, and I question that here. I appeal to evolutionary rollback, the phenomenon where an organ becomes non-functional and eventually atrophies or disappears-e.g. cave-dwelling fish losing their eyes. I argue that even if cognitive modules evolved in the Stone Age to solve problems of social interaction, conditions since then have favoured rollback of those modules. This is because there are institutions that solve those problems-e.g. legal systems. Moreover, evidence suggests that where external resources are available to perform cognitive tasks, humans often use them instead of internal ones. In arguing that Stone Age cognitive modules are unchanged, Evolutionary Psychologists say that evolutionary change is necessarily slow, and that there is high genetic similarity between human populations worldwide. I counter-argue that what is necessarily slow is the building-up of complex mechanisms. Undoing this can be much quicker. Moreover, rollback of cognitive mechanisms need not require any genetic change. Finally, I argue that cross-cultural similarity in some trait need not be rooted in genetic similarity. This is not intended as decisive evidence that rollback has occurred. To finish, I suggest ways we might decide whether moral attitudes are likely to be rooted in unchanged Stone Age modules, given that I have argued that cross-cultural similarity is not enough.},
note={29564652[pmid]},
note={PMC5862922[pmcid]},
issn={0391-9714},
doi={10.1007/s40656-018-0190-5},
url={https://pubmed.ncbi.nlm.nih.gov/29564652},
url={https://doi.org/10.1007/s40656-018-0190-5},
language={eng}
}
@article{DBLP:journals/corr/abs-1908-09635,
  author    = {Ninareh Mehrabi and
               Fred Morstatter and
               Nripsuta Saxena and
               Kristina Lerman and
               Aram Galstyan},
  title     = {A Survey on Bias and Fairness in Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1908.09635},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.09635},
  archivePrefix = {arXiv},
  eprint    = {1908.09635},
  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-09635.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@Article{Bach2019,
author={Bach, Stefan
and Thiemann, Andreas
and Zucco, Aline},
title={Looking for the missing rich: tracing the top tail of the wealth distribution},
journal={International Tax and Public Finance},
year={2019},
month={Dec},
day={01},
volume={26},
number={6},
pages={1234-1258},
abstract={We analyse the top tail of the wealth distribution in France, Germany, and Spain using the first and second waves of the Household Finance and Consumption Survey (HFCS). Since top wealth is likely to be under-represented in household surveys, we integrate big fortunes from rich lists, estimate a Pareto distribution, and impute the missing rich. In addition to the Forbes list, we rely on national rich lists since they represent a broader base of the big fortunes in those countries. As a result, the top 1{\%} wealth share increases notably for the three selected countries after imputing the top wealth. We find that national rich lists can improve the estimation of the Pareto coefficient in particular when the list of national USD billionaires is short.},
issn={1573-6970},
doi={10.1007/s10797-019-09578-1},
url={https://doi.org/10.1007/s10797-019-09578-1}
}
@inproceedings{NIPS2017_b8b9c74a,
 author = {Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On Fairness and Calibration},
 url = {https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf},
 volume = {30},
 year = {2017}
}
@inproceedings{10.1145/2783258.2783311,
author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
title = {Certifying and Removing Disparate Impact},
year = {2015},
isbn = {9781450336642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2783258.2783311},
doi = {10.1145/2783258.2783311},
abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process.When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses.We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {259–268},
numpages = {10},
keywords = {disparate impact, machine learning, fairness},
location = {Sydney, NSW, Australia},
series = {KDD '15}
}
@article{DBLP:journals/corr/HardtPS16,
  author    = {Moritz Hardt and
               Eric Price and
               Nathan Srebro},
  title     = {Equality of Opportunity in Supervised Learning},
  journal   = {CoRR},
  volume    = {abs/1610.02413},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02413},
  archivePrefix = {arXiv},
  eprint    = {1610.02413},
  timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HardtPS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{GrgicHlaca2016TheCF,
  title={The Case for Process Fairness in Learning: Feature Selection for Fair Decision Making},
  author={Nina Grgic-Hlaca and M. Zafar and K. Gummadi and Adrian Weller},
  year={2016}
}
@inproceedings{NIPS2017_9a49a25d,
 author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Optimized Pre-Processing for Discrimination Prevention},
 url = {https://proceedings.neurips.cc/paper/2017/file/9a49a25d845a483fae4be7e341368e36-Paper.pdf},
 volume = {30},
 year = {2017}
}
@article{preprocessing,
author = {Kamiran, Faisal and Calders, Toon},
year = {2011},
month = {10},
pages = {},
title = {Data Pre-Processing Techniques for Classification without Discrimination},
volume = {33},
journal = {Knowledge and Information Systems},
doi = {10.1007/s10115-011-0463-8}
}
@inproceedings{inproceedings,
author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
year = {2012},
month = {09},
pages = {35-50},
title = {Fairness-Aware Classifier with Prejudice Remover Regularizer},
doi = {10.1007/978-3-642-33486-3_3}
}
@article{DBLP:journals/corr/abs-1805-11202,
  author    = {Depeng Xu and
               Shuhan Yuan and
               Lu Zhang and
               Xintao Wu},
  title     = {FairGAN: Fairness-aware Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1805.11202},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.11202},
  archivePrefix = {arXiv},
  eprint    = {1805.11202},
  timestamp = {Wed, 27 Feb 2019 14:10:50 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-11202.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1802-06309,
  author    = {David Madras and
               Elliot Creager and
               Toniann Pitassi and
               Richard S. Zemel},
  title     = {Learning Adversarially Fair and Transferable Representations},
  journal   = {CoRR},
  volume    = {abs/1802.06309},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.06309},
  archivePrefix = {arXiv},
  eprint    = {1802.06309},
  timestamp = {Mon, 13 Aug 2018 16:49:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-06309.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Madras2018PredictRI,
  title={Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer},
  author={David Madras and T. Pitassi and R. Zemel},
  booktitle={NeurIPS},
  year={2018}
}

@article{poll,
author = {Zhang, Baobao and Dafoe, Allan},
year = {2019},
month = {01},
pages = {},
title = {Artificial Intelligence: American Attitudes and Trends},
journal = {SSRN Electronic Journal},
doi = {10.2139/ssrn.3312874}
}